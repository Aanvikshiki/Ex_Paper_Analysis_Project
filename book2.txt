{"12.4 Allocation Methods":"""The direct-access nature of disks gives us flexibility in the implementation of files. In almost every case, many files are stored on the same disk. The main
problem is how to allocate space to these files so that disk space is utilized
effectively and files can be accessed quickly. Three major methods of allocating
disk space are in wide use: contiguous, linked, and indexed. Each method has
advantages and disadvantages. Although some systems support all three, it is
more common for a system to use one method for all files within a file-system
type.
12.4.1 Contiguous Allocation
Contiguous allocation requires that each file occupy a set of contiguous blocks
on the disk. Disk addresses define a linear ordering on the disk. With this
ordering, assuming that only one job is accessing the disk, accessing block b +
1 after block b normally requires no head movement. When head movement
is needed (from the last sector of one cylinder to the first sector of the next
cylinder), the head need only move from one track to the next. Thus, the
number of disk seeks required for accessing contiguously allocated files is
minimal, as is seek time when a seek is finally needed.
Contiguous allocation of a file is defined by the disk address and length (in
block units) of the first block. If the file is n blocks long and starts at location
b, then it occupies blocks b, b + 1, b + 2, ..., b + n − 1. The directory entry for
each file indicates the address of the starting block and the length of the area
allocated for this file (Figure 12.5).
Accessing a file that has been allocated contiguously is easy. For sequential
access, the file system remembers the disk address of the last block referenced
and, when necessary, reads the next block. For direct access to block i of a
554 Chapter 12 File-System Implementation
directory
0 1 2 3
4 5 6 7
8 9 10 11
12 13 14 15
16 17 18 19
20 21 22 23
24 25 26 27
28 29 30 31
count
f
tr
mail
list
start
0
14
19
28
6
length
2
3
6
4
2
file
count
tr
mail
list
f
Figure 12.5 Contiguous allocation of disk space.
file that starts at block b, we can immediately access block b + i. Thus, both
sequential and direct access can be supported by contiguous allocation.
Contiguous allocation has some problems, however. One difficulty is
finding space for a new file. The system chosen to manage free space determines
how this task is accomplished; these management systems are discussed in
Section 12.5. Any management system can be used, but some are slower than
others.
The contiguous-allocation problem can be seen as a particular application
of the general dynamic storage-allocation problem discussed in Section 8.3,
which involves how to satisfy a request of size n from a list of free holes. First
fit and best fit are the most common strategies used to select a free hole from
the set of available holes. Simulations have shown that both first fit and best fit
are more efficient than worst fit in terms of both time and storage utilization.
Neither first fit nor best fit is clearly best in terms of storage utilization, but
first fit is generally faster.
All these algorithms suffer from the problem of external fragmentation.
As files are allocated and deleted, the free disk space is broken into little pieces.
External fragmentation exists whenever free space is broken into chunks. It
becomes a problem when the largest contiguous chunk is insufficient for a
request; storage is fragmented into a number of holes, none of which is large
enough to store the data. Depending on the total amount of disk storage and the
average file size, external fragmentation may be a minor or a major problem.
One strategy for preventing loss of significant amounts of disk space to
external fragmentation is to copy an entire file system onto another disk. The
original disk is then freed completely, creating one large contiguous free space.
We then copy the files back onto the original disk by allocating contiguous
space from this one large hole. This scheme effectively compacts all free space
into one contiguous space, solving the fragmentation problem. The cost of this
12.4 Allocation Methods 555
compaction is time, however, and the cost can be particularly high for large
hard disks. Compacting these disks may take hours and may be necessary on
a weekly basis. Some systems require that this function be done off-line, with
the file system unmounted. During this down time, normal system operation
generally cannot be permitted, so such compaction is avoided at all costs on
production machines. Most modern systems that need defragmentation can
perform it on-line during normal system operations, but the performance
penalty can be substantial.
Another problem with contiguous allocation is determining how much
space is needed for a file. When the file is created, the total amount of space
it will need must be found and allocated. How does the creator (program or
person) know the size of the file to be created? In some cases, this determination
may be fairly simple (copying an existing file, for example). In general,
however, the size of an output file may be difficult to estimate.
If we allocate too little space to a file, we may find that the file cannot
be extended. Especially with a best-fit allocation strategy, the space on both
sides of the file may be in use. Hence, we cannot make the file larger in place.
Two possibilities then exist. First, the user program can be terminated, with
an appropriate error message. The user must then allocate more space and
run the program again. These repeated runs may be costly. To prevent them,
the user will normally overestimate the amount of space needed, resulting
in considerable wasted space. The other possibility is to find a larger hole,
copy the contents of the file to the new space, and release the previous space.
This series of actions can be repeated as long as space exists, although it can
be time consuming. The user need never be informed explicitly about what
is happening, however; the system continues despite the problem, although
more and more slowly.
Even if the total amount of space needed for a file is known in advance,
preallocation may be inefficient. A file that will grow slowly over a long period
(months or years) must be allocated enough space for its final size, even though
much of that space will be unused for a long time. The file therefore has a large
amount of internal fragmentation.
To minimize these drawbacks, some operating systems use a modified
contiguous-allocation scheme. Here, a contiguous chunk of space is allocated
initially. Then, if that amount proves not to be large enough, another chunk of
contiguous space, known as an extent, is added. The location of a file’s blocks
is then recorded as a location and a block count, plus a link to the first block
of the next extent. On some systems, the owner of the file can set the extent
size, but this setting results in inefficiencies if the owner is incorrect. Internal
fragmentation can still be a problem if the extents are too large, and external
fragmentation can become a problem as extents of varying sizes are allocated
and deallocated. The commercial Veritas file system uses extents to optimize
performance. Veritas is a high-performance replacement for the standard UNIX
UFS.
12.4.2 Linked Allocation
Linked allocation solves all problems of contiguous allocation. With linked
allocation, each file is a linked list of disk blocks; the disk blocks may be
scattered anywhere on the disk. The directory contains a pointer to the first
556 Chapter 12 File-System Implementation
0 1 2 3
4 5 7
8 9 10 11
12 13 14
16 17 18 19
20 21 22 23
24 25 26 27
28 29 30 31
15
6
file
jeep
start
9
directory
end
25
1
1
-1
2
Figure 12.6 Linked allocation of disk space.
and last blocks of the file. For example, a file of five blocks might start at block
9 and continue at block 16, then block 1, then block 10, and finally block 25
(Figure 12.6). Each block contains a pointer to the next block. These pointers
are not made available to the user. Thus, if each block is 512 bytes in size, and
a disk address (the pointer) requires 4 bytes, then the user sees blocks of 508
bytes.
To create a new file, we simply create a new entry in the directory. With
linked allocation, each directory entry has a pointer to the first disk block of
the file. This pointer is initialized to null (the end-of-list pointer value) to
signify an empty file. The size field is also set to 0. A write to the file causes
the free-space management system to find a free block, and this new block
is written to and is linked to the end of the file. To read a file, we simply
read blocks by following the pointers from block to block. There is no external
fragmentation with linked allocation, and any free block on the free-space list
can be used to satisfy a request. The size of a file need not be declared when the
file is created. A file can continue to grow as long as free blocks are available.
Consequently, it is never necessary to compact disk space.
Linked allocation does have disadvantages, however. The major problem
is that it can be used effectively only for sequential-access files. To find the
ith block of a file, we must start at the beginning of that file and follow the
pointers until we get to the ith block. Each access to a pointer requires a disk
read, and some require a disk seek. Consequently, it is inefficient to support a
direct-access capability for linked-allocation files.
Another disadvantage is the space required for the pointers. If a pointer
requires 4 bytes out of a 512-byte block, then 0.78 percent of the disk is being
used for pointers, rather than for information. Each file requires slightly more
space than it would otherwise.
The usual solution to this problem is to collect blocks into multiples, called
clusters, and to allocate clusters rather than blocks. For instance, the file system
12.4 Allocation Methods 557
may define a cluster as four blocks and operate on the disk only in cluster
units. Pointers then use a much smaller percentage of the file’s disk space.
This method allows the logical-to-physical block mapping to remain simple
but improves disk throughput (because fewer disk-head seeks are required)
and decreases the space needed for block allocation and free-list management.
The cost of this approach is an increase in internal fragmentation, because
more space is wasted when a cluster is partially full than when a block is
partially full. Clusters can be used to improve the disk-access time for many
other algorithms as well, so they are used in most file systems.
Yet another problem of linked allocation is reliability. Recall that the files
are linked together by pointers scattered all over the disk, and consider what
would happen if a pointer were lost or damaged. A bug in the operating-system
software or a disk hardware failure might result in picking up the wrong
pointer. This error could in turn result in linking into the free-space list or into
another file. One partial solution is to use doubly linked lists, and another is
to store the file name and relative block number in each block. However, these
schemes require even more overhead for each file.
An important variation on linked allocation is the use of a file-allocation
table (FAT). This simple but efficient method of disk-space allocation was used
by the MS-DOS operating system. A section of disk at the beginning of each
volume is set aside to contain the table. The table has one entry for each disk
block and is indexed by block number. The FAT is used in much the same
way as a linked list. The directory entry contains the block number of the
first block of the file. The table entry indexed by that block number contains
the block number of the next block in the file. This chain continues until it
reaches the last block, which has a special end-of-file value as the table entry.
An unused block is indicated by a table value of 0. Allocating a new block to
a file is a simple matter of finding the first 0-valued table entry and replacing
the previous end-of-file value with the address of the new block. The 0 is then
replaced with the end-of-file value. An illustrative example is the FAT structure
shown in Figure 12.7 for a file consisting of disk blocks 217, 618, and 339.
The FAT allocation scheme can result in a significant number of disk head
seeks, unless the FAT is cached. The disk head must move to the start of the
volume to read the FAT and find the location of the block in question, then
move to the location of the block itself. In the worst case, both moves occur for
each of the blocks. A benefit is that random-access time is improved, because
the disk head can find the location of any block by reading the information in
the FAT.
12.4.3 Indexed Allocation
Linked allocation solves the external-fragmentation and size-declaration problems of contiguous allocation. However, in the absence of a FAT, linked
allocation cannot support efficient direct access, since the pointers to the blocks
are scattered with the blocks themselves all over the disk and must be retrieved
in order. Indexed allocation solves this problem by bringing all the pointers
together into one location: the index block.
Each file has its own index block, which is an array of disk-block addresses.
The ith entry in the index block points to the ith block of the file. The directory
558 Chapter 12 File-System Implementation
• • •
directory entry
test 217
name start block
0
217 618
339
618 339
number of disk blocks –1
FAT
Figure 12.7 File-allocation table.
contains the address of the index block (Figure 12.8). To find and read the ith
block, we use the pointer in the ith index-block entry. This scheme is similar to
the paging scheme described in Section 8.5.
When the file is created, all pointers in the index block are set to null.
When the ith block is first written, a block is obtained from the free-space
manager, and its address is put in the ith index-block entry.
Indexed allocation supports direct access, without suffering from external
fragmentation, because any free block on the disk can satisfy a request for more
space. Indexed allocation does suffer from wasted space, however. The pointer
directory
0 1 2 3
4 5 7
8 9 10 11
12 13 14
16 17 18 19
20 21 22 23
24 25 26 27
28 29 30 31
15
6
9
16
1
10
25
–1
–1
–1
file
jeep
index block
19
19
Figure 12.8 Indexed allocation of disk space.
12.4 Allocation Methods 559
overhead of the index block is generally greater than the pointer overhead of
linked allocation. Consider a common case in which we have a file of only one
or two blocks. With linked allocation, we lose the space of only one pointer per
block. With indexed allocation, an entire index block must be allocated, even
if only one or two pointers will be non-null.
This point raises the question of how large the index block should be. Every
file must have an index block, so we want the index block to be as small as
possible. If the index block is too small, however, it will not be able to hold
enough pointers for a large file, and a mechanism will have to be available to
deal with this issue. Mechanisms for this purpose include the following:
• Linked scheme. An index block is normally one disk block. Thus, it can
be read and written directly by itself. To allow for large files, we can link
together several index blocks. For example, an index block might contain a
small header giving the name of the file and a set of the first 100 disk-block
addresses. The next address (the last word in the index block) is null (for
a small file) or is a pointer to another index block (for a large file).
• Multilevel index. A variant of linked representation uses a first-level index
block to point to a set of second-level index blocks, which in turn point to
the file blocks. To access a block, the operating system uses the first-level
index to find a second-level index block and then uses that block to find the
desired data block. This approach could be continued to a third or fourth
level, depending on the desired maximum file size. With 4,096-byte blocks,
we could store 1,024 four-byte pointers in an index block. Two levels of
indexes allow 1,048,576 data blocks and a file size of up to 4 GB.
• Combined scheme. Another alternative, used in UNIX-based file systems,
is to keep the first, say, 15 pointers of the index block in the file’s inode.
The first 12 of these pointers point to direct blocks; that is, they contain
addresses of blocks that contain data of the file. Thus, the data for small
files (of no more than 12 blocks) do not need a separate index block. If the
block size is 4 KB, then up to 48 KB of data can be accessed directly. The next
three pointers point to indirect blocks. The first points to a single indirect
block, which is an index block containing not data but the addresses of
blocks that do contain data. The second points to a double indirect block,
which contains the address of a block that contains the addresses of blocks
that contain pointers to the actual data blocks. The last pointer contains
the address of a triple indirect block. (A UNIX inode is shown in Figure
12.9.)
Under this method, the number of blocks that can be allocated to a file
exceeds the amount of space addressable by the 4-byte file pointers used
by many operating systems. A 32-bit file pointer reaches only 232 bytes,
or 4 GB. Many UNIX and Linux implementations now support 64-bit file
pointers, which allows files and file systems to be several exbibytes in size.
The ZFS file system supports 128-bit file pointers.
Indexed-allocation schemes suffer from some of the same performance
problems as does linked allocation. Specifically, the index blocks can be cached
in memory, but the data blocks may be spread all over a volume.
560 Chapter 12 File-System Implementation
direct blocks
data
data
data
data
data
data
data
data
data
data
•
•
• •
•
•
•
•
•
•
•
•
•
•
•
•
•
•
mode
owners (2)
timestamps (3)
size block count
single indirect
double indirect
triple indirect
Figure 12.9 The UNIX inode.
12.4.4 Performance
The allocation methods that we have discussed vary in their storage efficiency
and data-block access times. Both are important criteria in selecting the proper
method or methods for an operating system to implement.
Before selecting an allocation method, we need to determine how the
systems will be used. A system with mostly sequential access should not use
the same method as a system with mostly random access.
For any type of access, contiguous allocation requires only one access to get
a disk block. Since we can easily keep the initial address of the file in memory,
we can calculate immediately the disk address of the ith block (or the next
block) and read it directly.
For linked allocation, we can also keep the address of the next block in
memory and read it directly. This method is fine for sequential access; for
direct access, however, an access to the ith block might require i disk reads. This
problem indicates why linked allocation should not be used for an application
requiring direct access.
As a result, some systems support direct-access files by using contiguous
allocation and sequential-access files by using linked allocation. For these
systems, the type of access to be made must be declared when the file is created.
A file created for sequential access will be linked and cannot be used for direct
access. A file created for direct access will be contiguous and can support both
direct access and sequential access, but its maximum length must be declared
when it is created. In this case, the operating system must have appropriate
data structures and algorithms to support both allocation methods. Files can be
converted from one type to another by the creation of a new file of the desired
type, into which the contents of the old file are copied. The old file may then
be deleted and the new file renamed.
12.5 Free-Space Management 561
Indexed allocation is more complex. If the index block is already in memory,
then the access can be made directly. However, keeping the index block in
memory requires considerable space. If this memory space is not available,
then we may have to read first the index block and then the desired data
block. For a two-level index, two index-block reads might be necessary. For an
extremely large file, accessing a block near the end of the file would require
reading in all the index blocks before the needed data block finally could
be read. Thus, the performance of indexed allocation depends on the index
structure, on the size of the file, and on the position of the block desired.
Some systems combine contiguous allocation with indexed allocation by
using contiguous allocation for small files (up to three or four blocks) and
automatically switching to an indexed allocation if the file grows large. Since
most files are small, and contiguous allocation is efficient for small files, average
performance can be quite good.
Many other optimizations are in use. Given the disparity between CPU
speed and disk speed, it is not unreasonable to add thousands of extra
instructions to the operating system to save just a few disk-head movements.
Furthermore, this disparity is increasing over time, to the point where hundreds
of thousands of instructions could reasonably be used to optimize head
movements.""",
"1.11 Computing Environments":"""So far, we have briefly described several aspects of computer systems and the
operating systems that manage them. We turn now to a discussion of how
operating systems are used in a variety of computing environments.
1.11.1 Traditional Computing
As computing has matured, the lines separating many of the traditional computing environments have blurred. Consider the “typical office environment.”
Just a few years ago, this environment consisted of PCs connected to a network,
with servers providing file and print services. Remote access was awkward,
and portability was achieved by use of laptop computers. Terminals attached
to mainframes were prevalent at many companies as well, with even fewer
remote access and portability options.
The current trend is toward providing more ways to access these computing
environments. Web technologies and increasing WAN bandwidth are stretching
the boundaries of traditional computing. Companies establish portals, which
provide Web accessibility to their internal servers. Network computers (or
thin clients)—which are essentially terminals that understand web-based
computing—are used in place of traditional workstations where more security
or easier maintenance is desired. Mobile computers can synchronize with PCs
to allow very portable use of company information. Mobile computers can also
connect to wireless networks and cellular data networks to use the company’s
Web portal (as well as the myriad other Web resources).
At home, most users once had a single computer with a slow modem
connection to the office, the Internet, or both. Today, network-connection
speeds once available only at great cost are relatively inexpensive in many
places, giving home users more access to more data. These fast data connections
are allowing home computers to serve up Web pages and to run networks that
include printers, client PCs, and servers. Many homes use firewalls to protect
their networks from security breaches.
In the latter half of the 20th century, computing resources were relatively
scarce. (Before that, they were nonexistent!) For a period of time, systems
were either batch or interactive. Batch systems processed jobs in bulk, with
predetermined input from files or other data sources. Interactive systems
waited for input from users. To optimize the use of the computing resources,
multiple users shared time on these systems. Time-sharing systems used a
36 Chapter 1 Introduction
timer and scheduling algorithms to cycle processes rapidly through the CPU,
giving each user a share of the resources.
Today, traditional time-sharing systems are uncommon. The same scheduling technique is still in use on desktop computers, laptops, servers, and even
mobile computers, but frequently all the processes are owned by the same
user (or a single user and the operating system). User processes, and system
processes that provide services to the user, are managed so that each frequently
gets a slice of computer time. Consider the windows created while a user
is working on a PC, for example, and the fact that they may be performing
different tasks at the same time. Even a web browser can be composed of
multiple processes, one for each website currently being visited, with time
sharing applied to each web browser process.
1.11.2 Mobile Computing
Mobile computing refers to computing on handheld smartphones and tablet
computers. These devices share the distinguishing physical features of being
portable and lightweight. Historically, compared with desktop and laptop
computers, mobile systems gave up screen size, memory capacity, and overall
functionality in return for handheld mobile access to services such as e-mail
and web browsing. Over the past few years, however, features on mobile
devices have become so rich that the distinction in functionality between, say,
a consumer laptop and a tablet computer may be difficult to discern. In fact,
we might argue that the features of a contemporary mobile device allow it to
provide functionality that is either unavailable or impractical on a desktop or
laptop computer.
Today, mobile systems are used not only for e-mail and web browsing but
also for playing music and video, reading digital books, taking photos, and
recording high-definition video. Accordingly, tremendous growth continues
in the wide range of applications that run on such devices. Many developers
are now designing applications that take advantage of the unique features of
mobile devices, such as global positioning system (GPS) chips, accelerometers,
and gyroscopes. An embedded GPS chip allows a mobile device to use satellites
to determine its precise location on earth. That functionality is especially useful
in designing applications that provide navigation—for example, telling users
which way to walk or drive or perhaps directing them to nearby services, such
as restaurants. An accelerometer allows a mobile device to detect its orientation
with respect to the ground and to detect certain other forces, such as tilting
and shaking. In several computer games that employ accelerometers, players
interface with the system not by using a mouse or a keyboard but rather by
tilting, rotating, and shaking the mobile device! Perhaps more a practical use
of these features is found in augmented-reality applications, which overlay
information on a display of the current environment. It is difficult to imagine
how equivalent applications could be developed on traditional laptop or
desktop computer systems.
To provide access to on-line services, mobile devices typically use either
IEEE standard 802.11 wireless or cellular data networks. The memory capacity
and processing speed of mobile devices, however, are more limited than those
of PCs. Whereas a smartphone or tablet may have 64 GB in storage, it is not
uncommon to find 1 TB in storage on a desktop computer. Similarly, because
1.11 Computing Environments 37
power consumption is such a concern, mobile devices often use processors that
are smaller, are slower, and offer fewer processing cores than processors found
on traditional desktop and laptop computers.
Two operating systems currently dominate mobile computing: Apple iOS
and Google Android. iOS was designed to run on Apple iPhone and iPad
mobile devices. Android powers smartphones and tablet computers available
from many manufacturers. We examine these two mobile operating systems in
further detail in Chapter 2.
1.11.3 Distributed Systems
A distributed system is a collection of physically separate, possibly heterogeneous, computer systems that are networked to provide users with access to
the various resources that the system maintains. Access to a shared resource
increases computation speed, functionality, data availability, and reliability.
Some operating systems generalize network access as a form of file access, with
the details of networking contained in the network interface’s device driver.
Others make users specifically invoke network functions. Generally, systems
contain a mix of the two modes—for example FTP and NFS. The protocols
that create a distributed system can greatly affect that system’s utility and
popularity.
A network, in the simplest terms, is a communication path between
two or more systems. Distributed systems depend on networking for their
functionality. Networks vary by the protocols used, the distances between
nodes, and the transport media. TCP/IP is the most common network protocol,
and it provides the fundamental architecture of the Internet. Most operating
systems support TCP/IP, including all general-purpose ones. Some systems
support proprietary protocols to suit their needs. To an operating system, a
network protocol simply needs an interface device—a network adapter, for
example—with a device driver to manage it, as well as software to handle
data. These concepts are discussed throughout this book.
Networks are characterized based on the distances between their nodes.
A local-area network (LAN) connects computers within a room, a building,
or a campus. A wide-area network (WAN) usually links buildings, cities, or
countries. A global company may have a WAN to connect its offices worldwide,
for example. These networks may run one protocol or several protocols. The
continuing advent of new technologies brings about new forms of networks.
For example, a metropolitan-area network (MAN) could link buildings within
a city. BlueTooth and 802.11 devices use wireless technology to communicate
over a distance of several feet, in essence creating a personal-area network
(PAN) between a phone and a headset or a smartphone and a desktop computer.
The media to carry networks are equally varied. They include copper wires,
fiber strands, and wireless transmissions between satellites, microwave dishes,
and radios. When computing devices are connected to cellular phones, they
create a network. Even very short-range infrared communication can be used
for networking. At a rudimentary level, whenever computers communicate,
they use or create a network. These networks also vary in their performance
and reliability.
Some operating systems have taken the concept of networks and distributed systems further than the notion of providing network connectivity.
38 Chapter 1 Introduction
A network operating system is an operating system that provides features
such as file sharing across the network, along with a communication scheme
that allows different processes on different computers to exchange messages.
A computer running a network operating system acts autonomously from all
other computers on the network, although it is aware of the network and is
able to communicate with other networked computers. A distributed operating
system provides a less autonomous environment. The different computers
communicate closely enough to provide the illusion that only a single operating
system controls the network. We cover computer networks and distributed
systems in Chapter 17.
1.11.4 Client –Server Computing
As PCs have become faster, more powerful, and cheaper, designers have shifted
away from centralized system architecture. Terminals connected to centralized
systems are now being supplanted by PCs and mobile devices. Correspondingly, user-interface functionality once handled directly by centralized systems
is increasingly being handled by PCs, quite often through a web interface. As
a result, many of today’s systems act as server systems to satisfy requests
generated by client systems. This form of specialized distributed system, called
a client–server system, has the general structure depicted in Figure 1.18.
Server systems can be broadly categorized as compute servers and file
servers:
• The compute-server system provides an interface to which a client can
send a request to perform an action (for example, read data). In response,
the server executes the action and sends the results to the client. A server
running a database that responds to client requests for data is an example
of such a system.
• The file-server system provides a file-system interface where clients can
create, update, read, and delete files. An example of such a system is a web
server that delivers files to clients running web browsers.
Server Network
client
desktop
client
laptop
client
smartphone
Figure 1.18 General structure of a client – server system.
1.11 Computing Environments 39
1.11.5 Peer-to-Peer Computing
Another structure for a distributed system is the peer-to-peer (P2P) system
model. In this model, clients and servers are not distinguished from one
another. Instead, all nodes within the system are considered peers, and each
may act as either a client or a server, depending on whether it is requesting or
providing a service. Peer-to-peer systems offer an advantage over traditional
client-server systems. In a client-server system, the server is a bottleneck; but
in a peer-to-peer system, services can be provided by several nodes distributed
throughout the network.
To participate in a peer-to-peer system, a node must first join the network
of peers. Once a node has joined the network, it can begin providing services
to—and requesting services from—other nodes in the network. Determining
what services are available is accomplished in one of two general ways:
• When a node joins a network, it registers its service with a centralized
lookup service on the network. Any node desiring a specific service first
contacts this centralized lookup service to determine which node provides
the service. The remainder of the communication takes place between the
client and the service provider.
• An alternative scheme uses no centralized lookup service. Instead, a peer
acting as a client must discover what node provides a desired service by
broadcasting a request for the service to all other nodes in the network. The
node (or nodes) providing that service responds to the peer making the
request. To support this approach, a discovery protocol must be provided
that allows peers to discover services provided by other peers in the
network. Figure 1.19 illustrates such a scenario.
Peer-to-peer networks gained widespread popularity in the late 1990s with
several file-sharing services, such as Napster and Gnutella, that enabled peers
to exchange files with one another. The Napster system used an approach
similar to the first type described above: a centralized server maintained an
index of all files stored on peer nodes in the Napster network, and the actual
client
client client
client client
Figure 1.19 Peer-to-peer system with no centralized service.
40 Chapter 1 Introduction
exchange of files took place between the peer nodes. The Gnutella system used
a technique similar to the second type: a client broadcasted file requests to
other nodes in the system, and nodes that could service the request responded
directly to the client. The future of exchanging files remains uncertain because
peer-to-peer networks can be used to exchange copyrighted materials (music,
for example) anonymously, and there are laws governing the distribution of
copyrighted material. Notably, Napster ran into legal trouble for copyright
infringement and its services were shut down in 2001.
Skype is another example of peer-to-peer computing. It allows clients to
make voice calls and video calls and to send text messages over the Internet
using a technology known as voice over IP (VoIP). Skype uses a hybrid peerto-peer approach. It includes a centralized login server, but it also incorporates
decentralized peers and allows two peers to communicate.
1.11.6 Virtualization
Virtualization is a technology that allows operating systems to run as applications within other operating systems. At first blush, there seems to be
little reason for such functionality. But the virtualization industry is vast and
growing, which is a testament to its utility and importance.
Broadly speaking, virtualization is one member of a class of software
that also includes emulation. Emulation is used when the source CPU type
is different from the target CPU type. For example, when Apple switched from
the IBM Power CPU to the Intel x86 CPU for its desktop and laptop computers,
it included an emulation facility called “Rosetta,” which allowed applications
compiled for the IBM CPU to run on the Intel CPU. That same concept can be
extended to allow an entire operating system written for one platform to run
on another. Emulation comes at a heavy price, however. Every machine-level
instruction that runs natively on the source system must be translated to the
equivalent function on the target system, frequently resulting in several target
instructions. If the source and target CPUs have similar performance levels, the
emulated code can run much slower than the native code.
A common example of emulation occurs when a computer language is
not compiled to native code but instead is either executed in its high-level
form or translated to an intermediate form. This is known as interpretation.
Some languages, such as BASIC, can be either compiled or interpreted. Java, in
contrast, is always interpreted. Interpretation is a form of emulation in that the
high-level language code is translated to native CPU instructions, emulating
not another CPU but a theoretical virtual machine on which that language could
run natively. Thus, we can run Java programs on “Java virtual machines,” but
technically those virtual machines are Java emulators.
With virtualization, in contrast, an operating system that is natively compiled for a particular CPU architecture runs within another operating system
also native to that CPU. Virtualization first came about on IBM mainframes
as a method for multiple users to run tasks concurrently. Running multiple
virtual machines allowed (and still allows) many users to run tasks on a system
designed for a single user. Later, in response to problems with running multiple
Microsoft Windows XP applications on the Intel x86 CPU, VMware created a
new virtualization technology in the form of an application that ran on XP.
That application ran one or more guest copies of Windows or other native
1.11 Computing Environments 41
(a)
processes
hardware
kernel
(b)
programming
interface
processes
processes
processes
kernel kernel kernel
VM1 VM3 VM2
manager
hardware
virtual machine
Figure 1.20 VMware.
x86 operating systems, each running its own applications. (See Figure 1.20.)
Windows was the host operating system, and the VMware application was the
virtual machine manager VMM. The VMM runs the guest operating systems,
manages their resource use, and protects each guest from the others.
Even though modern operating systems are fully capable of running
multiple applications reliably, the use of virtualization continues to grow. On
laptops and desktops, a VMM allows the user to install multiple operating
systems for exploration or to run applications written for operating systems
other than the native host. For example, an Apple laptop running Mac OS
X on the x86 CPU can run a Windows guest to allow execution of Windows
applications. Companies writing software for multiple operating systems
can use virtualization to run all of those operating systems on a single
physical server for development, testing, and debugging. Within data centers,
virtualization has become a common method of executing and managing
computing environments. VMMs like VMware, ESX, and Citrix XenServer no
longer run on host operating systems but rather are the hosts. Full details of
the features and implementation of virtualization are found in Chapter 16.
1.11.7 Cloud Computing
Cloud computing is a type of computing that delivers computing, storage,
and even applications as a service across a network. In some ways, it’s a
logical extension of virtualization, because it uses virtualization as a base for
its functionality. For example, the Amazon Elastic Compute Cloud (EC2)facility
has thousands of servers, millions of virtual machines, and petabytes of storage
available for use by anyone on the Internet. Users pay per month based on how
much of those resources they use.
There are actually many types of cloud computing, including the following:
• Public cloud—a cloud available via the Internet to anyone willing to pay
for the services
42 Chapter 1 Introduction
• Private cloud—a cloud run by a company for that company’s own use
• Hybrid cloud—a cloud that includes both public and private cloud
components
• Software as a service (SaaS)—one or more applications (such as word
processors or spreadsheets) available via the Internet
• Platform as a service (PaaS)—a software stack ready for application use
via the Internet (for example, a database server)
• Infrastructure as a service (IaaS)—servers or storage available over the
Internet (for example, storage available for making backup copies of
production data)
These cloud-computing types are not discrete, as a cloud computing environment may provide a combination of several types. For example, an organization
may provide both SaaS and IaaS as a publicly available service.
Certainly, there are traditional operating systems within many of the
types of cloud infrastructure. Beyond those are the VMMs that manage the
virtual machines in which the user processes run. At a higher level, the VMMs
themselves are managed by cloud management tools, such as Vware vCloud
Director and the open-source Eucalyptus toolset. These tools manage the
resources within a given cloud and provide interfaces to the cloud components,
making a good argument for considering them a new type of operating system.
Figure 1.21 illustrates a public cloud providing IaaS. Notice that both the
cloud services and the cloud user interface are protected by a firewall.
firewall
cloud
customer
interface
load balancer
virtual
machines
virtual
machines
servers servers
storage
Internet
customer
requests
cloud
management
commands
cloud
managment
services
Figure 1.21 Cloud computing.
1.12 Open-Source Operating Systems 43
1.11.8 Real-Time Embedded Systems
Embedded computers are the most prevalent form of computers in existence.
These devices are found everywhere, from car engines and manufacturing
robots to DVDs and microwave ovens. They tend to have very specific tasks.
The systems they run on are usually primitive, and so the operating systems
provide limited features. Usually, they have little or no user interface, preferring
to spend their time monitoring and managing hardware devices, such as
automobile engines and robotic arms.
These embedded systems vary considerably. Some are general-purpose
computers, running standard operating systems—such as Linux—with
special-purpose applications to implement the functionality. Others are hardware devices with a special-purpose embedded operating system providing
just the functionality desired. Yet others are hardware devices with applicationspecific integrated circuits (ASICs) that perform their tasks without an operating system.
The use of embedded systems continues to expand. The power of these
devices, both as standalone units and as elements of networks and the web,
is sure to increase as well. Even now, entire houses can be computerized, so
that a central computer—either a general-purpose computer or an embedded
system—can control heating and lighting, alarm systems, and even coffee
makers. Web access can enable a home owner to tell the house to heat up
before she arrives home. Someday, the refrigerator can notify the grocery store
when it notices the milk is gone.
Embedded systems almost always run real-time operating systems. A
real-time system is used when rigid time requirements have been placed on
the operation of a processor or the flow of data; thus, it is often used as a
control device in a dedicated application. Sensors bring data to the computer.
The computer must analyze the data and possibly adjust controls to modify
the sensor inputs. Systems that control scientific experiments, medical imaging
systems, industrial control systems, and certain display systems are realtime systems. Some automobile-engine fuel-injection systems, home-appliance
controllers, and weapon systems are also real-time systems.
A real-time system has well-defined, fixed time constraints. Processing
must be done within the defined constraints, or the system will fail. For
instance, it would not do for a robot arm to be instructed to halt after it had
smashed into the car it was building. A real-time system functions correctly
only if it returns the correct result within its time constraints. Contrast this
system with a time-sharing system, where it is desirable (but not mandatory)
to respond quickly, or a batch system, which may have no time constraints at
all.
In Chapter 6, we consider the scheduling facility needed to implement
real-time functionality in an operating system. In Chapter 9, we describe the
design of memory management for real-time computing. Finally, in Chapters
18 and 19, we describe the real-time components of the Linux and Windows 7
operating systems.""",
"5.3 Peterson’s Solution":"""Next, we illustrate a classic software-based solution to the critical-section
problem known as Peterson’s solution. Because of the way modern computer
architectures perform basic machine-language instructions, such as load and
store, there are no guarantees that Peterson’s solution will work correctly on
such architectures. However, we present the solution because it provides a good
algorithmic description of solving the critical-section problem and illustrates
some of the complexities involved in designing software that addresses the
requirements of mutual exclusion, progress, and bounded waiting.
208 Chapter 5 Process Synchronization
do {
flag[i] = true;
turn = j;
while (flag[j] && turn == j);
critical section
flag[i] = false;
remainder section
} while (true);
Figure 5.2 The structure of process Pi in Peterson’s solution.
Peterson’s solution is restricted to two processes that alternate execution
between their critical sections and remainder sections. The processes are
numbered P0 and P1. For convenience, when presenting Pi , we use Pj to
denote the other process; that is, j equals 1 − i.
Peterson’s solution requires the two processes to share two data items:
int turn;
boolean flag[2];
The variable turn indicates whose turn it is to enter its critical section. That is,
if turn == i, then process Pi is allowed to execute in its critical section. The
flag array is used to indicate if a process is ready to enter its critical section.
For example, if flag[i] is true, this value indicates that Pi is ready to enter
its critical section. With an explanation of these data structures complete, we
are now ready to describe the algorithm shown in Figure 5.2.
To enter the critical section, process Pi first sets flag[i] to be true and
then sets turn to the value j, thereby asserting that if the other process wishes
to enter the critical section, it can do so. If both processes try to enter at the same
time, turn will be set to both i and j at roughly the same time. Only one of these
assignments will last; the other will occur but will be overwritten immediately.
The eventual value of turn determines which of the two processes is allowed
to enter its critical section first.
We now prove that this solution is correct. We need to show that:
1. Mutual exclusion is preserved.
2. The progress requirement is satisfied.
3. The bounded-waiting requirement is met.
To prove property 1, we note that each Pi enters its critical section only
if either flag[j] == false or turn == i. Also note that, if both processes
can be executing in their critical sections at the same time, then flag[0] ==
flag[1] == true. These two observations imply that P0 and P1 could not have
successfully executed their while statements at about the same time, since the
5.4 Synchronization Hardware 209
value of turn can be either 0 or 1 but cannot be both. Hence, one of the processes
—say, Pj —must have successfully executed the while statement, whereas Pi
had to execute at least one additional statement (“turn == j”). However, at
that time, flag[j] == true and turn == j, and this condition will persist as
long as Pj is in its critical section; as a result, mutual exclusion is preserved.
To prove properties 2 and 3, we note that a process Pi can be prevented from
entering the critical section only if it is stuck in the while loop with the condition
flag[j] == true and turn == j; this loop is the only one possible. If Pj is not
ready to enter the critical section, then flag[j] == false, and Pi can enter its
critical section. If Pj has set flag[j] to true and is also executing in its while
statement, then either turn == i or turn == j. If turn == i, then Pi will enter
the critical section. If turn == j, then Pj will enter the critical section. However,
once Pj exits its critical section, it will reset flag[j] to false, allowing Pi to
enter its critical section. If Pj resets flag[j] to true, it must also set turn to i.
Thus, since Pi does not change the value of the variable turn while executing
the while statement, Pi will enter the critical section (progress) after at most
one entry by Pj (bounded waiting).""",
"5.5 Mutex Locks":"""The hardware-based solutions to the critical-section problem presented in
Section 5.4 are complicated as well as generally inaccessible to application
programmers. Instead, operating-systems designers build software tools to
solve the critical-section problem. The simplest of these tools is the mutex
lock. (In fact, the term mutex is short for mutual exclusion.) We use the mutex
lock to protect critical regions and thus prevent race conditions. That is, a
process must acquire the lock before entering a critical section; it releases the
lock when it exits the critical section. The acquire()function acquires the lock,
and the release() function releases the lock, as illustrated in Figure 5.8.
A mutex lock has a boolean variable available whose value indicates if
the lock is available or not. If the lock is available, a call to acquire() succeeds,
and the lock is then considered unavailable. A process that attempts to acquire
an unavailable lock is blocked until the lock is released.
The definition of acquire() is as follows:
acquire() {
while (!available)
; /* busy wait */
available = false;;
}
5.6 Semaphores 213
do {
acquire lock
critical section
release lock
remainder section
} while (true);
Figure 5.8 Solution to the critical-section problem using mutex locks.
The definition of release() is as follows:
release() {
available = true;
}
Calls to either acquire() or release() must be performed atomically.
Thus, mutex locks are often implemented using one of the hardware mechanisms described in Section 5.4, and we leave the description of this technique
as an exercise.
The main disadvantage of the implementation given here is that it requires
busy waiting. While a process is in its critical section, any other process that
tries to enter its critical section must loop continuously in the call to acquire().
In fact, this type of mutex lock is also called a spinlock because the process
“spins” while waiting for the lock to become available. (We see the same issue
with the code examples illustrating the test and set() instruction and the
compare and swap() instruction.) This continual looping is clearly a problem
in a real multiprogramming system, where a single CPU is shared among many
processes. Busy waiting wastes CPU cycles that some other process might be
able to use productively.
Spinlocks do have an advantage, however, in that no context switch is
required when a process must wait on a lock, and a context switch may
take considerable time. Thus, when locks are expected to be held for short
times, spinlocks are useful. They are often employed on multiprocessor systems
where one thread can “spin” on one processor while another thread performs
its critical section on another processor.
Later in this chapter (Section 5.7), we examine how mutex locks can be
used to solve classical synchronization problems. We also discuss how these
locks are used in several operating systems, as well as in Pthreads.""",
"1.3 Computer-System Architecture":"""In Section 1.2, we introduced the general structure of a typical computer system.
A computer system can be organized in a number of different ways, which we
1.3 Computer-System Architecture 13
thread of execution
instructions
and
data
instruction execution
cycle
data movement
DMA
memory
interrupt
cache
data
I/O request
CPU (*N)
device
(*M)
Figure 1.5 How a modern computer system works.
can categorize roughly according to the number of general-purpose processors
used.
1.3.1 Single-Processor Systems
Until recently, most computer systems used a single processor. On a singleprocessor system, there is one main CPU capable of executing a general-purpose
instruction set, including instructions from user processes. Almost all singleprocessor systems have other special-purpose processors as well. They may
come in the form of device-specific processors, such as disk, keyboard, and
graphics controllers; or, on mainframes, they may come in the form of more
general-purpose processors, such as I/O processors that move data rapidly
among the components of the system.
All of these special-purpose processors run a limited instruction set and
do not run user processes. Sometimes, they are managed by the operating
system, in that the operating system sends them information about their next
task and monitors their status. For example, a disk-controller microprocessor
receives a sequence of requests from the main CPU and implements its own disk
queue and scheduling algorithm. This arrangement relieves the main CPU of
the overhead of disk scheduling. PCs contain a microprocessor in the keyboard
to convert the keystrokes into codes to be sent to the CPU. In other systems
or circumstances, special-purpose processors are low-level components built
into the hardware. The operating system cannot communicate with these
processors; they do their jobs autonomously. The use of special-purpose
microprocessors is common and does not turn a single-processor system into
14 Chapter 1 Introduction
a multiprocessor. If there is only one general-purpose CPU, then the system is
a single-processor system.
1.3.2 Multiprocessor Systems
Within the past several years, multiprocessor systems (also known as parallel
systems or multicore systems) have begun to dominate the landscape of
computing. Such systems have two or more processors in close communication,
sharing the computer bus and sometimes the clock, memory, and peripheral
devices. Multiprocessor systems first appeared prominently appeared in
servers and have since migrated to desktop and laptop systems. Recently,
multiple processors have appeared on mobile devices such as smartphones
and tablet computers.
Multiprocessor systems have three main advantages:
1. Increased throughput. By increasing the number of processors, we expect
to get more work done in less time. The speed-up ratio with N processors
is not N, however; rather, it is less than N. When multiple processors
cooperate on a task, a certain amount of overhead is incurred in keeping
all the parts working correctly. This overhead, plus contention for shared
resources, lowers the expected gain from additional processors. Similarly,
N programmers working closely together do not produce N times the
amount of work a single programmer would produce.
2. Economy of scale. Multiprocessor systems can cost less than equivalent
multiple single-processor systems, because they can share peripherals,
mass storage, and power supplies. If several programs operate on the
same set of data, it is cheaper to store those data on one disk and to have
all the processors share them than to have many computers with local
disks and many copies of the data.
3. Increased reliability. If functions can be distributed properly among
several processors, then the failure of one processor will not halt the
system, only slow it down. If we have ten processors and one fails, then
each of the remaining nine processors can pick up a share of the work of
the failed processor. Thus, the entire system runs only 10 percent slower,
rather than failing altogether.
Increased reliability of a computer system is crucial in many applications.
The ability to continue providing service proportional to the level of surviving
hardware is called graceful degradation. Some systems go beyond graceful
degradation and are called fault tolerant, because they can suffer a failure of
any single component and still continue operation. Fault tolerance requires
a mechanism to allow the failure to be detected, diagnosed, and, if possible,
corrected. The HP NonStop (formerly Tandem) system uses both hardware and
software duplication to ensure continued operation despite faults. The system
consists of multiple pairs of CPUs, working in lockstep. Both processors in the
pair execute each instruction and compare the results. If the results differ, then
one CPU of the pair is at fault, and both are halted. The process that was being
executed is then moved to another pair of CPUs, and the instruction that failed
1.3 Computer-System Architecture 15
is restarted. This solution is expensive, since it involves special hardware and
considerable hardware duplication.
The multiple-processor systems in use today are of two types. Some
systems use asymmetric multiprocessing, in which each processor is assigned
a specific task. A boss processor controls the system; the other processors either
look to the boss for instruction or have predefined tasks. This scheme defines
a boss–worker relationship. The boss processor schedules and allocates work
to the worker processors.
The most common systems use symmetric multiprocessing (SMP), in
which each processor performs all tasks within the operating system. SMP
means that all processors are peers; no boss–worker relationship exists
between processors. Figure 1.6 illustrates a typical SMP architecture. Notice
that each processor has its own set of registers, as well as a private—or local
—cache. However, all processors share physical memory. An example of an
SMP system is AIX, a commercial version of UNIX designed by IBM. An AIX
system can be configured to employ dozens of processors. The benefit of this
model is that many processes can run simultaneously—N processes can run
if there are N CPUs—without causing performance to deteriorate significantly.
However, we must carefully control I/O to ensure that the data reach the
appropriate processor. Also, since the CPUs are separate, one may be sitting
idle while another is overloaded, resulting in inefficiencies. These inefficiencies
can be avoided if the processors share certain data structures. A multiprocessor
system of this form will allow processes and resources—such as memory—
to be shared dynamically among the various processors and can lower the
variance among the processors. Such a system must be written carefully, as
we shall see in Chapter 5. Virtually all modern operating systems—including
Windows, Mac OS X, and Linux—now provide support for SMP.
The difference between symmetric and asymmetric multiprocessing may
result from either hardware or software. Special hardware can differentiate the
multiple processors, or the software can be written to allow only one boss and
multiple workers. For instance, Sun Microsystems’ operating system SunOS
Version 4 provided asymmetric multiprocessing, whereas Version 5 (Solaris) is
symmetric on the same hardware.
Multiprocessing adds CPUs to increase computing power. If the CPU has an
integrated memory controller, then adding CPUs can also increase the amount
CPU0
registers
cache
CPU1
registers
cache
CPU2
registers
cache
memory
Figure 1.6 Symmetric multiprocessing architecture.
16 Chapter 1 Introduction
of memory addressable in the system. Either way, multiprocessing can cause
a system to change its memory access model from uniform memory access
(UMA) to non-uniform memory access (NUMA). UMA is defined as the situation
in which access to any RAM from any CPU takes the same amount of time. With
NUMA, some parts of memory may take longer to access than other parts,
creating a performance penalty. Operating systems can minimize the NUMA
penalty through resource management, as discussed in Section 9.5.4.
A recent trend in CPU design is to include multiple computing cores
on a single chip. Such multiprocessor systems are termed multicore. They
can be more efficient than multiple chips with single cores because on-chip
communication is faster than between-chip communication. In addition, one
chip with multiple cores uses significantly less power than multiple single-core
chips.
It is important to note that while multicore systems are multiprocessor
systems, not all multiprocessor systems are multicore, as we shall see in Section
1.3.3. In our coverage of multiprocessor systems throughout this text, unless
we state otherwise, we generally use the more contemporary term multicore,
which excludes some multiprocessor systems.
In Figure 1.7, we show a dual-core design with two cores on the same
chip. In this design, each core has its own register set as well as its own local
cache. Other designs might use a shared cache or a combination of local and
shared caches. Aside from architectural considerations, such as cache, memory,
and bus contention, these multicore CPUs appear to the operating system as
N standard processors. This characteristic puts pressure on operating system
designers—and application programmers—to make use of those processing
cores.
Finally, blade servers are a relatively recent development in which multiple
processor boards, I/O boards, and networking boards are placed in the same
chassis. The difference between these and traditional multiprocessor systems
is that each blade-processor board boots independently and runs its own
operating system. Some blade-server boards are multiprocessor as well, which
blurs the lines between types of computers. In essence, these servers consist of
multiple independent multiprocessor systems.
CPU core0
registers
cache
CPU core1
registers
cache
memory
Figure 1.7 A dual-core design with two cores placed on the same chip.
1.3 Computer-System Architecture 17
1.3.3 Clustered Systems
Another type of multiprocessor system is a clustered system, which gathers
together multiple CPUs. Clustered systems differ from the multiprocessor
systems described in Section 1.3.2 in that they are composed of two or more
individual systems—or nodes—joined together. Such systems are considered
loosely coupled. Each node may be a single processor system or a multicore
system. We should note that the definition of clustered is not concrete; many
commercial packages wrestle to define a clustered system and why one form
is better than another. The generally accepted definition is that clustered
computers share storage and are closely linked via a local-area network LAN
(as described in Chapter 17) or a faster interconnect, such as InfiniBand.
Clustering is usually used to provide high-availability service—that is,
service will continue even if one or more systems in the cluster fail. Generally,
we obtain high availability by adding a level of redundancy in the system.
A layer of cluster software runs on the cluster nodes. Each node can monitor
one or more of the others (over the LAN). If the monitored machine fails,
the monitoring machine can take ownership of its storage and restart the
applications that were running on the failed machine. The users and clients of
the applications see only a brief interruption of service.
Clustering can be structured asymmetrically or symmetrically. In asymmetric clustering, one machine is in hot-standby mode while the other is
running the applications. The hot-standby host machine does nothing but
monitor the active server. If that server fails, the hot-standby host becomes
the active server. In symmetric clustering, two or more hosts are running
applications and are monitoring each other. This structure is obviously more
efficient, as it uses all of the available hardware. However it does require that
more than one application be available to run.
Since a cluster consists of several computer systems connected via a
network, clusters can also be used to provide high-performance computing
environments. Such systems can supply significantly greater computational
power than single-processor or even SMP systems because they can run an
application concurrently on all computers in the cluster. The application must
have been written specifically to take advantage of the cluster, however. This
involves a technique known as parallelization, which divides a program into
separate components that run in parallel on individual computers in the cluster.
Typically, these applications are designed so that once each computing node in
the cluster has solved its portion of the problem, the results from all the nodes
are combined into a final solution.
Other forms of clusters include parallel clusters and clustering over a
wide-area network (WAN) (as described in Chapter 17). Parallel clusters allow
multiple hosts to access the same data on shared storage. Because most
operating systems lack support for simultaneous data access by multiple hosts,
parallel clusters usually require the use of special versions of software and
special releases of applications. For example, Oracle Real Application Cluster
is a version of Oracle’s database that has been designed to run on a parallel
cluster. Each machine runs Oracle, and a layer of software tracks access to the
shared disk. Each machine has full access to all data in the database. To provide
this shared access, the system must also supply access control and locking to
18 Chapter 1 Introduction
BEOWULF CLUSTERS
Beowulf clusters are designed to solve high-performance computing tasks.
A Beowulf cluster consists of commodity hardware—such as personal
computers—connected via a simple local-area network. No single specific
software package is required to construct a cluster. Rather, the nodes use a
set of open-source software libraries to communicate with one another. Thus,
there are a variety of approaches to constructing a Beowulf cluster. Typically,
though, Beowulf computing nodes run the Linux operating system. Since
Beowulf clusters require no special hardware and operate using open-source
software that is available free, they offer a low-cost strategy for building
a high-performance computing cluster. In fact, some Beowulf clusters built
from discarded personal computers are using hundreds of nodes to solve
computationally expensive scientific computing problems.
ensure that no conflicting operations occur. This function, commonly known
as a distributed lock manager (DLM), is included in some cluster technology.
Cluster technology is changing rapidly. Some cluster products support
dozens of systems in a cluster, as well as clustered nodes that are separated
by miles. Many of these improvements are made possible by storage-area
networks (SANs), as described in Section 10.3.3, which allow many systems
to attach to a pool of storage. If the applications and their data are stored on
the SAN, then the cluster software can assign the application to run on any
host that is attached to the SAN. If the host fails, then any other host can take
over. In a database cluster, dozens of hosts can share the same database, greatly
increasing performance and reliability. Figure 1.8 depicts the general structure
of a clustered system.
computer
interconnect
computer
interconnect
computer
storage area
network
Figure 1.8 General structure of a clustered system.
1.4 Operating-System Structure 19
job 1
0
Max
operating system
job 2
job 3
job 4
Figure 1.9 Memory layout for a multiprogramming system.""",
"7.2 Deadlock Characterization":"""In a deadlock, processes never finish executing, and system resources are tied
up, preventing other jobs from starting. Before we discuss the various methods
for dealing with the deadlock problem, we look more closely at features that
characterize deadlocks.
DEADLOCK WITH MUTEX LOCKS
Let’s see how deadlock can occur in a multithreaded Pthread program
using mutex locks. The pthread mutex init() function initializes
an unlocked mutex. Mutex locks are acquired and released using
pthread mutex lock() and pthread mutex unlock(), respectively. If a thread attempts to acquire a locked mutex, the call to
pthread mutex lock() blocks the thread until the owner of the mutex
lock invokes pthread mutex unlock().
Two mutex locks are created in the following code example:
/* Create and initialize the mutex locks */
pthread mutex t first mutex;
pthread mutex t second mutex;
pthread mutex init(&first mutex,NULL);
pthread mutex init(&second mutex,NULL);
Next, two threads—thread one and thread two—are created, and both
these threads have access to both mutex locks. thread one and thread two
318 Chapter 7 Deadlocks
DEADLOCK WITH MUTEX LOCKS (Continued)
run in the functions do work one() and do work two(), respectively, as
shown below:
/* thread one runs in this function */
void *do work one(void *param)
{
pthread mutex lock(&first mutex);
pthread mutex lock(&second mutex);
/**
* Do some work
*/
pthread mutex unlock(&second mutex);
pthread mutex unlock(&first mutex);
pthread exit(0);
}
/* thread two runs in this function */
void *do work two(void *param)
{
pthread mutex lock(&second mutex);
pthread mutex lock(&first mutex);
/**
* Do some work
*/
pthread mutex unlock(&first mutex);
pthread mutex unlock(&second mutex);
pthread exit(0);
}
In this example, thread one attempts to acquire the mutex locks in the order
(1) first mutex, (2) second mutex, while thread two attempts to acquire
the mutex locks in the order (1) second mutex, (2) first mutex. Deadlock
is possible if thread one acquires first mutex while thread two acquires
second mutex.
Note that, even though deadlock is possible, it will not occur if thread one
can acquire and release the mutex locks for first mutex and second mutex
before thread two attempts to acquire the locks. And, of course, the order
in which the threads run depends on how they are scheduled by the CPU
scheduler. This example illustrates a problem with handling deadlocks: it is
difficult to identify and test for deadlocks that may occur only under certain
scheduling circumstances.
7.2.1 Necessary Conditions
A deadlock situation can arise if the following four conditions hold simultaneously in a system:
7.2 Deadlock Characterization 319
1. Mutual exclusion. At least one resource must be held in a nonsharable
mode; that is, only one process at a time can use the resource. If another
process requests that resource, the requesting process must be delayed
until the resource has been released.
2. Hold and wait. A process must be holding at least one resource and
waiting to acquire additional resources that are currently being held by
other processes.
3. No preemption. Resources cannot be preempted; that is, a resource can
be released only voluntarily by the process holding it, after that process
has completed its task.
4. Circular wait. A set {P0, P1, ..., Pn} of waiting processes must exist such
that P0 is waiting for a resource held by P1, P1 is waiting for a resource
held by P2, ..., Pn−1 is waiting for a resource held by Pn, and Pn is waiting
for a resource held by P0.
We emphasize that all four conditions must hold for a deadlock to
occur. The circular-wait condition implies the hold-and-wait condition, so the
four conditions are not completely independent. We shall see in Section 7.4,
however, that it is useful to consider each condition separately.
7.2.2 Resource-Allocation Graph
Deadlocks can be described more precisely in terms of a directed graph called
a system resource-allocation graph. This graph consists of a set of vertices V
and a set of edges E. The set of vertices V is partitioned into two different types
of nodes: P = {P1, P2, ..., Pn}, the set consisting of all the active processes in the
system, and R = {R1, R2, ..., Rm}, the set consisting of all resource types in the
system.
A directed edge from process Pi to resource type Rj is denoted by Pi → Rj ;
it signifies that process Pi has requested an instance of resource type Rj and
is currently waiting for that resource. A directed edge from resource type Rj
to process Pi is denoted by Rj → Pi ; it signifies that an instance of resource
type Rj has been allocated to process Pi . A directed edge Pi → Rj is called a
request edge; a directed edge Rj → Pi is called an assignment edge.
Pictorially, we represent each process Pi as a circle and each resource type
Rj as a rectangle. Since resource type Rj may have more than one instance, we
represent each such instance as a dot within the rectangle. Note that a request
edge points to only the rectangle Rj , whereas an assignment edge must also
designate one of the dots in the rectangle.
When process Pi requests an instance of resource type Rj , a request edge
is inserted in the resource-allocation graph. When this request can be fulfilled,
the request edge is instantaneously transformed to an assignment edge. When
the process no longer needs access to the resource, it releases the resource. As
a result, the assignment edge is deleted.
The resource-allocation graph shown in Figure 7.1 depicts the following
situation.
• The sets P, R, and E:
◦ P = {P1, P2, P3}
320 Chapter 7 Deadlocks
R1 R3
R2
R4
P1 P2 P3
Figure 7.1 Resource-allocation graph.
◦ R = {R1, R2, R3, R4}
◦ E = {P1 → R1, P2 → R3, R1 → P2, R2 → P2, R2 → P1, R3 → P3}
• Resource instances:
◦ One instance of resource type R1
◦ Two instances of resource type R2
◦ One instance of resource type R3
◦ Three instances of resource type R4
• Process states:
◦ Process P1 is holding an instance of resource type R2 and is waiting for
an instance of resource type R1.
◦ Process P2 is holding an instance of R1 and an instance of R2 and is
waiting for an instance of R3.
◦ Process P3 is holding an instance of R3.
Given the definition of a resource-allocation graph, it can be shown that, if
the graph contains no cycles, then no process in the system is deadlocked. If
the graph does contain a cycle, then a deadlock may exist.
If each resource type has exactly one instance, then a cycle implies that a
deadlock has occurred. If the cycle involves only a set of resource types, each
of which has only a single instance, then a deadlock has occurred. Each process
involved in the cycle is deadlocked. In this case, a cycle in the graph is both a
necessary and a sufficient condition for the existence of deadlock.
If each resource type has several instances, then a cycle does not necessarily
imply that a deadlock has occurred. In this case, a cycle in the graph is a
necessary but not a sufficient condition for the existence of deadlock.
To illustrate this concept, we return to the resource-allocation graph
depicted in Figure 7.1. Suppose that process P3 requests an instance of resource
7.2 Deadlock Characterization 321
R1 R3
R2
R4
P1 P2 P3
Figure 7.2 Resource-allocation graph with a deadlock.
type R2. Since no resource instance is currently available, we add a request edge
P3 → R2 to the graph (Figure 7.2). At this point, two minimal cycles exist in the
system:
P1 → R1 → P2 → R3 → P3 → R2 → P1
P2 → R3 → P3 → R2 → P2
Processes P1, P2, and P3 are deadlocked. Process P2 is waiting for the resource
R3, which is held by process P3. Process P3 is waiting for either process P1 or
process P2 to release resource R2. In addition, process P1 is waiting for process
P2 to release resource R1.
Now consider the resource-allocation graph in Figure 7.3. In this example,
we also have a cycle:
P1 → R1 → P3 → R2 → P1
R2
R1
P3
P4
P2
P1
Figure 7.3 Resource-allocation graph with a cycle but no deadlock.
322 Chapter 7 Deadlocks
However, there is no deadlock. Observe that process P4 may release its instance
of resource type R2. That resource can then be allocated to P3, breaking the cycle.
In summary, if a resource-allocation graph does not have a cycle, then the
system is not in a deadlocked state. If there is a cycle, then the system may or
may not be in a deadlocked state. This observation is important when we deal
with the deadlock problem.""",
"7.3 Methods for Handling Deadlocks":"""Generally speaking, we can deal with the deadlock problem in one of three
ways:
• We can use a protocol to prevent or avoid deadlocks, ensuring that the
system will never enter a deadlocked state.
• We can allow the system to enter a deadlocked state, detect it, and recover.
• We can ignore the problem altogether and pretend that deadlocks never
occur in the system.
The third solution is the one used by most operating systems, including Linux
and Windows. It is then up to the application developer to write programs that
handle deadlocks.
Next, we elaborate briefly on each of the three methods for handling
deadlocks. Then, in Sections 7.4 through 7.7, we present detailed algorithms.
Before proceeding, we should mention that some researchers have argued that
none of the basic approaches alone is appropriate for the entire spectrum of
resource-allocation problems in operating systems. The basic approaches can
be combined, however, allowing us to select an optimal approach for each class
of resources in a system.
To ensure that deadlocks never occur, the system can use either a deadlockprevention or a deadlock-avoidance scheme. Deadlock prevention provides a
set of methods to ensure that at least one of the necessary conditions (Section
7.2.1) cannot hold. These methods prevent deadlocks by constraining how
requests for resources can be made. We discuss these methods in Section 7.4.
Deadlock avoidance requires that the operating system be given additional
information in advance concerning which resources a process will request
and use during its lifetime. With this additional knowledge, the operating
system can decide for each request whether or not the process should wait.
To decide whether the current request can be satisfied or must be delayed, the
system must consider the resources currently available, the resources currently
allocated to each process, and the future requests and releases of each process.
We discuss these schemes in Section 7.5.
If a system does not employ either a deadlock-prevention or a deadlockavoidance algorithm, then a deadlock situation may arise. In this environment,
the system can provide an algorithm that examines the state of the system to
determine whether a deadlock has occurred and an algorithm to recover from
the deadlock (if a deadlock has indeed occurred). We discuss these issues in
Section 7.6 and Section 7.7.
7.4 Deadlock Prevention 323
In the absence of algorithms to detect and recover from deadlocks, we may
arrive at a situation in which the system is in a deadlocked state yet has no
way of recognizing what has happened. In this case, the undetected deadlock
will cause the system’s performance to deteriorate, because resources are being
held by processes that cannot run and because more and more processes, as
they make requests for resources, will enter a deadlocked state. Eventually, the
system will stop functioning and will need to be restarted manually.
Although this method may not seem to be a viable approach to the deadlock
problem, it is nevertheless used in most operating systems, as mentioned
earlier. Expense is one important consideration. Ignoring the possibility of
deadlocks is cheaper than the other approaches. Since in many systems,
deadlocks occur infrequently (say, once per year), the extra expense of the
other methods may not seem worthwhile. In addition, methods used to recover
from other conditions may be put to use to recover from deadlock. In some
circumstances, a system is in a frozen state but not in a deadlocked state.
We see this situation, for example, with a real-time process running at the
highest priority (or any process running on a nonpreemptive scheduler) and
never returning control to the operating system. The system must have manual
recovery methods for such conditions and may simply use those techniques
for deadlock recovery.""",
"7.4 Deadlock Prevention":"""As we noted in Section 7.2.1, for a deadlock to occur, each of the four necessary
conditions must hold. By ensuring that at least one of these conditions cannot
hold, we can prevent the occurrence of a deadlock. We elaborate on this
approach by examining each of the four necessary conditions separately.
7.4.1 Mutual Exclusion
The mutual exclusion condition must hold. That is, at least one resource must be
nonsharable. Sharable resources, in contrast, do not require mutually exclusive
access and thus cannot be involved in a deadlock. Read-only files are a good
example of a sharable resource. If several processes attempt to open a read-only
file at the same time, they can be granted simultaneous access to the file. A
process never needs to wait for a sharable resource. In general, however, we
cannot prevent deadlocks by denying the mutual-exclusion condition, because
some resources are intrinsically nonsharable. For example, a mutex lock cannot
be simultaneously shared by several processes.
7.4.2 Hold and Wait
To ensure that the hold-and-wait condition never occurs in the system, we must
guarantee that, whenever a process requests a resource, it does not hold any
other resources. One protocol that we can use requires each process to request
and be allocated all its resources before it begins execution. We can implement
this provision by requiring that system calls requesting resources for a process
precede all other system calls.
324 Chapter 7 Deadlocks
An alternative protocol allows a process to request resources only when
it has none. A process may request some resources and use them. Before it
can request any additional resources, it must release all the resources that it is
currently allocated.
To illustrate the difference between these two protocols, we consider a
process that copies data from a DVD drive to a file on disk, sorts the file, and
then prints the results to a printer. If all resources must be requested at the
beginning of the process, then the process must initially request the DVD drive,
disk file, and printer. It will hold the printer for its entire execution, even though
it needs the printer only at the end.
The second method allows the process to request initially only the DVD
drive and disk file. It copies from the DVD drive to the disk and then releases
both the DVD drive and the disk file. The process must then request the disk
file and the printer. After copying the disk file to the printer, it releases these
two resources and terminates.
Both these protocols have two main disadvantages. First, resource utilization may be low, since resources may be allocated but unused for a long period.
In the example given, for instance, we can release the DVD drive and disk file,
and then request the disk file and printer, only if we can be sure that our data
will remain on the disk file. Otherwise, we must request all resources at the
beginning for both protocols.
Second, starvation is possible. A process that needs several popular
resources may have to wait indefinitely, because at least one of the resources
that it needs is always allocated to some other process.
7.4.3 No Preemption
The third necessary condition for deadlocks is that there be no preemption
of resources that have already been allocated. To ensure that this condition
does not hold, we can use the following protocol. If a process is holding
some resources and requests another resource that cannot be immediately
allocated to it (that is, the process must wait), then all resources the process is
currently holding are preempted. In other words, these resources are implicitly
released. The preempted resources are added to the list of resources for which
the process is waiting. The process will be restarted only when it can regain its
old resources, as well as the new ones that it is requesting.
Alternatively, if a process requests some resources, we first check whether
they are available. If they are, we allocate them. If they are not, we check
whether they are allocated to some other process that is waiting for additional
resources. If so, we preempt the desired resources from the waiting process and
allocate them to the requesting process. If the resources are neither available
nor held by a waiting process, the requesting process must wait. While it is
waiting, some of its resources may be preempted, but only if another process
requests them. A process can be restarted only when it is allocated the new
resources it is requesting and recovers any resources that were preempted
while it was waiting.
This protocol is often applied to resources whose state can be easily saved
and restored later, such as CPU registers and memory space. It cannot generally
be applied to such resources as mutex locks and semaphores.
7.4 Deadlock Prevention 325
7.4.4 Circular Wait
The fourth and final condition for deadlocks is the circular-wait condition. One
way to ensure that this condition never holds is to impose a total ordering of
all resource types and to require that each process requests resources in an
increasing order of enumeration.
To illustrate, we let R = {R1, R2, ..., Rm} be the set of resource types. We
assign to each resource type a unique integer number, which allows us to
compare two resources and to determine whether one precedes another in our
ordering. Formally, we define a one-to-one function F: R → N, where N is the
set of natural numbers. For example, if the set of resource types R includes
tape drives, disk drives, and printers, then the function F might be defined as
follows:
F(tape drive) = 1
F(disk drive) = 5
F(printer) = 12
We can now consider the following protocol to prevent deadlocks: Each
process can request resources only in an increasing order of enumeration. That
is, a process can initially request any number of instances of a resource type
—say, Ri . After that, the process can request instances of resource type Rj if
and only if F(Rj) > F(Ri). For example, using the function defined previously,
a process that wants to use the tape drive and printer at the same time must
first request the tape drive and then request the printer. Alternatively, we can
require that a process requesting an instance of resource type Rj must have
released any resources Ri such that F(Ri) ≥ F(Rj). Note also that if several
instances of the same resource type are needed, a single request for all of them
must be issued.
If these two protocols are used, then the circular-wait condition cannot
hold. We can demonstrate this fact by assuming that a circular wait exists
(proof by contradiction). Let the set of processes involved in the circular wait be
{P0, P1, ..., Pn}, where Pi is waiting for a resource Ri , which is held by process
Pi+1. (Modulo arithmetic is used on the indexes, so that Pn is waiting for
a resource Rn held by P0.) Then, since process Pi+1 is holding resource Ri
while requesting resource Ri+1, we must have F(Ri) < F(Ri+1) for all i. But
this condition means that F(R0) < F(R1) < ... < F(Rn) < F(R0). By transitivity,
F(R0) < F(R0), which is impossible. Therefore, there can be no circular wait.
We can accomplish this scheme in an application program by developing
an ordering among all synchronization objects in the system. All requests for
synchronization objects must be made in increasing order. For example, if the
lock ordering in the Pthread program shown in Figure 7.4 was
F(first mutex)=1
F(second mutex)=5
then thread two could not request the locks out of order.
Keep in mind that developing an ordering, or hierarchy, does not in itself
prevent deadlock. It is up to application developers to write programs that
follow the ordering. Also note that the function F should be defined according
to the normal order of usage of the resources in a system. For example, because
326 Chapter 7 Deadlocks
/* thread one runs in this function */
void *do work one(void *param)
{
pthread mutex lock(&first mutex);
pthread mutex lock(&second mutex);
/**
* Do some work
*/
pthread mutex unlock(&second mutex);
pthread mutex unlock(&first mutex);
pthread exit(0);
}
/* thread two runs in this function */
void *do work two(void *param)
{
pthread mutex lock(&second mutex);
pthread mutex lock(&first mutex);
/**
* Do some work
*/
pthread mutex unlock(&first mutex);
pthread mutex unlock(&second mutex);
pthread exit(0);
}
Figure 7.4 Deadlock example.
the tape drive is usually needed before the printer, it would be reasonable to
define F(tape drive) < F(printer).
Although ensuring that resources are acquired in the proper order is the
responsibility of application developers, certain software can be used to verify
that locks are acquired in the proper order and to give appropriate warnings
when locks are acquired out of order and deadlock is possible. One lock-order
verifier, which works on BSD versions of UNIX such as FreeBSD, is known as
witness. Witness uses mutual-exclusion locks to protect critical sections, as
described in Chapter 5. It works by dynamically maintaining the relationship
of lock orders in a system. Let’s use the program shown in Figure 7.4 as an
example. Assume that thread one is the first to acquire the locks and does so in
the order (1) first mutex, (2) second mutex. Witness records the relationship
that first mutex must be acquired before second mutex. If thread two later
acquires the locks out of order, witness generates a warning message on the
system console.
It is also important to note that imposing a lock ordering does not guarantee
deadlock prevention if locks can be acquired dynamically. For example, assume
we have a function that transfers funds between two accounts. To prevent a
race condition, each account has an associated mutex lock that is obtained from
a get lock() function such as shown in Figure 7.5:
7.5 Deadlock Avoidance 327
void transaction(Account from, Account to, double amount)
{
mutex lock1, lock2;
lock1 = get lock(from);
lock2 = get lock(to);
acquire(lock1);
acquire(lock2);
withdraw(from, amount);
deposit(to, amount);
release(lock2);
release(lock1);
}
Figure 7.5 Deadlock example with lock ordering.
Deadlock is possible if two threads simultaneously invoke the transaction()
function, transposing different accounts. That is, one thread might invoke
transaction(checking account, savings account, 25);
and another might invoke
transaction(savings account, checking account, 50);
We leave it as an exercise for students to fix this situation.""",
"7.5 Deadlock Avoidance":"""Deadlock-prevention algorithms, as discussed in Section 7.4, prevent deadlocks
by limiting how requests can be made. The limits ensure that at least one of
the necessary conditions for deadlock cannot occur. Possible side effects of
preventing deadlocks by this method, however, are low device utilization and
reduced system throughput.
An alternative method for avoiding deadlocks is to require additional
information about how resources are to be requested. For example, in a system
with one tape drive and one printer, the system might need to know that
process P will request first the tape drive and then the printer before releasing
both resources, whereas process Q will request first the printer and then the
tape drive. With this knowledge of the complete sequence of requests and
releases for each process, the system can decide for each request whether or
not the process should wait in order to avoid a possible future deadlock. Each
request requires that in making this decision the system consider the resources
currently available, the resources currently allocated to each process, and the
future requests and releases of each process.
The various algorithms that use this approach differ in the amount and
type of information required. The simplest and most useful model requires
that each process declare the maximum number of resources of each type that
it may need. Given this a priori information, it is possible to construct an
328 Chapter 7 Deadlocks
algorithm that ensures that the system will never enter a deadlocked state. A
deadlock-avoidance algorithm dynamically examines the resource-allocation
state to ensure that a circular-wait condition can never exist. The resourceallocation state is defined by the number of available and allocated resources
and the maximum demands of the processes. In the following sections, we
explore two deadlock-avoidance algorithms.
7.5.1 Safe State
A state is safe if the system can allocate resources to each process (up to its
maximum) in some order and still avoid a deadlock. More formally, a system
is in a safe state only if there exists a safe sequence. A sequence of processes
<P1, P2, ..., Pn> is a safe sequence for the current allocation state if, for each
Pi , the resource requests that Pi can still make can be satisfied by the currently
available resources plus the resources held by all Pj , with j < i. In this situation,
if the resources that Pi needs are not immediately available, then Pi can wait
until all Pj have finished. When they have finished, Pi can obtain all of its
needed resources, complete its designated task, return its allocated resources,
and terminate. When Pi terminates, Pi+1 can obtain its needed resources, and
so on. If no such sequence exists, then the system state is said to be unsafe.
A safe state is not a deadlocked state. Conversely, a deadlocked state is
an unsafe state. Not all unsafe states are deadlocks, however (Figure 7.6).
An unsafe state may lead to a deadlock. As long as the state is safe, the
operating system can avoid unsafe (and deadlocked) states. In an unsafe state,
the operating system cannot prevent processes from requesting resources in
such a way that a deadlock occurs. The behavior of the processes controls
unsafe states.
To illustrate, we consider a system with twelve magnetic tape drives and
three processes: P0, P1, and P2. Process P0 requires ten tape drives, process P1
may need as many as four tape drives, and process P2 may need up to nine tape
drives. Suppose that, at time t0, process P0 is holding five tape drives, process
P1 is holding two tape drives, and process P2 is holding two tape drives. (Thus,
there are three free tape drives.)
deadlock
unsafe
safe
Figure 7.6 Safe, unsafe, and deadlocked state spaces.
7.5 Deadlock Avoidance 329
Maximum Needs Current Needs
P0 10 5
P1 4 2
P2 9 2
At time t0, the system is in a safe state. The sequence <P1, P0, P2> satisfies
the safety condition. Process P1 can immediately be allocated all its tape drives
and then return them (the system will then have five available tape drives);
then process P0 can get all its tape drives and return them (the system will then
have ten available tape drives); and finally process P2 can get all its tape drives
and return them (the system will then have all twelve tape drives available).
A system can go from a safe state to an unsafe state. Suppose that, at time
t1, process P2 requests and is allocated one more tape drive. The system is no
longer in a safe state. At this point, only process P1 can be allocated all its tape
drives. When it returns them, the system will have only four available tape
drives. Since process P0 is allocated five tape drives but has a maximum of ten,
it may request five more tape drives. If it does so, it will have to wait, because
they are unavailable. Similarly, process P2 may request six additional tape
drives and have to wait, resulting in a deadlock. Our mistake was in granting
the request from process P2 for one more tape drive. If we had made P2 wait
until either of the other processes had finished and released its resources, then
we could have avoided the deadlock.
Given the concept of a safe state, we can define avoidance algorithms that
ensure that the system will never deadlock. The idea is simply to ensure that the
system will always remain in a safe state. Initially, the system is in a safe state.
Whenever a process requests a resource that is currently available, the system
must decide whether the resource can be allocated immediately or whether
the process must wait. The request is granted only if the allocation leaves the
system in a safe state.
In this scheme, if a process requests a resource that is currently available,
it may still have to wait. Thus, resource utilization may be lower than it would
otherwise be.
7.5.2 Resource-Allocation-Graph Algorithm
If we have a resource-allocation system with only one instance of each resource
type, we can use a variant of the resource-allocation graph defined in Section
7.2.2 for deadlock avoidance. In addition to the request and assignment edges
already described, we introduce a new type of edge, called a claim edge.
A claim edge Pi → Rj indicates that process Pi may request resource Rj at
some time in the future. This edge resembles a request edge in direction but is
represented in the graph by a dashed line. When process Pi requests resource
Rj , the claim edge Pi → Rj is converted to a request edge. Similarly, when a
resource Rj is released by Pi , the assignment edge Rj → Pi is reconverted to a
claim edge Pi → Rj .
Note that the resources must be claimed a priori in the system. That is,
before process Pi starts executing, all its claim edges must already appear in
the resource-allocation graph. We can relax this condition by allowing a claim
edge Pi → Rj to be added to the graph only if all the edges associated with
process Pi are claim edges.
330 Chapter 7 Deadlocks
R1
R2
P1 P2
Figure 7.7 Resource-allocation graph for deadlock avoidance.
Now suppose that process Pi requests resource Rj . The request can be
granted only if converting the request edge Pi → Rj to an assignment edge
Rj → Pi does not result in the formation of a cycle in the resource-allocation
graph. We check for safety by using a cycle-detection algorithm. An algorithm
for detecting a cycle in this graph requires an order of n2 operations, where n
is the number of processes in the system.
If no cycle exists, then the allocation of the resource will leave the system
in a safe state. If a cycle is found, then the allocation will put the system in
an unsafe state. In that case, process Pi will have to wait for its requests to be
satisfied.
To illustrate this algorithm, we consider the resource-allocation graph of
Figure 7.7. Suppose that P2 requests R2. Although R2 is currently free, we
cannot allocate it to P2, since this action will create a cycle in the graph (Figure
7.8). A cycle, as mentioned, indicates that the system is in an unsafe state. If P1
requests R2, and P2 requests R1, then a deadlock will occur.
7.5.3 Banker’s Algorithm
The resource-allocation-graph algorithm is not applicable to a resourceallocation system with multiple instances of each resource type. The deadlockavoidance algorithm that we describe next is applicable to such a system but
is less efficient than the resource-allocation graph scheme. This algorithm is
commonly known as the banker’s algorithm. The name was chosen because
the algorithm could be used in a banking system to ensure that the bank never
R1
R2
P1 P2
Figure 7.8 An unsafe state in a resource-allocation graph.
7.5 Deadlock Avoidance 331
allocated its available cash in such a way that it could no longer satisfy the
needs of all its customers.
When a new process enters the system, it must declare the maximum
number of instances of each resource type that it may need. This number may
not exceed the total number of resources in the system. When a user requests
a set of resources, the system must determine whether the allocation of these
resources will leave the system in a safe state. If it will, the resources are
allocated; otherwise, the process must wait until some other process releases
enough resources.
Several data structures must be maintained to implement the banker’s
algorithm. These data structures encode the state of the resource-allocation
system. We need the following data structures, where n is the number of
processes in the system and m is the number of resource types:
• Available. A vector of length m indicates the number of available resources
of each type. If Available[j] equals k, then k instances of resource type Rj
are available.
• Max. An n × m matrix defines the maximum demand of each process.
If Max[i][j] equals k, then process Pi may request at most k instances of
resource type Rj .
• Allocation. An n × m matrix defines the number of resources of each type
currently allocated to each process. If Allocation[i][j] equals k, then process
Pi is currently allocated k instances of resource type Rj .
• Need. An n × m matrix indicates the remaining resource need of each
process. If Need[i][j] equals k, then process Pi may need k more instances
of resource type Rj to complete its task. Note that Need[i][j] equals Max[i][j]
− Allocation[i][j].
These data structures vary over time in both size and value.
To simplify the presentation of the banker’s algorithm, we next establish
some notation. Let X and Y be vectors of length n. We say that X ≤ Y if and
only if X[i] ≤ Y[i] for all i = 1, 2, ..., n. For example, if X = (1,7,3,2) and Y =
(0,3,2,1), then Y ≤ X. In addition, Y < X if Y ≤ X and Y = X.
We can treat each row in the matrices Allocation and Need as vectors
and refer to them as Allocationi and Needi . The vector Allocationi specifies
the resources currently allocated to process Pi ; the vector Needi specifies the
additional resources that process Pi may still request to complete its task.
7.5.3.1 Safety Algorithm
We can now present the algorithm for finding out whether or not a system is
in a safe state. This algorithm can be described as follows:
1. Let Work and Finish be vectors of length m and n, respectively. Initialize
Work = Available and Finish[i] = false for i = 0, 1, ..., n − 1.
2. Find an index i such that both
a. Finish[i] == false
b. Needi ≤ Work
332 Chapter 7 Deadlocks
If no such i exists, go to step 4.
3. Work = Work + Allocationi
Finish[i] = true
Go to step 2.
4. If Finish[i] == true for all i, then the system is in a safe state.
This algorithm may require an order of m × n2 operations to determine whether
a state is safe.
7.5.3.2 Resource-Request Algorithm
Next, we describe the algorithm for determining whether requests can be safely
granted.
Let Requesti be the request vector for process Pi . If Requesti [j] == k, then
process Pi wants k instances of resource type Rj . When a request for resources
is made by process Pi , the following actions are taken:
1. If Requesti ≤ Needi , go to step 2. Otherwise, raise an error condition, since
the process has exceeded its maximum claim.
2. If Requesti ≤ Available, go to step 3. Otherwise, Pi must wait, since the
resources are not available.
3. Have the system pretend to have allocated the requested resources to
process Pi by modifying the state as follows:
Available = Available–Requesti ;
Allocationi = Allocationi + Requesti ;
Needi = Needi –Requesti ;
If the resulting resource-allocation state is safe, the transaction is completed, and process Pi is allocated its resources. However, if the new state
is unsafe, then Pi must wait for Requesti , and the old resource-allocation
state is restored.
7.5.3.3 An Illustrative Example
To illustrate the use of the banker’s algorithm, consider a system with five
processes P0 through P4 and three resource types A, B, and C. Resource type A
has ten instances, resource type B has five instances, and resource type C has
seven instances. Suppose that, at time T0, the following snapshot of the system
has been taken:
Allocation Max Available
ABC ABC ABC
P0 010 753 332
P1 200 322
P2 302 902
P3 211 222
P4 002 433
7.6 Deadlock Detection 333
The content of the matrix Need is defined to be Max − Allocation and is as
follows:
Need
ABC
P0 743
P1 122
P2 600
P3 011
P4 431
We claim that the system is currently in a safe state. Indeed, the sequence
<P1, P3, P4, P2, P0> satisfies the safety criteria. Suppose now that process
P1 requests one additional instance of resource type A and two instances of
resource type C, so Request1 = (1,0,2). To decide whether this request can be
immediately granted, we first check that Request1 ≤ Available—that is, that
(1,0,2) ≤ (3,3,2), which is true. We then pretend that this request has been
fulfilled, and we arrive at the following new state:
Allocation Need Available
ABC ABC ABC
P0 010 743 230
P1 302 020
P2 302 600
P3 211 011
P4 002 431
We must determine whether this new system state is safe. To do so, we
execute our safety algorithm and find that the sequence <P1, P3, P4, P0, P2>
satisfies the safety requirement. Hence, we can immediately grant the request
of process P1.
You should be able to see, however, that when the system is in this state, a
request for (3,3,0) by P4 cannot be granted, since the resources are not available.
Furthermore, a request for (0,2,0) by P0 cannot be granted, even though the
resources are available, since the resulting state is unsafe.
We leave it as a programming exercise for students to implement the
banker’s algorithm.""",
"7.6 Deadlock Detection":"""If a system does not employ either a deadlock-prevention or a deadlockavoidance algorithm, then a deadlock situation may occur. In this environment,
the system may provide:
• An algorithm that examines the state of the system to determine whether
a deadlock has occurred
• An algorithm to recover from the deadlock
334 Chapter 7 Deadlocks
P3
P5
P4
P1 P2
R2
R1 R3 R4
R5
P3
P5
P4
P1 P2
(a) (b)
Figure 7.9 (a) Resource-allocation graph. (b) Corresponding wait-for graph.
In the following discussion, we elaborate on these two requirements as they
pertain to systems with only a single instance of each resource type, as well as to
systems with several instances of each resource type. At this point, however, we
note that a detection-and-recovery scheme requires overhead that includes not
only the run-time costs of maintaining the necessary information and executing
the detection algorithm but also the potential losses inherent in recovering from
a deadlock.
7.6.1 Single Instance of Each Resource Type
If all resources have only a single instance, then we can define a deadlockdetection algorithm that uses a variant of the resource-allocation graph, called
a wait-for graph. We obtain this graph from the resource-allocation graph by
removing the resource nodes and collapsing the appropriate edges.
More precisely, an edge from Pi to Pj in a wait-for graph implies that
process Pi is waiting for process Pj to release a resource that Pi needs. An edge
Pi → Pj exists in a wait-for graph if and only if the corresponding resourceallocation graph contains two edges Pi → Rq and Rq → Pj for some resource
Rq . In Figure 7.9, we present a resource-allocation graph and the corresponding
wait-for graph.
As before, a deadlock exists in the system if and only if the wait-for graph
contains a cycle. To detect deadlocks, the system needs to maintain the waitfor graph and periodically invoke an algorithm that searches for a cycle in
the graph. An algorithm to detect a cycle in a graph requires an order of n2
operations, where n is the number of vertices in the graph.
7.6.2 Several Instances of a Resource Type
The wait-for graph scheme is not applicable to a resource-allocation system
with multiple instances of each resource type. We turn now to a deadlock-
7.6 Deadlock Detection 335
detection algorithm that is applicable to such a system. The algorithm employs
several time-varying data structures that are similar to those used in the
banker’s algorithm (Section 7.5.3):
• Available. A vector of length m indicates the number of available resources
of each type.
• Allocation. An n × m matrix defines the number of resources of each type
currently allocated to each process.
• Request. An n × m matrix indicates the current request of each process.
If Request[i][j] equals k, then process Pi is requesting k more instances of
resource type Rj .
The ≤ relation between two vectors is defined as in Section 7.5.3. To simplify
notation, we again treat the rows in the matrices Allocation and Request as
vectors; we refer to them as Allocationi and Requesti . The detection algorithm
described here simply investigates every possible allocation sequence for the
processes that remain to be completed. Compare this algorithm with the
banker’s algorithm of Section 7.5.3.
1. Let Work and Finish be vectors of length m and n, respectively. Initialize
Work = Available. For i = 0, 1, ..., n–1, if Allocationi = 0, then Finish[i] =
false. Otherwise, Finish[i] = true.
2. Find an index i such that both
a. Finish[i] == false
b. Requesti ≤ Work
If no such i exists, go to step 4.
3. Work = Work + Allocationi
Finish[i] = true
Go to step 2.
4. If Finish[i] == false for some i, 0 ≤ i < n, then the system is in a deadlocked
state. Moreover, if Finish[i] == false, then process Pi is deadlocked.
This algorithm requires an order of m × n2 operations to detect whether the
system is in a deadlocked state.
You may wonder why we reclaim the resources of process Pi (in step 3) as
soon as we determine that Requesti ≤ Work (in step 2b). We know that Pi is
currently not involved in a deadlock (since Requesti ≤ Work). Thus, we take
an optimistic attitude and assume that Pi will require no more resources to
complete its task; it will thus soon return all currently allocated resources to
the system. If our assumption is incorrect, a deadlock may occur later. That
deadlock will be detected the next time the deadlock-detection algorithm is
invoked.
To illustrate this algorithm, we consider a system with five processes P0
through P4 and three resource types A, B, and C. Resource type A has seven
instances, resource type B has two instances, and resource type C has six
336 Chapter 7 Deadlocks
instances. Suppose that, at time T0, we have the following resource-allocation
state:
Allocation Request Available
ABC ABC ABC
P0 010 000 000
P1 200 202
P2 303 000
P3 211 100
P4 002 002
We claim that the system is not in a deadlocked state. Indeed, if we execute
our algorithm, we will find that the sequence <P0, P2, P3, P1, P4> results in
Finish[i] == true for all i.
Suppose now that process P2 makes one additional request for an instance
of type C. The Request matrix is modified as follows:
Request
ABC
P0 000
P1 202
P2 001
P3 100
P4 002
We claim that the system is now deadlocked. Although we can reclaim the
resources held by process P0, the number of available resources is not sufficient
to fulfill the requests of the other processes. Thus, a deadlock exists, consisting
of processes P1, P2, P3, and P4.
7.6.3 Detection-Algorithm Usage
When should we invoke the detection algorithm? The answer depends on two
factors:
1. How often is a deadlock likely to occur?
2. How many processes will be affected by deadlock when it happens?
If deadlocks occur frequently, then the detection algorithm should be invoked
frequently. Resources allocated to deadlocked processes will be idle until the
deadlock can be broken. In addition, the number of processes involved in the
deadlock cycle may grow.
Deadlocks occur only when some process makes a request that cannot be
granted immediately. This request may be the final request that completes a
chain of waiting processes. In the extreme, then, we can invoke the deadlockdetection algorithm every time a request for allocation cannot be granted
immediately. In this case, we can identify not only the deadlocked set of
7.7 Recovery from Deadlock 337
processes but also the specific process that “caused” the deadlock. (In reality,
each of the deadlocked processes is a link in the cycle in the resource graph, so
all of them, jointly, caused the deadlock.) If there are many different resource
types, one request may create many cycles in the resource graph, each cycle
completed by the most recent request and “caused” by the one identifiable
process.
Of course, invoking the deadlock-detection algorithm for every resource
request will incur considerable overhead in computation time. A less expensive
alternative is simply to invoke the algorithm at defined intervals—for example,
once per hour or whenever CPU utilization drops below 40 percent. (A deadlock
eventually cripples system throughput and causes CPU utilization to drop.) If
the detection algorithm is invoked at arbitrary points in time, the resource
graph may contain many cycles. In this case, we generally cannot tell which of
the many deadlocked processes “caused” the deadlock.""",
"16.3 Benefits and Features":"""
Several advantages make virtualization attractive. Most of them are fundamentally related to the ability to share the same hardware yet run several different
execution environments (that is, different operating systems) concurrently.
One important advantage of virtualization is that the host system is
protected from the virtual machines, just as the virtual machines are protected
from each other. A virus inside a guest operating system might damage that
operating system but is unlikely to affect the host or the other guests. Because
each virtual machine is almost completely isolated from all other virtual
machines, there are almost no protection problems.
A potential disadvantage of isolation is that it can prevent sharing of
resources. Two approaches to provide sharing have been implemented. First,
it is possible to share a file-system volume and thus to share files. Second,
it is possible to define a network of virtual machines, each of which can
16.3 Benefits and Features 715
send information over the virtual communications network. The network
is modeled after physical communication networks but is implemented in
software. Of course, the VMM is free to allow any number of its guests to
use physical resources, such as a physical network connection (with sharing
provided by the VMM), in which case the allowed guests could communicate
with each other via the physical network.
One feature common to most virtualization implementations is the ability
to freeze, or suspend, a running virtual machine. Many operating systems
provide that basic feature for processes, but VMMs go one step further and
allow copies and snapshots to be made of the guest. The copy can be used to
create a new VM or to move a VM from one machine to another with its current
state intact. The guest can then resume where it was, as if on its original
machine, creating a clone. The snapshot records a point in time, and the guest
can be reset to that point if necessary (for example, if a change was made
but is no longer wanted). Often, VMMs allow many snapshots to be taken. For
example, snapshots might record a guest’s state every day for a month, making
restoration to any of those snapshot states possible. These abilities are used to
good advantage in virtual environments.
A virtual machine system is a perfect vehicle for operating-system research
and development. Normally, changing an operating system is a difficult task.
Operating systems are large and complex programs, and a change in one
part may cause obscure bugs to appear in some other part. The power of
the operating system makes changing it particularly dangerous. Because the
operating system executes in kernel mode, a wrong change in a pointer could
cause an error that would destroy the entire file system. Thus, it is necessary
to test all changes to the operating system carefully.
Furthermore, the operating system runs on and controls the entire machine,
meaning that the system must be stopped and taken out of use while changes
are made and tested. This period is commonly called system-development
time. Since it makes the system unavailable to users, system-development
time on shared systems is often scheduled late at night or on weekends, when
system load is low.
A virtual-machine system can eliminate much of this latter problem.
System programmers are given their own virtual machine, and system development is done on the virtual machine instead of on a physical machine. Normal
system operation is disrupted only when a completed and tested change is
ready to be put into production.
Another advantage of virtual machines for developers is that multiple
operating systems can run concurrently on the developer’s workstation. This
virtualized workstation allows for rapid porting and testing of programs in
varying environments. In addition, multiple versions of a program can run,
each in its own isolated operating system, within one system. Similarly, qualityassurance engineers can test their applications in multiple environments
without buying, powering, and maintaining a computer for each environment.
A major advantage of virtual machines in production data-center use is
system consolidation, which involves taking two or more separate systems
and running them in virtual machines on one system. Such physical-to-virtual
conversions result in resource optimization, since many lightly used systems
can be combined to create one more heavily used system.
716 Chapter 16 Virtual Machines
Consider, too, that management tools that are part of the VMM allow system
administrators to manage many more systems than they otherwise could.
A virtual environment might include 100 physical servers, each running 20
virtual servers. Without virtualization, 2,000 servers would require several
system administrators. With virtualization and its tools, the same work can be
managed by one or two administrators. One of the tools that make this possible
is templating, in which one standard virtual machine image, including an
installed and configured guest operating system and applications, is saved and
used as a source for multiple running VMs. Other features include managing
the patching of all guests, backing up and restoring the guests, and monitoring
their resource use.
Virtualization can improve not only resource utilization but also resource
management. Some VMMs include a live migration feature that moves a
running guest from one physical server to another without interrupting
its operation or active network connections. If a server is overloaded, live
migration can thus free resources on the source host while not disrupting the
guest. Similarly, when host hardware must be repaired or upgraded, guests
can be migrated to other servers, the evacuated host can be maintained, and
then the guests can be migrated back. This operation occurs without downtime
and without interruption to users.
Think about the possible effects of virtualization on how applications are
deployed. If a system can easily add, remove, and move a virtual machine,
then why install applications on that system directly? Instead, the application
could be preinstalled on a tuned and customized operating system in a virtual
machine. This method would offer several benefits for application developers.
Application management would become easier, less tuning would be required,
and technical support of the application would be more straightforward.
System administrators would find the environment easier to manage as well.
Installation would be simple, and redeploying the application to another
system would be much easier than the usual steps of uninstalling and
reinstalling. For widespread adoption of this methodology to occur, though, the
format of virtual machines must be standardized so that any virtual machine
will run on any virtualization platform. The “Open Virtual Machine Format” is
an attempt to provide such standardization, and it could succeed in unifying
virtual machine formats.
Virtualization has laid the foundation for many other advances in computer
facility implementation, management, and monitoring. Cloud computing,
for example, is made possible by virtualization in which resources such as
CPU, memory, and I/O are provided as services to customers using Internet
technologies. By using APIs, a program can tell a cloud computing facility to
create thousands of VMs, all running a specific guest operating system and
application, which others can access via the Internet. Many multiuser games,
photo-sharing sites, and other web services use this functionality.
In the area of desktop computing, virtualization is enabling desktop and
laptop computer users to connect remotely to virtual machines located in
remote data centers and access their applications as if they were local. This
practice can increase security, because no data are stored on local disks at the
user’s site. The cost of the user’s computing resource may also decrease. The
user must have networking, CPU, and some memory, but all that these system
components need to do is display an image of the guest as its runs remotely (via
16.4 Building Blocks 717
a protocol such as RDP). Thus, they need not be expensive, high-performance
components. Other uses of virtualization are sure to follow as it becomes more
prevalent and hardware support continues to improve.""",
"3.1 Process Concept": """A question that arises in discussing operating systems involves what to call
all the CPU activities. A batch system executes jobs, whereas a time-shared
105
106 Chapter 3 Processes
system has user programs, or tasks. Even on a single-user system, a user may
be able to run several programs at one time: a word processor, a Web browser,
and an e-mail package. And even if a user can execute only one program at a
time, such as on an embedded device that does not support multitasking, the
operating system may need to support its own internal programmed activities,
such as memory management. In many respects, all these activities are similar,
so we call all of them processes.
The terms job and process are used almost interchangeably in this text.
Although we personally prefer the term process, much of operating-system
theory and terminology was developed during a time when the major activity
of operating systems was job processing. It would be misleading to avoid
the use of commonly accepted terms that include the word job (such as job
scheduling) simply because process has superseded job.
3.1.1 The Process
Informally, as mentioned earlier, a process is a program in execution. A process
is more than the program code, which is sometimes known as the text section.
It also includes the current activity, as represented by the value of the program
counter and the contents of the processor’s registers. A process generally also
includes the process stack, which contains temporary data (such as function
parameters, return addresses, and local variables), and a data section, which
contains global variables. A process may also include a heap, which is memory
that is dynamically allocated during process run time. The structure of a process
in memory is shown in Figure 3.1.
We emphasize that a program by itself is not a process. A program is a
passive entity, such as a file containing a list of instructions stored on disk
(often called an executable file). In contrast, a process is an active entity,
with a program counter specifying the next instruction to execute and a set
of associated resources. A program becomes a process when an executable file
is loaded into memory. Two common techniques for loading executable files
text
0
max
data
heap
stack
Figure 3.1 Process in memory.
3.1 Process Concept 107
are double-clicking an icon representing the executable file and entering the
name of the executable file on the command line (as in prog.exe or a.out).
Although two processes may be associated with the same program, they
are nevertheless considered two separate execution sequences. For instance,
several users may be running different copies of the mail program, or the same
user may invoke many copies of the web browser program. Each of these is a
separate process; and although the text sections are equivalent, the data, heap,
and stack sections vary. It is also common to have a process that spawns many
processes as it runs. We discuss such matters in Section 3.4.
Note that a process itself can be an execution environment for other
code. The Java programming environment provides a good example. In most
circumstances, an executable Java program is executed within the Java virtual
machine (JVM). The JVM executes as a process that interprets the loaded Java
code and takes actions (via native machine instructions) on behalf of that code.
For example, to run the compiled Java program Program.class, we would
enter
java Program
The command java runs the JVM as an ordinary process, which in turns
executes the Java program Program in the virtual machine. The concept is the
same as simulation, except that the code, instead of being written for a different
instruction set, is written in the Java language.
3.1.2 Process State
As a process executes, it changes state. The state of a process is defined in part
by the current activity of that process. A process may be in one of the following
states:
• New. The process is being created.
• Running. Instructions are being executed.
• Waiting. The process is waiting for some event to occur (such as an I/O
completion or reception of a signal).
• Ready. The process is waiting to be assigned to a processor.
• Terminated. The process has finished execution.
These names are arbitrary, and they vary across operating systems. The states
that they represent are found on all systems, however. Certain operating
systems also more finely delineate process states. It is important to realize
that only one process can be running on any processor at any instant. Many
processes may be ready and waiting, however. The state diagram corresponding
to these states is presented in Figure 3.2.
3.1.3 Process Control Block
Each process is represented in the operating system by a process control block
(PCB)—also called a task control block. A PCB is shown in Figure 3.3. It contains
many pieces of information associated with a specific process, including these:
108 Chapter 3 Processes
new terminated
ready running
admitted interrupt
scheduler dispatch I/O or event completion I/O or event wait
exit
waiting
Figure 3.2 Diagram of process state.
• Process state. The state may be new, ready, running, waiting, halted, and
so on.
• Program counter. The counter indicates the address of the next instruction
to be executed for this process.
• CPU registers. The registers vary in number and type, depending on
the computer architecture. They include accumulators, index registers,
stack pointers, and general-purpose registers, plus any condition-code
information. Along with the program counter, this state information must
be saved when an interrupt occurs, to allow the process to be continued
correctly afterward (Figure 3.4).
• CPU-scheduling information. This information includes a process priority,
pointers to scheduling queues, and any other scheduling parameters.
(Chapter 6 describes process scheduling.)
• Memory-management information. This information may include such
items as the value of the base and limit registers and the page tables, or the
segment tables, depending on the memory system used by the operating
system (Chapter 8).
process state
process number
program counter
memory limits
list of open files
registers
•
•
•
Figure 3.3 Process control block (PCB).
3.1 Process Concept 109
process P0 process P1
save state into PCB0
save state into PCB1
reload state from PCB1
reload state from PCB0
operating system
idle
idle
idle executing
executing
executing
interrupt or system call
interrupt or system call
•
•
•
•
•
•
Figure 3.4 Diagram showing CPU switch from process to process.
• Accounting information. This information includes the amount of CPU
and real time used, time limits, account numbers, job or process numbers,
and so on.
• I/O status information. This information includes the list of I/O devices
allocated to the process, a list of open files, and so on.
In brief, the PCB simply serves as the repository for any information that may
vary from process to process.
3.1.4 Threads
The process model discussed so far has implied that a process is a program that
performs a single thread of execution. For example, when a process is running
a word-processor program, a single thread of instructions is being executed.
This single thread of control allows the process to perform only one task at
a time. The user cannot simultaneously type in characters and run the spell
checker within the same process, for example. Most modern operating systems
have extended the process concept to allow a process to have multiple threads
of execution and thus to perform more than one task at a time. This feature
is especially beneficial on multicore systems, where multiple threads can run
in parallel. On a system that supports threads, the PCB is expanded to include
information for each thread. Other changes throughout the system are also
needed to support threads. Chapter 4 explores threads in detail.
110 Chapter 3 Processes
PROCESS REPRESENTATION IN LINUX
The process control block in the Linux operating system is represented by
the C structure task struct, which is found in the <linux/sched.h>
include file in the kernel source-code directory. This structure contains all the
necessary information for representing a process, including the state of the
process, scheduling and memory-management information, list of open files,
and pointers to the process’s parent and a list of its children and siblings. (A
process’s parent is the process that created it; its children are any processes
that it creates. Its siblings are children with the same parent process.) Some
of these fields include:
long state; /* state of the process */
struct sched entity se; /* scheduling information */
struct task struct *parent; /* this process’s parent */
struct list head children; /* this process’s children */
struct files struct *files; /* list of open files */
struct mm struct *mm; /* address space of this process */
For example, the state of a process is represented by the field long state
in this structure. Within the Linux kernel, all active processes are represented
using a doubly linked list of task struct. The kernel maintains a pointer—
current—to the process currently executing on the system, as shown below:
struct task_struct
process information
•
•
•
struct task_struct
process information
•
•
•
current
(currently executing proccess)
struct task_struct
process information
•
•
•
• • •
As an illustration of how the kernel might manipulate one of the fields in
the task struct for a specified process, let’s assume the system would like
to change the state of the process currently running to the value new state.
If current is a pointer to the process currently executing, its state is changed
with the following:
current->state = new state; """,
"8.5 Paging": """Segmentation permits the physical address space of a process to be noncontiguous. Paging is another memory-management scheme that offers this
advantage. However, paging avoids external fragmentation and the need for
8.5 Paging 367
logical address space
subroutine stack
symbol
table
main
program
Sqrt
1400
physical memory
2400
3200
segment 2
4300
4700
5700
6300
6700
segment table
limit
0
1
2
3
4
1000
400
400
1100
1000
base
1400
6300
4300
3200
4700
segment 0
segment 3
segment 4
segment 1 segment 2
segment 0
segment 3
segment 4
segment 1
Figure 8.9 Example of segmentation.
compaction, whereas segmentation does not. It also solves the considerable
problem of fitting memory chunks of varying sizes onto the backing store.
Most memory-management schemes used before the introduction of paging
suffered from this problem. The problem arises because, when code fragments
or data residing in main memory need to be swapped out, space must be found
on the backing store. The backing store has the same fragmentation problems
discussed in connection with main memory, but access is much slower, so
compaction is impossible. Because of its advantages over earlier methods,
paging in its various forms is used in most operating systems, from those for
mainframes through those for smartphones. Paging is implemented through
cooperation between the operating system and the computer hardware.
8.5.1 Basic Method
The basic method for implementing paging involves breaking physical memory into fixed-sized blocks called frames and breaking logical memory into
blocks of the same size called pages. When a process is to be executed, its
pages are loaded into any available memory frames from their source (a file
system or the backing store). The backing store is divided into fixed-sized
blocks that are the same size as the memory frames or clusters of multiple
frames. This rather simple idea has great functionality and wide ramifications.
For example, the logical address space is now totally separate from the physical
address space, so a process can have a logical 64-bit address space even though
the system has less than 264 bytes of physical memory.
The hardware support for paging is illustrated in Figure 8.10. Every address
generated by the CPU is divided into two parts: a page number (p) and a page
368 Chapter 8 Main Memory
physical
memory
f
logical
address
page table
physical
address
CPU p
p
f
d f d
f0000 … 0000
f1111 … 1111
Figure 8.10 Paging hardware.
offset (d). The page number is used as an index into a page table. The page table
contains the base address of each page in physical memory. This base address
is combined with the page offset to define the physical memory address that
is sent to the memory unit. The paging model of memory is shown in Figure
8.11.
page 0
page 1
page 2
page 3
logical
memory
page 1
page 3
page 0
page 2
physical
memory
page table
frame
number
1
4
3
7
0
1
2
3
0
1
2
3
4
5
6
7
Figure 8.11 Paging model of logical and physical memory.
8.5 Paging 369
The page size (like the frame size) is defined by the hardware. The size of a
page is a power of 2, varying between 512 bytes and 1 GB per page, depending
on the computer architecture. The selection of a power of 2 as a page size
makes the translation of a logical address into a page number and page offset
particularly easy. If the size of the logical address space is 2m, and a page size is
2n bytes, then the high-order m − n bits of a logical address designate the page
number, and the n low-order bits designate the page offset. Thus, the logical
address is as follows:
p d
page number page offset
m – n n
where p is an index into the page table and d is the displacement within the
page.
As a concrete (although minuscule) example, consider the memory in
Figure 8.12. Here, in the logical address, n= 2 and m = 4. Using a page size
of 4 bytes and a physical memory of 32 bytes (8 pages), we show how the
programmer’s view of memory can be mapped into physical memory. Logical
address 0 is page 0, offset 0. Indexing into the page table, we find that page 0
logical memory
physical memory
page table
i
j
k
l
m
n
o
p
a
b
c
d
e
f
g
h
a
b
c
d
e
f
g
h
i
j
k
l
m
n
o
p
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
0
0
4
8
12
16
20
24
28
1
2
3
5
6
1
2
Figure 8.12 Paging example for a 32-byte memory with 4-byte pages.
370 Chapter 8 Main Memory
OBTAINING THE PAGE SIZE ON LINUX SYSTEMS
On a Linux system, the page size varies according to architecture, and
there are several ways of obtaining the page size. One approach is to use
the getpagesize() system call. Another strategy is to enter the following
command on the command line:
getconf PAGESIZE
Each of these techniques returns the page size as a number of bytes.
is in frame 5. Thus, logical address 0 maps to physical address 20 [= (5 × 4) +
0]. Logical address 3 (page 0, offset 3) maps to physical address 23 [= (5 × 4) +
3]. Logical address 4 is page 1, offset 0; according to the page table, page 1 is
mapped to frame 6. Thus, logical address 4 maps to physical address 24 [= (6
× 4) + 0]. Logical address 13 maps to physical address 9.
You may have noticed that paging itself is a form of dynamic relocation.
Every logical address is bound by the paging hardware to some physical
address. Using paging is similar to using a table of base (or relocation) registers,
one for each frame of memory.
When we use a paging scheme, we have no external fragmentation: any free
frame can be allocated to a process that needs it. However, we may have some
internal fragmentation. Notice that frames are allocated as units. If the memory
requirements of a process do not happen to coincide with page boundaries,
the last frame allocated may not be completely full. For example, if page size
is 2,048 bytes, a process of 72,766 bytes will need 35 pages plus 1,086 bytes. It
will be allocated 36 frames, resulting in internal fragmentation of 2,048 − 1,086
= 962 bytes. In the worst case, a process would need n pages plus 1 byte. It
would be allocated n + 1 frames, resulting in internal fragmentation of almost
an entire frame.
If process size is independent of page size, we expect internal fragmentation
to average one-half page per process. This consideration suggests that small
page sizes are desirable. However, overhead is involved in each page-table
entry, and this overhead is reduced as the size of the pages increases. Also,
disk I/O is more efficient when the amount data being transferred is larger
(Chapter 10). Generally, page sizes have grown over time as processes, data
sets, and main memory have become larger. Today, pages typically are between
4 KB and 8 KB in size, and some systems support even larger page sizes. Some
CPUs and kernels even support multiple page sizes. For instance, Solaris uses
page sizes of 8 KB and 4 MB, depending on the data stored by the pages.
Researchers are now developing support for variable on-the-fly page size.
Frequently, on a 32-bit CPU, each page-table entry is 4 bytes long, but that
size can vary as well. A 32-bit entry can point to one of 232 physical page frames.
If frame size is 4 KB (212), then a system with 4-byte entries can address 244 bytes
(or 16 TB) of physical memory. We should note here that the size of physical
memory in a paged memory system is different from the maximum logical size
of a process. As we further explore paging, we introduce other information that
must be kept in the page-table entries. That information reduces the number
8.5 Paging 371
(a)
free-frame list
14
13
18
20
15
13
14
15
16
17
18
19
20
21
page 0
page 1
page 2
page 3
new process
(b)
free-frame list
15 13 page 1
page 0
page 2
page 3
14
15
16
17
18
19
20
21
page 0
page 1
page 2
page 3
new process
new-process page table
0 14
1
2
3
13
18
20
Figure 8.13 Free frames (a) before allocation and (b) after allocation.
of bits available to address page frames. Thus, a system with 32-bit page-table
entries may address less physical memory than the possible maximum. A 32-bit
CPU uses 32-bit addresses, meaning that a given process space can only be 232
bytes (4 TB). Therefore, paging lets us use physical memory that is larger than
what can be addressed by the CPU’s address pointer length.
When a process arrives in the system to be executed, its size, expressed
in pages, is examined. Each page of the process needs one frame. Thus, if the
process requires n pages, at least n frames must be available in memory. If n
frames are available, they are allocated to this arriving process. The first page
of the process is loaded into one of the allocated frames, and the frame number
is put in the page table for this process. The next page is loaded into another
frame, its frame number is put into the page table, and so on (Figure 8.13).
An important aspect of paging is the clear separation between the programmer’s view of memory and the actual physical memory. The programmer views
memory as one single space, containing only this one program. In fact, the user
program is scattered throughout physical memory, which also holds other
programs. The difference between the programmer’s view of memory and
the actual physical memory is reconciled by the address-translation hardware.
The logical addresses are translated into physical addresses. This mapping is
hidden from the programmer and is controlled by the operating system. Notice
that the user process by definition is unable to access memory it does not own.
It has no way of addressing memory outside of its page table, and the table
includes only those pages that the process owns.
Since the operating system is managing physical memory, it must be aware
of the allocation details of physical memory—which frames are allocated,
which frames are available, how many total frames there are, and so on. This
information is generally kept in a data structure called a frame table. The frame
table has one entry for each physical page frame, indicating whether the latter
372 Chapter 8 Main Memory
is free or allocated and, if it is allocated, to which page of which process or
processes.
In addition, the operating system must be aware that user processes operate
in user space, and all logical addresses must be mapped to produce physical
addresses. If a user makes a system call (to do I/O, for example) and provides
an address as a parameter (a buffer, for instance), that address must be mapped
to produce the correct physical address. The operating system maintains a copy
of the page table for each process, just as it maintains a copy of the instruction
counter and register contents. This copy is used to translate logical addresses to
physical addresses whenever the operating system must map a logical address
to a physical address manually. It is also used by the CPU dispatcher to define
the hardware page table when a process is to be allocated the CPU. Paging
therefore increases the context-switch time.
8.5.2 Hardware Support
Each operating system has its own methods for storing page tables. Some
allocate a page table for each process. A pointer to the page table is stored with
the other register values (like the instruction counter) in the process control
block. When the dispatcher is told to start a process, it must reload the user
registers and define the correct hardware page-table values from the stored user
page table. Other operating systems provide one or at most a few page tables,
which decreases the overhead involved when processes are context-switched.
The hardware implementation of the page table can be done in several
ways. In the simplest case, the page table is implemented as a set of dedicated
registers. These registers should be built with very high-speed logic to make the
paging-address translation efficient. Every access to memory must go through
the paging map, so efficiency is a major consideration. The CPU dispatcher
reloads these registers, just as it reloads the other registers. Instructions to load
or modify the page-table registers are, of course, privileged, so that only the
operating system can change the memory map. The DEC PDP-11 is an example
of such an architecture. The address consists of 16 bits, and the page size is 8
KB. The page table thus consists of eight entries that are kept in fast registers.
The use of registers for the page table is satisfactory if the page table is
reasonably small (for example, 256 entries). Most contemporary computers,
however, allow the page table to be very large (for example, 1 million entries).
For these machines, the use of fast registers to implement the page table is
not feasible. Rather, the page table is kept in main memory, and a page-table
base register (PTBR) points to the page table. Changing page tables requires
changing only this one register, substantially reducing context-switch time.
The problem with this approach is the time required to access a user
memory location. If we want to access location i, we must first index into
the page table, using the value in the PTBR offset by the page number for i. This
task requires a memory access. It provides us with the frame number, which
is combined with the page offset to produce the actual address. We can then
access the desired place in memory. With this scheme, two memory accesses
are needed to access a byte (one for the page-table entry, one for the byte). Thus,
memory access is slowed by a factor of 2. This delay would be intolerable under
most circumstances. We might as well resort to swapping!
8.5 Paging 373
The standard solution to this problem is to use a special, small, fastlookup hardware cache called a translation look-aside buffer (TLB). The TLB
is associative, high-speed memory. Each entry in the TLB consists of two parts:
a key (or tag) and a value. When the associative memory is presented with an
item, the item is compared with all keys simultaneously. If the item is found,
the corresponding value field is returned. The search is fast; a TLB lookup in
modern hardware is part of the instruction pipeline, essentially adding no
performance penalty. To be able to execute the search within a pipeline step,
however, the TLB must be kept small. It is typically between 32 and 1,024 entries
in size. Some CPUs implement separate instruction and data address TLBs. That
can double the number of TLB entries available, because those lookups occur
in different pipeline steps. We can see in this development an example of the
evolution of CPU technology: systems have evolved from having no TLBs to
having multiple levels of TLBs, just as they have multiple levels of caches.
The TLB is used with page tables in the following way. The TLB contains
only a few of the page-table entries. When a logical address is generated by the
CPU, its page number is presented to the TLB. If the page number is found, its
frame number is immediately available and is used to access memory. As just
mentioned, these steps are executed as part of the instruction pipeline within
the CPU, adding no performance penalty compared with a system that does
not implement paging.
If the page number is not in the TLB (known as a TLB miss), a memory
reference to the page table must be made. Depending on the CPU, this may be
done automatically in hardware or via an interrupt to the operating system.
When the frame number is obtained, we can use it to access memory (Figure
8.14). In addition, we add the page number and frame number to the TLB, so
page table
f
CPU
logical
address
p d
f d
physical
address
physical
memory
p
TLB miss
page
number
frame
number
TLB hit
TLB
Figure 8.14 Paging hardware with TLB.
374 Chapter 8 Main Memory
that they will be found quickly on the next reference. If the TLB is already full
of entries, an existing entry must be selected for replacement. Replacement
policies range from least recently used (LRU) through round-robin to random.
Some CPUs allow the operating system to participate in LRU entry replacement,
while others handle the matter themselves. Furthermore, some TLBs allow
certain entries to be wired down, meaning that they cannot be removed from
the TLB. Typically, TLB entries for key kernel code are wired down.
Some TLBs store address-space identifiers (ASIDs) in each TLB entry. An
ASID uniquely identifies each process and is used to provide address-space
protection for that process. When the TLB attempts to resolve virtual page
numbers, it ensures that the ASID for the currently running process matches the
ASID associated with the virtual page. If the ASIDs do not match, the attempt is
treated as a TLB miss. In addition to providing address-space protection, an ASID
allows the TLB to contain entries for several different processes simultaneously.
If the TLB does not support separate ASIDs, then every time a new page table
is selected (for instance, with each context switch), the TLB must be flushed
(or erased) to ensure that the next executing process does not use the wrong
translation information. Otherwise, the TLB could include old entries that
contain valid virtual addresses but have incorrect or invalid physical addresses
left over from the previous process.
The percentage of times that the page number of interest is found in the
TLB is called the hit ratio. An 80-percent hit ratio, for example, means that
we find the desired page number in the TLB 80 percent of the time. If it takes
100 nanoseconds to access memory, then a mapped-memory access takes 100
nanoseconds when the page number is in the TLB. If we fail to find the page
number in the TLB then we must first access memory for the page table and
frame number (100 nanoseconds) and then access the desired byte in memory
(100 nanoseconds), for a total of 200 nanoseconds. (We are assuming that a
page-table lookup takes only one memory access, but it can take more, as we
shall see.) To find the effective memory-access time, we weight the case by its
probability:
effective access time = 0.80 × 100 + 0.20 × 200
= 120 nanoseconds
In this example, we suffer a 20-percent slowdown in average memory-access
time (from 100 to 120 nanoseconds).
For a 99-percent hit ratio, which is much more realistic, we have
effective access time = 0.99 × 100 + 0.01 × 200
= 101 nanoseconds
This increased hit rate produces only a 1 percent slowdown in access time.
As we noted earlier, CPUs today may provide multiple levels of TLBs.
Calculating memory access times in modern CPUs is therefore much more
complicated than shown in the example above. For instance, the Intel Core
i7 CPU has a 128-entry L1 instruction TLB and a 64-entry L1 data TLB. In the
case of a miss at L1, it takes the CPU six cycles to check for the entry in the L2
512-entry TLB. A miss in L2 means that the CPU must either walk through the
8.5 Paging 375
page-table entries in memory to find the associated frame address, which can
take hundreds of cycles, or interrupt to the operating system to have it do the
work.
A complete performance analysis of paging overhead in such a system
would require miss-rate information about each TLB tier. We can see from the
general information above, however, that hardware features can have a significant effect on memory performance and that operating-system improvements
(such as paging) can result in and, in turn, be affected by hardware changes
(such as TLBs). We will further explore the impact of the hit ratio on the TLB in
Chapter 9.
TLBs are a hardware feature and therefore would seem to be of little concern
to operating systems and their designers. But the designer needs to understand
the function and features of TLBs, which vary by hardware platform. For
optimal operation, an operating-system design for a given platform must
implement paging according to the platform’s TLB design. Likewise, a change in
the TLB design (for example, between generations of Intel CPUs) may necessitate
a change in the paging implementation of the operating systems that use it.
8.5.3 Protection
Memory protection in a paged environment is accomplished by protection bits
associated with each frame. Normally, these bits are kept in the page table.
One bit can define a page to be read–write or read-only. Every reference
to memory goes through the page table to find the correct frame number. At
the same time that the physical address is being computed, the protection bits
can be checked to verify that no writes are being made to a read-only page. An
attempt to write to a read-only page causes a hardware trap to the operating
system (or memory-protection violation).
We can easily expand this approach to provide a finer level of protection.
We can create hardware to provide read-only, read–write, or execute-only
protection; or, by providing separate protection bits for each kind of access, we
can allow any combination of these accesses. Illegal attempts will be trapped
to the operating system.
One additional bit is generally attached to each entry in the page table: a
valid–invalid bit. When this bit is set to valid, the associated page is in the
process’s logical address space and is thus a legal (or valid) page. When the
bit is set toinvalid, the page is not in the process’s logical address space. Illegal
addresses are trapped by use of the valid–invalid bit. The operating system
sets this bit for each page to allow or disallow access to the page.
Suppose, for example, that in a system with a 14-bit address space (0 to
16383), we have a program that should use only addresses 0 to 10468. Given
a page size of 2 KB, we have the situation shown in Figure 8.15. Addresses in
pages 0, 1, 2, 3, 4, and 5 are mapped normally through the page table. Any
attempt to generate an address in pages 6 or 7, however, will find that the
valid–invalid bit is set to invalid, and the computer will trap to the operating
system (invalid page reference).
Notice that this scheme has created a problem. Because the program
extends only to address 10468, any reference beyond that address is illegal.
However, references to page 5 are classified as valid, so accesses to addresses
up to 12287 are valid. Only the addresses from 12288 to 16383 are invalid. This
376 Chapter 8 Main Memory
page 0
page 0
page 1
page 2
page 3
page 4
page 5
page n
•
•
•
00000
0
1
2
3
4
5
6
7
8
9
frame number
0
1
2
3
4
5
6
7
2
3
4
7
8
9
0
0
v
v
v
v
v
v
i
i
page table
valid–invalid bit
10,468
12,287
page 1
page 2
page 3
page 4
page 5
Figure 8.15 Valid (v) or invalid (i) bit in a page table.
problem is a result of the 2-KB page size and reflects the internal fragmentation
of paging.
Rarely does a process use all its address range. In fact, many processes
use only a small fraction of the address space available to them. It would be
wasteful in these cases to create a page table with entries for every page in the
address range. Most of this table would be unused but would take up valuable
memory space. Some systems provide hardware, in the form of a page-table
length register (PTLR), to indicate the size of the page table. This value is
checked against every logical address to verify that the address is in the valid
range for the process. Failure of this test causes an error trap to the operating
system.
8.5.4 Shared Pages
An advantage of paging is the possibility of sharing common code. This consideration is particularly important in a time-sharing environment. Consider a
system that supports 40 users, each of whom executes a text editor. If the text
editor consists of 150 KB of code and 50 KB of data space, we need 8,000 KB to
support the 40 users. If the code is reentrant code (or pure code), however, it
can be shared, as shown in Figure 8.16. Here, we see three processes sharing
a three-page editor—each page 50 KB in size (the large page size is used to
simplify the figure). Each process has its own data page.
Reentrant code is non-self-modifying code: it never changes during execution. Thus, two or more processes can execute the same code at the same time.
8.5 Paging 377
7
6
5
4 ed 2
3 ed 1
2
1 data 1
0
3
4
6
1
page table
for P1
process P1
data 1
ed 2
ed 3
ed 1
3
4
6
2
page table
for P3
process P3
data 3
ed 2
ed 3
ed 1
3
4
6
7
page table
for P2
process P2
data 2
ed 2
ed 3
ed 1
8
9
10
11
data 3
data 2
ed 3
Figure 8.16 Sharing of code in a paging environment.
Each process has its own copy of registers and data storage to hold the data for
the process’s execution. The data for two different processes will, of course, be
different.
Only one copy of the editor need be kept in physical memory. Each user’s
page table maps onto the same physical copy of the editor, but data pages are
mapped onto different frames. Thus, to support 40 users, we need only one
copy of the editor (150 KB), plus 40 copies of the 50 KB of data space per user.
The total space required is now 2,150 KB instead of 8,000 KB—a significant
savings.
Other heavily used programs can also be shared—compilers, window
systems, run-time libraries, database systems, and so on. To be sharable, the
code must be reentrant. The read-only nature of shared code should not be
left to the correctness of the code; the operating system should enforce this
property.
The sharing of memory among processes on a system is similar to the
sharing of the address space of a task by threads, described in Chapter 4.
Furthermore, recall that in Chapter 3 we described shared memory as a method
of interprocess communication. Some operating systems implement shared
memory using shared pages.
Organizing memory according to pages provides numerous benefits in
addition to allowing several processes to share the same physical pages. We
cover several other benefits in Chapter 9.""",
"8.6 Structure of the Page Table": """In this section, we explore some of the most common techniques for structuring
the page table, including hierarchical paging, hashed page tables, and inverted
page tables.
8.6.1 Hierarchical Paging
Most modern computer systems support a large logical address space
(232 to 264). In such an environment, the page table itself becomes excessively
large. For example, consider a system with a 32-bit logical address space. If
the page size in such a system is 4 KB (212), then a page table may consist of
up to 1 million entries (232/212). Assuming that each entry consists of 4 bytes,
each process may need up to 4 MB of physical address space for the page table
alone. Clearly, we would not want to allocate the page table contiguously in
main memory. One simple solution to this problem is to divide the page table
into smaller pieces. We can accomplish this division in several ways.
One way is to use a two-level paging algorithm, in which the page table
itself is also paged (Figure 8.17). For example, consider again the system with
a 32-bit logical address space and a page size of 4 KB. A logical address is
divided into a page number consisting of 20 bits and a page offset consisting
of 12 bits. Because we page the page table, the page number is further divided
•
•
•
•
•
•
outer page
table
page of
page table
page table
memory
929
900
929
900
708
500
100
1
0
•
•
•
100
708
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
1
500
Figure 8.17 A two-level page-table scheme.
8.6 Structure of the Page Table 379
logical address
outer page
table
p1 p2
p1
page of
page table
p2
d
d
Figure 8.18 Address translation for a two-level 32-bit paging architecture.
into a 10-bit page number and a 10-bit page offset. Thus, a logical address is as
follows:
p1 p2 d
page number page offset
10 10 12
where p1 is an index into the outer page table and p2 is the displacement
within the page of the inner page table. The address-translation method for this
architecture is shown in Figure 8.18. Because address translation works from
the outer page table inward, this scheme is also known as a forward-mapped
page table.
Consider the memory management of one of the classic systems, the VAX
minicomputer from Digital Equipment Corporation (DEC). The VAX was the
most popular minicomputer of its time and was sold from 1977 through 2000.
The VAX architecture supported a variation of two-level paging. The VAX is a 32-
bit machine with a page size of 512 bytes. The logical address space of a process
is divided into four equal sections, each of which consists of 230 bytes. Each
section represents a different part of the logical address space of a process. The
first 2 high-order bits of the logical address designate the appropriate section.
The next 21 bits represent the logical page number of that section, and the final
9 bits represent an offset in the desired page. By partitioning the page table in
this manner, the operating system can leave partitions unused until a process
needs them. Entire sections of virtual address space are frequently unused, and
multilevel page tables have no entries for these spaces, greatly decreasing the
amount of memory needed to store virtual memory data structures.
An address on the VAX architecture is as follows:
s p d
section page offset
2 21 9
where s designates the section number, p is an index into the page table, and d
is the displacement within the page. Even when this scheme is used, the size
of a one-level page table for a VAX process using one section is 221 bits ∗ 4
380 Chapter 8 Main Memory
bytes per entry = 8 MB. To further reduce main-memory use, the VAX pages the
user-process page tables.
For a system with a 64-bit logical address space, a two-level paging scheme
is no longer appropriate. To illustrate this point, let’s suppose that the page
size in such a system is 4 KB (212). In this case, the page table consists of up
to 252 entries. If we use a two-level paging scheme, then the inner page tables
can conveniently be one page long, or contain 210 4-byte entries. The addresses
look like this:
p1 p2 d
outer page inner page offset
42 10 12
The outer page table consists of 242 entries, or 244 bytes. The obvious way to
avoid such a large table is to divide the outer page table into smaller pieces.
(This approach is also used on some 32-bit processors for added flexibility and
efficiency.)
We can divide the outer page table in various ways. For example, we can
page the outer page table, giving us a three-level paging scheme. Suppose that
the outer page table is made up of standard-size pages (210 entries, or 212 bytes).
In this case, a 64-bit address space is still daunting:
p1 p2 p3
2nd outer page outer page inner page
32 10 10
d
offset
12
The outer page table is still 234 bytes (16 GB) in size.
The next step would be a four-level paging scheme, where the second-level
outer page table itself is also paged, and so forth. The 64-bit UltraSPARC would
require seven levels of paging—a prohibitive number of memory accesses—
to translate each logical address. You can see from this example why, for 64-bit
architectures, hierarchical page tables are generally considered inappropriate.
8.6.2 Hashed Page Tables
A common approach for handling address spaces larger than 32 bits is to use
a hashed page table, with the hash value being the virtual page number. Each
entry in the hash table contains a linked list of elements that hash to the same
location (to handle collisions). Each element consists of three fields: (1) the
virtual page number, (2) the value of the mapped page frame, and (3) a pointer
to the next element in the linked list.
The algorithm works as follows: The virtual page number in the virtual
address is hashed into the hash table. The virtual page number is compared
with field 1 in the first element in the linked list. If there is a match, the
corresponding page frame (field 2) is used to form the desired physical address.
If there is no match, subsequent entries in the linked list are searched for a
matching virtual page number. This scheme is shown in Figure 8.19.
A variation of this scheme that is useful for 64-bit address spaces has
been proposed. This variation uses clustered page tables, which are similar to
8.6 Structure of the Page Table 381
hash table
q s
logical address
physical
address
physical
memory
p d r d
p r hash
function • • •
Figure 8.19 Hashed page table.
hashed page tables except that each entry in the hash table refers to several
pages (such as 16) rather than a single page. Therefore, a single page-table
entry can store the mappings for multiple physical-page frames. Clustered
page tables are particularly useful for sparse address spaces, where memory
references are noncontiguous and scattered throughout the address space.
8.6.3 Inverted Page Tables
Usually, each process has an associated page table. The page table has one
entry for each page that the process is using (or one slot for each virtual
address, regardless of the latter’s validity). This table representation is a natural
one, since processes reference pages through the pages’ virtual addresses. The
operating system must then translate this reference into a physical memory
address. Since the table is sorted by virtual address, the operating system is
able to calculate where in the table the associated physical address entry is
located and to use that value directly. One of the drawbacks of this method
is that each page table may consist of millions of entries. These tables may
consume large amounts of physical memory just to keep track of how other
physical memory is being used.
To solve this problem, we can use an inverted page table. An inverted
page table has one entry for each real page (or frame) of memory. Each entry
consists of the virtual address of the page stored in that real memory location,
with information about the process that owns the page. Thus, only one page
table is in the system, and it has only one entry for each page of physical
memory. Figure 8.20 shows the operation of an inverted page table. Compare
it with Figure 8.10, which depicts a standard page table in operation. Inverted
page tables often require that an address-space identifier (Section 8.5.2) be
stored in each entry of the page table, since the table usually contains several
different address spaces mapping physical memory. Storing the address-space
identifier ensures that a logical page for a particular process is mapped to the
corresponding physical page frame. Examples of systems using inverted page
tables include the 64-bit UltraSPARC and PowerPC.
382 Chapter 8 Main Memory
page table
CPU
logical
address physical
address physical
memory
i
pid p
pid
search
p
d i d
Figure 8.20 Inverted page table.
To illustrate this method, we describe a simplified version of the inverted
page table used in the IBM RT. IBM was the first major company to use inverted
page tables, starting with the IBM System 38 and continuing through the
RS/6000 and the current IBM Power CPUs. For the IBM RT, each virtual address
in the system consists of a triple:
<process-id, page-number, offset>.
Each inverted page-table entry is a pair <process-id, page-number> where the
process-id assumes the role of the address-space identifier. When a memory
reference occurs, part of the virtual address, consisting of <process-id, pagenumber>, is presented to the memory subsystem. The inverted page table
is then searched for a match. If a match is found—say, at entry i—then the
physical address <i, offset> is generated. If no match is found, then an illegal
address access has been attempted.
Although this scheme decreases the amount of memory needed to store
each page table, it increases the amount of time needed to search the table when
a page reference occurs. Because the inverted page table is sorted by physical
address, but lookups occur on virtual addresses, the whole table might need
to be searched before a match is found. This search would take far too long.
To alleviate this problem, we use a hash table, as described in Section 8.6.2,
to limit the search to one—or at most a few—page-table entries. Of course,
each access to the hash table adds a memory reference to the procedure, so one
virtual memory reference requires at least two real memory reads—one for the
hash-table entry and one for the page table. (Recall that the TLB is searched first,
before the hash table is consulted, offering some performance improvement.)
Systems that use inverted page tables have difficulty implementing shared
memory. Shared memory is usually implemented as multiple virtual addresses
(one for each process sharing the memory) that are mapped to one physical
address. This standard method cannot be used with inverted page tables;
because there is only one virtual page entry for every physical page, one
8.7 Example: Intel 32 and 64-bit Architectures 383
physical page cannot have two (or more) shared virtual addresses. A simple
technique for addressing this issue is to allow the page table to contain only
one mapping of a virtual address to the shared physical address. This means
that references to virtual addresses that are not mapped result in page faults.
8.6.4 Oracle SPARC Solaris
Consider as a final example a modern 64-bit CPU and operating system that are
tightly integrated to provide low-overhead virtual memory. Solaris running
on the SPARC CPU is a fully 64-bit operating system and as such has to solve
the problem of virtual memory without using up all of its physical memory
by keeping multiple levels of page tables. Its approach is a bit complex but
solves the problem efficiently using hashed page tables. There are two hash
tables—one for the kernel and one for all user processes. Each maps memory
addresses from virtual to physical memory. Each hash-table entry represents a
contiguous area of mapped virtual memory, which is more efficient than having
a separate hash-table entry for each page. Each entry has a base address and a
span indicating the number of pages the entry represents.
Virtual-to-physical translation would take too long if each address required
searching through a hash table, so the CPU implements a TLB that holds
translation table entries (TTEs) for fast hardware lookups. A cache of these TTEs
reside in a translation storage buffer (TSB), which includes an entry per recently
accessed page. When a virtual address reference occurs, the hardware searches
the TLB for a translation. If none is found, the hardware walks through the
in-memory TSB looking for the TTE that corresponds to the virtual address that
caused the lookup. This TLB walk functionality is found on many modern CPUs.
If a match is found in the TSB, the CPU copies the TSB entry into the TLB, and
the memory translation completes. If no match is found in the TSB, the kernel
is interrupted to search the hash table. The kernel then creates a TTE from the
appropriate hash table and stores it in the TSB for automatic loading into the TLB
by the CPU memory-management unit. Finally, the interrupt handler returns
control to the MMU, which completes the address translation and retrieves the
requested byte or word from main memory""",
"9.4 Page Replacement": """In our earlier discussion of the page-fault rate, we assumed that each page
faults at most once, when it is first referenced. This representation is not strictly
accurate, however. If a process of ten pages actually uses only half of them, then
demand paging saves the I/O necessary to load the five pages that are never
used. We could also increase our degree of multiprogramming by running
twice as many processes. Thus, if we had forty frames, we could run eight
processes, rather than the four that could run if each required ten frames (five
of which were never used).
If we increase our degree of multiprogramming, we are over-allocating
memory. If we run six processes, each of which is ten pages in size but actually
uses only five pages, we have higher CPU utilization and throughput, with
ten frames to spare. It is possible, however, that each of these processes, for a
particular data set, may suddenly try to use all ten of its pages, resulting in a
need for sixty frames when only forty are available.
Further, consider that system memory is not used only for holding program
pages. Buffers forI/O also consume a considerable amount of memory. This use""",
"10.5 Disk Management": """The operating system is responsible for several other aspects of disk management, too. Here we discuss disk initialization, booting from disk, and bad-block
recovery.
10.5 Disk Management 479
10.5.1 Disk Formatting
A new magnetic disk is a blank slate: it is just a platter of a magnetic recording
material. Before a disk can store data, it must be divided into sectors that the
disk controller can read and write. This process is called low-level formatting,
or physical formatting. Low-level formatting fills the disk with a special data
structure for each sector. The data structure for a sector typically consists of a
header, a data area (usually 512 bytes in size), and a trailer. The header and
trailer contain information used by the disk controller, such as a sector number
and an error-correcting code (ECC). When the controller writes a sector of data
during normal I/O, the ECC is updated with a value calculated from all the bytes
in the data area. When the sector is read, the ECC is recalculated and compared
with the stored value. If the stored and calculated numbers are different, this
mismatch indicates that the data area of the sector has become corrupted and
that the disk sector may be bad (Section 10.5.3). The ECC is an error-correcting
code because it contains enough information, if only a few bits of data have
been corrupted, to enable the controller to identify which bits have changed
and calculate what their correct values should be. It then reports a recoverable
soft error. The controller automatically does the ECC processing whenever a
sector is read or written.
Most hard disks are low-level-formatted at the factory as a part of the
manufacturing process. This formatting enables the manufacturer to test the
disk and to initialize the mapping from logical block numbers to defect-free
sectors on the disk. For many hard disks, when the disk controller is instructed
to low-level-format the disk, it can also be told how many bytes of data space
to leave between the header and trailer of all sectors. It is usually possible to
choose among a few sizes, such as 256, 512, and 1,024 bytes. Formatting a disk
with a larger sector size means that fewer sectors can fit on each track; but it
also means that fewer headers and trailers are written on each track and more
space is available for user data. Some operating systems can handle only a
sector size of 512 bytes.
Before it can use a disk to hold files, the operating system still needs to
record its own data structures on the disk. It does so in two steps. The first step
is to partition the disk into one or more groups of cylinders. The operating
system can treat each partition as though it were a separate disk. For instance,
one partition can hold a copy of the operating system’s executable code, while
another holds user files. The second step is logical formatting, or creation of a
file system. In this step, the operating system stores the initial file-system data
structures onto the disk. These data structures may include maps of free and
allocated space and an initial empty directory.
To increase efficiency, most file systems group blocks together into larger
chunks, frequently called clusters. Disk I/O is done via blocks, but file system
I/O is done via clusters, effectively assuring that I/O has more sequential-access
and fewer random-access characteristics.
Some operating systems give special programs the ability to use a disk
partition as a large sequential array of logical blocks, without any file-system
data structures. This array is sometimes called the raw disk, and I/O to this
array is termed raw I/O. For example, some database systems prefer raw
I/O because it enables them to control the exact disk location where each
database record is stored. Raw I/O bypasses all the file-system services, such
480 Chapter 10 Mass-Storage Structure
as the buffer cache, file locking, prefetching, space allocation, file names, and
directories. We can make certain applications more efficient by allowing them
to implement their own special-purpose storage services on a raw partition,
but most applications perform better when they use the regular file-system
services.
10.5.2 Boot Block
For a computer to start running—for instance, when it is powered up or
rebooted—it must have an initial program to run. This initial bootstrap
program tends to be simple. It initializes all aspects of the system, from CPU
registers to device controllers and the contents of main memory, and then
starts the operating system. To do its job, the bootstrap program finds the
operating-system kernel on disk, loads that kernel into memory, and jumps to
an initial address to begin the operating-system execution.
For most computers, the bootstrap is stored in read-only memory (ROM).
This location is convenient, because ROM needs no initialization and is at a fixed
location that the processor can start executing when powered up or reset. And,
since ROM is read only, it cannot be infected by a computer virus. The problem is
that changing this bootstrap code requires changing the ROM hardware chips.
For this reason, most systems store a tiny bootstrap loader program in the boot
ROM whose only job is to bring in a full bootstrap program from disk. The full
bootstrap program can be changed easily: a new version is simply written onto
the disk. The full bootstrap program is stored in the “boot blocks” at a fixed
location on the disk. A disk that has a boot partition is called a boot disk or
system disk.
The code in the boot ROM instructs the disk controller to read the boot
blocks into memory (no device drivers are loaded at this point) and then starts
executing that code. The full bootstrap program is more sophisticated than the
bootstrap loader in the boot ROM. It is able to load the entire operating system
from a non-fixed location on disk and to start the operating system running.
Even so, the full bootstrap code may be small.
Let’s consider as an example the boot process in Windows. First, note that
Windows allows a hard disk to be divided into partitions, and one partition
—identified as the boot partition—contains the operating system and device
drivers. The Windows system places its boot code in the first sector on the hard
disk, which it terms the master boot record, or MBR. Booting begins by running
code that is resident in the system’s ROM memory. This code directs the system
to read the boot code from the MBR. In addition to containing boot code, the
MBR contains a table listing the partitions for the hard disk and a flag indicating
which partition the system is to be booted from, as illustrated in Figure 10.9.
Once the system identifies the boot partition, it reads the first sector from that
partition (which is called the boot sector) and continues with the remainder of
the boot process, which includes loading the various subsystems and system
services.
10.5.3 Bad Blocks
Because disks have moving parts and small tolerances (recall that the disk
head flies just above the disk surface), they are prone to failure. Sometimes the
failure is complete; in this case, the disk needs to be replaced and its contents
10.5 Disk Management 481
MBR
partition 1
partition 2
partition 3
partition 4
boot
code
partition
table
boot partition
Figure 10.9 Booting from disk in Windows.
restored from backup media to the new disk. More frequently, one or more
sectors become defective. Most disks even come from the factory with bad
blocks. Depending on the disk and controller in use, these blocks are handled
in a variety of ways.
On simple disks, such as some disks with IDE controllers, bad blocks are
handled manually. One strategy is to scan the disk to find bad blocks while
the disk is being formatted. Any bad blocks that are discovered are flagged as
unusable so that the file system does not allocate them. If blocks go bad during
normal operation, a special program (such as the Linux badblocks command)
must be run manually to search for the bad blocks and to lock them away. Data
that resided on the bad blocks usually are lost.
More sophisticated disks are smarter about bad-block recovery. The controller maintains a list of bad blocks on the disk. The list is initialized during
the low-level formatting at the factory and is updated over the life of the disk.
Low-level formatting also sets aside spare sectors not visible to the operating
system. The controller can be told to replace each bad sector logically with one
of the spare sectors. This scheme is known as sector sparing or forwarding.
A typical bad-sector transaction might be as follows:
• The operating system tries to read logical block 87.
• The controller calculates the ECC and finds that the sector is bad. It reports
this finding to the operating system.
• The next time the system is rebooted, a special command is run to tell the
controller to replace the bad sector with a spare.
• After that, whenever the system requests logical block 87, the request is
translated into the replacement sector’s address by the controller.
Note that such a redirection by the controller could invalidate any optimization by the operating system’s disk-scheduling algorithm! For this reason,
most disks are formatted to provide a few spare sectors in each cylinder and
a spare cylinder as well. When a bad block is remapped, the controller uses a
spare sector from the same cylinder, if possible.
As an alternative to sector sparing, some controllers can be instructed to
replace a bad block by sector slipping. Here is an example: Suppose that
482 Chapter 10 Mass-Storage Structure
logical block 17 becomes defective and the first available spare follows sector
202. Sector slipping then remaps all the sectors from 17 to 202, moving them
all down one spot. That is, sector 202 is copied into the spare, then sector 201
into 202, then 200 into 201, and so on, until sector 18 is copied into sector 19.
Slipping the sectors in this way frees up the space of sector 18 so that sector 17
can be mapped to it.
The replacement of a bad block generally is not totally automatic, because
the data in the bad block are usually lost. Soft errors may trigger a process in
which a copy of the block data is made and the block is spared or slipped.
An unrecoverable hard error, however, results in lost data. Whatever file was
using that block must be repaired (for instance, by restoration from a backup
tape), and that requires manual intervention""",
"2.7 Operating-System Structure": """A system as large and complex as a modern operating system must be
engineered carefully if it is to function properly and be modified easily. A
common approach is to partition the task into small components, or modules,
rather than have one monolithic system. Each of these modules should be
a well-defined portion of the system, with carefully defined inputs, outputs,
and functions. We have already discussed briefly in Chapter 1 the common
components of operating systems. In this section, we discuss how these
components are interconnected and melded into a kernel.
2.7.1 Simple Structure
Many operating systems do not have well-defined structures. Frequently, such
systems started as small, simple, and limited systems and then grew beyond
their original scope. MS-DOS is an example of such a system. It was originally
designed and implemented by a few people who had no idea that it would
become so popular. It was written to provide the most functionality in the
least space, so it was not carefully divided into modules. Figure 2.11 shows its
structure.
In MS-DOS, the interfaces and levels of functionality are not well separated.
For instance, application programs are able to access the basic I/O routines
to write directly to the display and disk drives. Such freedom leaves MS-DOS
vulnerable to errant (or malicious) programs, causing entire system crashes
when user programs fail. Of course, MS-DOS was also limited by the hardware
of its era. Because the Intel 8088 for which it was written provides no dual
mode and no hardware protection, the designers of MS-DOS had no choice but
to leave the base hardware accessible.
Another example of limited structuring is the original UNIX operating
system. Like MS-DOS, UNIX initially was limited by hardware functionality. It
consists of two separable parts: the kernel and the system programs. The kernel
ROM BIOS device drivers
application program
MS-DOS device drivers
resident system program
Figure 2.11 MS-DOS layer structure.
2.7 Operating-System Structure 79
kernel
(the users)
shells and commands
compilers and interpreters
system libraries
system-call interface to the kernel
signals terminal
handling
character I/O system
terminal drivers
file system
swapping block I/O
system
disk and tape drivers
CPU scheduling
page replacement
demand paging
virtual memory
kernel interface to the hardware
terminal controllers
terminals
device controllers
disks and tapes
memory controllers
physical memory
Figure 2.12 Traditional UNIX system structure.
is further separated into a series of interfaces and device drivers, which have
been added and expanded over the years as UNIX has evolved. We can view the
traditional UNIX operating system as being layered to some extent, as shown in
Figure 2.12. Everything below the system-call interface and above the physical
hardware is the kernel. The kernel provides the file system, CPU scheduling,
memory management, and other operating-system functions through system
calls. Taken in sum, that is an enormous amount of functionality to be combined
into one level. This monolithic structure was difficult to implement and
maintain. It had a distinct performance advantage, however: there is very little
overhead in the system call interface or in communication within the kernel.
We still see evidence of this simple, monolithic structure in the UNIX, Linux,
and Windows operating systems.
2.7.2 Layered Approach
With proper hardware support, operating systems can be broken into pieces
that are smaller and more appropriate than those allowed by the original
MS-DOS and UNIX systems. The operating system can then retain much greater
control over the computer and over the applications that make use of that
computer. Implementers have more freedom in changing the inner workings
of the system and in creating modular operating systems. Under a topdown approach, the overall functionality and features are determined and
are separated into components. Information hiding is also important, because
it leaves programmers free to implement the low-level routines as they see fit,
provided that the external interface of the routine stays unchanged and that
the routine itself performs the advertised task.
A system can be made modular in many ways. One method is the layered
approach, in which the operating system is broken into a number of layers
(levels). The bottom layer (layer 0) is the hardware; the highest (layer N) is the
user interface. This layering structure is depicted in Figure 2.13.
80 Chapter 2 Operating-System Structures
layer N
user interface
•
•
•
layer 1
layer 0
hardware
Figure 2.13 A layered operating system.
An operating-system layer is an implementation of an abstract object made
up of data and the operations that can manipulate those data. A typical
operating-system layer—say, layer M—consists of data structures and a set
of routines that can be invoked by higher-level layers. Layer M, in turn, can
invoke operations on lower-level layers.
The main advantage of the layered approach is simplicity of construction
and debugging. The layers are selected so that each uses functions (operations)
and services of only lower-level layers. This approach simplifies debugging
and system verification. The first layer can be debugged without any concern
for the rest of the system, because, by definition, it uses only the basic hardware
(which is assumed correct) to implement its functions. Once the first layer is
debugged, its correct functioning can be assumed while the second layer is
debugged, and so on. If an error is found during the debugging of a particular
layer, the error must be on that layer, because the layers below it are already
debugged. Thus, the design and implementation of the system are simplified.
Each layer is implemented only with operations provided by lower-level
layers. A layer does not need to know how these operations are implemented;
it needs to know only what these operations do. Hence, each layer hides the
existence of certain data structures, operations, and hardware from higher-level
layers.
The major difficulty with the layered approach involves appropriately
defining the various layers. Because a layer can use only lower-level layers,
careful planning is necessary. For example, the device driver for the backing
store (disk space used by virtual-memory algorithms) must be at a lower
level than the memory-management routines, because memory management
requires the ability to use the backing store.
Other requirements may not be so obvious. The backing-store driver would
normally be above the CPU scheduler, because the driver may need to wait for
I/O and the CPU can be rescheduled during this time. However, on a large
2.7 Operating-System Structure 81
system, the CPU scheduler may have more information about all the active
processes than can fit in memory. Therefore, this information may need to be
swapped in and out of memory, requiring the backing-store driver routine to
be below the CPU scheduler.
A final problem with layered implementations is that they tend to be less
efficient than other types. For instance, when a user program executes an I/O
operation, it executes a system call that is trapped to the I/O layer, which calls
the memory-management layer, which in turn calls the CPU-scheduling layer,
which is then passed to the hardware. At each layer, the parameters may be
modified, data may need to be passed, and so on. Each layer adds overhead to
the system call. The net result is a system call that takes longer than does one
on a nonlayered system.
These limitations have caused a small backlash against layering in recent
years. Fewer layers with more functionality are being designed, providing
most of the advantages of modularized code while avoiding the problems of
layer definition and interaction.
2.7.3 Microkernels
We have already seen that as UNIX expanded, the kernel became large
and difficult to manage. In the mid-1980s, researchers at Carnegie Mellon
University developed an operating system called Mach that modularized
the kernel using the microkernel approach. This method structures the
operating system by removing all nonessential components from the kernel and
implementing them as system and user-level programs. The result is a smaller
kernel. There is little consensus regarding which services should remain in the
kernel and which should be implemented in user space. Typically, however,
microkernels provide minimal process and memory management, in addition
to a communication facility. Figure 2.14 illustrates the architecture of a typical
microkernel.
The main function of the microkernel is to provide communication between
the client program and the various services that are also running in user space.
Communication is provided through message passing, which was described
in Section 2.4.5. For example, if the client program wishes to access a file, it
Application
Program
File
System
Device
Driver
Interprocess
Communication
memory
managment
CPU
scheduling
messages messages
microkernel
hardware
user
mode
kernel
mode
Figure 2.14 Architecture of a typical microkernel.
82 Chapter 2 Operating-System Structures
must interact with the file server. The client program and service never interact
directly. Rather, they communicate indirectly by exchanging messages with the
microkernel.
One benefit of the microkernel approach is that it makes extending
the operating system easier. All new services are added to user space and
consequently do not require modification of the kernel. When the kernel does
have to be modified, the changes tend to be fewer, because the microkernel is
a smaller kernel. The resulting operating system is easier to port from one
hardware design to another. The microkernel also provides more security
and reliability, since most services are running as user—rather than kernel—
processes. If a service fails, the rest of the operating system remains untouched.
Some contemporary operating systems have used the microkernel
approach. Tru64 UNIX (formerly Digital UNIX) provides a UNIX interface to the
user, but it is implemented with a Mach kernel. The Mach kernel maps UNIX
system calls into messages to the appropriate user-level services. The Mac OS X
kernel (also known as Darwin) is also partly based on the Mach microkernel.
Another example is QNX, a real-time operating system for embedded
systems. The QNX Neutrino microkernel provides services for message passing
and process scheduling. It also handles low-level network communication
and hardware interrupts. All other services in QNX are provided by standard
processes that run outside the kernel in user mode.
Unfortunately, the performance of microkernels can suffer due to increased
system-function overhead. Consider the history of Windows NT. The first
release had a layered microkernel organization. This version’s performance
was low compared with that of Windows 95. Windows NT 4.0 partially
corrected the performance problem by moving layers from user space to
kernel space and integrating them more closely. By the time Windows XP
was designed, Windows architecture had become more monolithic than
microkernel.
2.7.4 Modules
Perhaps the best current methodology for operating-system design involves
using loadable kernel modules. Here, the kernel has a set of core components
and links in additional services via modules, either at boot time or during run
time. This type of design is common in modern implementations of UNIX, such
as Solaris, Linux, and Mac OS X, as well as Windows.
The idea of the design is for the kernel to provide core services while
other services are implemented dynamically, as the kernel is running. Linking
services dynamically is preferable to adding new features directly to the kernel,
which would require recompiling the kernel every time a change was made.
Thus, for example, we might build CPU scheduling and memory management
algorithms directly into the kernel and then add support for different file
systems by way of loadable modules.
The overall result resembles a layered system in that each kernel section
has defined, protected interfaces; but it is more flexible than a layered system,
because any module can call any other module. The approach is also similar to
the microkernel approach in that the primary module has only core functions
and knowledge of how to load and communicate with other modules; but it
2.7 Operating-System Structure 83
core Solaris
kernel
file systems
loadable
system calls
executable
formats
STREAMS
modules
miscellaneous
modules
device and
bus drivers
scheduling
classes
Figure 2.15 Solaris loadable modules.
is more efficient, because modules do not need to invoke message passing in
order to communicate.
The Solaris operating system structure, shown in Figure 2.15, is organized
around a core kernel with seven types of loadable kernel modules:
1. Scheduling classes
2. File systems
3. Loadable system calls
4. Executable formats
5. STREAMS modules
6. Miscellaneous
7. Device and bus drivers
Linux also uses loadable kernel modules, primarily for supporting device
drivers and file systems. We cover creating loadable kernel modules in Linux
as a programming exercise at the end of this chapter.
2.7.5 Hybrid Systems
In practice, very few operating systems adopt a single, strictly defined
structure. Instead, they combine different structures, resulting in hybrid
systems that address performance, security, and usability issues. For example,
both Linux and Solaris are monolithic, because having the operating system
in a single address space provides very efficient performance. However,
they are also modular, so that new functionality can be dynamically added
to the kernel. Windows is largely monolithic as well (again primarily for
performance reasons), but it retains some behavior typical of microkernel
systems, including providing support for separate subsystems (known as
operating-system personalities) that run as user-mode processes. Windows
systems also provide support for dynamically loadable kernel modules. We
provide case studies of Linux and Windows 7 in in Chapters 18 and 19,
respectively. In the remainder of this section, we explore the structure of
84 Chapter 2 Operating-System Structures
three hybrid systems: the Apple Mac OS X operating system and the two most
prominent mobile operating systems—iOS and Android.
2.7.5.1 Mac OS X
The Apple Mac OS X operating system uses a hybrid structure. As shown in
Figure 2.16, it is a layered system. The top layers include the Aqua user interface
(Figure 2.4) and a set of application environments and services. Notably,
the Cocoa environment specifies an API for the Objective-C programming
language, which is used for writing Mac OS X applications. Below these
layers is the kernel environment, which consists primarily of the Mach
microkernel and the BSD UNIX kernel. Mach provides memory management;
support for remote procedure calls (RPCs) and interprocess communication
(IPC) facilities, including message passing; and thread scheduling. The BSD
component provides a BSD command-line interface, support for networking
and file systems, and an implementation of POSIX APIs, including Pthreads.
In addition to Mach and BSD, the kernel environment provides an I/O kit
for development of device drivers and dynamically loadable modules (which
Mac OS X refers to as kernel extensions). As shown in Figure 2.16, the BSD
application environment can make use of BSD facilities directly.
2.7.5.2 iOS
iOS is a mobile operating system designed by Apple to run its smartphone, the
iPhone, as well as its tablet computer, the iPad. iOS is structured on the Mac
OS X operating system, with added functionality pertinent to mobile devices,
but does not directly run Mac OS X applications. The structure of iOS appears
in Figure 2.17.
Cocoa Touch is an API for Objective-C that provides several frameworks for
developing applications that run on iOS devices. The fundamental difference
between Cocoa, mentioned earlier, and Cocoa Touch is that the latter provides
support for hardware features unique to mobile devices, such as touch screens.
The media services layer provides services for graphics, audio, and video.
graphical user interface Aqua
application environments and services
kernel environment
Java Cocoa Quicktime BSD
Mach
I/O kit kernel extensions
BSD
Figure 2.16 The Mac OS X structure.
2.7 Operating-System Structure 85
Cocoa Touch
Media Services
Core Services
Core OS
Figure 2.17 Architecture of Apple’s iOS.
The core services layer provides a variety of features, including support for
cloud computing and databases. The bottom layer represents the core operating
system, which is based on the kernel environment shown in Figure 2.16.
2.7.5.3 Android
The Android operating system was designed by the Open Handset Alliance
(led primarily by Google) and was developed for Android smartphones and
tablet computers. Whereas iOS is designed to run on Apple mobile devices
and is close-sourced, Android runs on a variety of mobile platforms and is
open-sourced, partly explaining its rapid rise in popularity. The structure of
Android appears in Figure 2.18.
Android is similar to iOS in that it is a layered stack of software that
provides a rich set of frameworks for developing mobile applications. At the
bottom of this software stack is the Linux kernel, although it has been modified
by Google and is currently outside the normal distribution of Linux releases.
Applications
Application Framework
Android runtime
Core Libraries
Dalvik
virtual machine
Libraries
Linux kernel
SQLite openGL
surface
manager
webkit libc
media
framework
Figure 2.18 Architecture of Google’s Android.
86 Chapter 2 Operating-System Structures
Linux is used primarily for process, memory, and device-driver support for
hardware and has been expanded to include power management. The Android
runtime environment includes a core set of libraries as well as the Dalvik virtual
machine. Software designers for Android devices develop applications in the
Java language. However, rather than using the standard Java API, Google has
designed a separate Android API for Java development. The Java class files are
first compiled to Java bytecode and then translated into an executable file that
runs on the Dalvik virtual machine. The Dalvik virtual machine was designed
for Android and is optimized for mobile devices with limited memory and
CPU processing capabilities.
The set of libraries available for Android applications includes frameworks
for developing web browsers (webkit), database support (SQLite), and multimedia. The libc library is similar to the standard C library but is much smaller
and has been designed for the slower CPUs that characterize mobile devices""",
"14.4 Access Matrix": """Our general model of protection can be viewed abstractly as a matrix, called
an access matrix. The rows of the access matrix represent domains, and the
columns represent objects. Each entry in the matrix consists of a set of access
rights. Because the column defines objects explicitly, we can omit the object
name from the access right. The entry access(i,j) defines the set of operations
that a process executing in domain Di can invoke on object Oj .
To illustrate these concepts, we consider the access matrix shown in Figure
14.3. There are four domains and four objects—three files (F1, F2, F3) and one
laser printer. A process executing in domain D1 can read files F1 and F3. A
process executing in domain D4 has the same privileges as one executing in
object
printer
read
read execute
read
write
read
write
read
print
F1
D1
D2
D3
D4
F2 F3
domain
Figure 14.3 Access matrix.
14.4 Access Matrix 633
domain D1; but in addition, it can also write onto files F1 and F3. The laser
printer can be accessed only by a process executing in domain D2.
The access-matrix scheme provides us with the mechanism for specifying
a variety of policies. The mechanism consists of implementing the access
matrix and ensuring that the semantic properties we have outlined hold.
More specifically, we must ensure that a process executing in domain Di can
access only those objects specified in row i, and then only as allowed by the
access-matrix entries.
The access matrix can implement policy decisions concerning protection.
The policy decisions involve which rights should be included in the (i, j)
th
entry. We must also decide the domain in which each process executes. This
last policy is usually decided by the operating system.
The users normally decide the contents of the access-matrix entries. When
a user creates a new object Oj , the column Oj is added to the access matrix
with the appropriate initialization entries, as dictated by the creator. The user
may decide to enter some rights in some entries in column j and other rights
in other entries, as needed.
The access matrix provides an appropriate mechanism for defining and
implementing strict control for both static and dynamic association between
processes and domains. When we switch a process from one domain to another,
we are executing an operation (switch) on an object (the domain). We can
control domain switching by including domains among the objects of the
access matrix. Similarly, when we change the content of the access matrix,
we are performing an operation on an object: the access matrix. Again, we
can control these changes by including the access matrix itself as an object.
Actually, since each entry in the access matrix can be modified individually,
we must consider each entry in the access matrix as an object to be protected.
Now, we need to consider only the operations possible on these new objects
(domains and the access matrix) and decide how we want processes to be able
to execute these operations.
Processes should be able to switch from one domain to another. Switching
from domain Di to domain Dj is allowed if and only if the access right switch
∈ access(i, j). Thus, in Figure 14.4, a process executing in domain D2 can switch
laser
printer
read
read execute
read
write
read
write
read
print
switch
switch
switch switch
F1
D1
D1
D2
D2
D3
D3
D4
F2 F3 D4
object
domain
Figure 14.4 Access matrix of Figure 14.3 with domains as objects.
634 Chapter 14 Protection
to domain D3 or to domain D4. A process in domain D4 can switch to D1, and
one in domain D1 can switch to D2.
Allowing controlled change in the contents of the access-matrix entries
requires three additional operations: copy, owner, and control. We examine
these operations next.
The ability to copy an access right from one domain (or row) of the access
matrix to another is denoted by an asterisk (*) appended to the access right.
The copy right allows the access right to be copied only within the column
(that is, for the object) for which the right is defined. For example, in Figure
14.5(a), a process executing in domain D2 can copy the read operation into any
entry associated with file F2. Hence, the access matrix of Figure 14.5(a) can be
modified to the access matrix shown in Figure 14.5(b).
This scheme has two additional variants:
1. A right is copied from access(i, j) to access(k, j); it is then removed from
access(i, j). This action is a of a right, rather than a copy.
2. Propagation of the copy right may be limited. That is, when the right
R∗ is copied from access(i, j) to access(k, j), only the right R (not R∗)
is created. A process executing in domain Dk cannot further copy the
right R.
A system may select only one of these three copy rights, or it may provide
all three by identifying them as separate rights: copy, transfer, and limited
copy.
We also need a mechanism to allow addition of new rights and removal of
some rights. The owner right controls these operations. If access(i, j) includes
the owner right, then a process executing in domain Di can add and remove
object
read*
execute write*
execute execute
execute
F1
D1
D2
D3
F2 F3
domain
(a)
object
read*
execute write*
execute execute
execute read
F1
D1
D2
D3
F2 F3
domain
(b)
Figure 14.5 Access matrix with copy rights.
14.4 Access Matrix 635
object
read*
owner
write owner
execute
read*
owner
write
execute
F1
D1
D2
D3
F2 F3
domain
(a)
object
owner
read*
write*
write owner
execute
read*
owner
write
F1
D1
D2
D3
F2 F3
domain
(b)
write write
Figure 14.6 Access matrix with owner rights.
any right in any entry in column j. For example, in Figure 14.6(a), domain D1
is the owner of F1 and thus can add and delete any valid right in column F1.
Similarly, domain D2 is the owner of F2 and F3 and thus can add and remove
any valid right within these two columns. Thus, the access matrix of Figure
14.6(a) can be modified to the access matrix shown in Figure 14.6(b).
The copy and owner rights allow a process to change the entries in a
column. A mechanism is also needed to change the entries in a row. The
control right is applicable only to domain objects. If access(i, j) includes the
control right, then a process executing in domain Di can remove any access
right from row j. For example, suppose that, in Figure 14.4, we include the
control right in access(D2, D4). Then, a process executing in domain D2
could modify domain D4, as shown in Figure 14.7.
The copy and owner rights provide us with a mechanism to limit the
propagation of access rights. However, they do not give us the appropriate tools
for preventing the propagation (or disclosure) of information. The problem of
guaranteeing that no information initially held in an object can migrate outside
of its execution environment is called the confinement problem. This problem
is in general unsolvable (see the bibliographical notes at the end of the chapter).
These operations on the domains and the access matrix are not in themselves important, but they illustrate the ability of the access-matrix model to
allow us to implement and control dynamic protection requirements. New
objects and new domains can be created dynamically and included in the
636 Chapter 14 Protection
laser
printer
read
read execute
write write
read
print
switch
switch
switch switch
control
F1
D1
D1
D2
D2
D3
D3
D4
F2 F3 D4
object
domain
Figure 14.7 Modified access matrix of Figure 14.4.
access-matrix model. However, we have shown only that the basic mechanism
exists. System designers and users must make the policy decisions concerning
which domains are to have access to which objects in which ways""",
"14.5 Implementation of the Access Matrix": """How can the access matrix be implemented effectively? In general, the matrix
will be sparse; that is, most of the entries will be empty. Although datastructure techniques are available for representing sparse matrices, they are
not particularly useful for this application, because of the way in which
the protection facility is used. Here, we first describe several methods of
implementing the access matrix and then compare the methods.
14.5.1 Global Table
The simplest implementation of the access matrix is a global table consisting
of a set of ordered triples <domain, object, rights-set>. Whenever an
operation M is executed on an object Oj within domain Di , the global table
is searched for a triple <Di , Oj , Rk>, with M ∈ Rk . If this triple is found, the
operation is allowed to continue; otherwise, an exception (or error) condition
is raised.
This implementation suffers from several drawbacks. The table is usually
large and thus cannot be kept in main memory, so additional I/O is needed.
Virtual memory techniques are often used for managing this table. In addition,
it is difficult to take advantage of special groupings of objects or domains.
For example, if everyone can read a particular object, this object must have a
separate entry in every domain.
14.5.2 Access Lists for Objects
Each column in the access matrix can be implemented as an access list for
one object, as described in Section 11.6.2. Obviously, the empty entries can be
discarded. The resulting list for each object consists of ordered pairs <domain,
rights-set>, which define all domains with a nonempty set of access rights
for that object.
This approach can be extended easily to define a list plus a default set of
access rights. When an operation M on an object Oj is attempted in domain
14.5 Implementation of the Access Matrix 637
Di , we search the access list for object Oj , looking for an entry <Di , Rk> with
M ∈ Rk . If the entry is found, we allow the operation; if it is not, we check the
default set. If M is in the default set, we allow the access. Otherwise, access is
denied, and an exception condition occurs. For efficiency, we may check the
default set first and then search the access list.
14.5.3 Capability Lists for Domains
Rather than associating the columns of the access matrix with the objects as
access lists, we can associate each row with its domain. A capability list for
a domain is a list of objects together with the operations allowed on those
objects. An object is often represented by its physical name or address, called
a capability. To execute operation M on object Oj , the process executes the
operation M, specifying the capability (or pointer) for object Oj as a parameter.
Simple possession of the capability means that access is allowed.
The capability list is associated with a domain, but it is never directly
accessible to a process executing in that domain. Rather, the capability list
is itself a protected object, maintained by the operating system and accessed
by the user only indirectly. Capability-based protection relies on the fact that
the capabilities are never allowed to migrate into any address space directly
accessible by a user process (where they could be modified). If all capabilities
are secure, the object they protect is also secure against unauthorized access.
Capabilities were originally proposed as a kind of secure pointer, to
meet the need for resource protection that was foreseen as multiprogrammed
computer systems came of age. The idea of an inherently protected pointer
provides a foundation for protection that can be extended up to the application
level.
To provide inherent protection, we must distinguish capabilities from other
kinds of objects, and they must be interpreted by an abstract machine on which
higher-level programs run. Capabilities are usually distinguished from other
data in one of two ways:
• Each object has a tag to denote whether it is a capability or accessible
data. The tags themselves must not be directly accessible by an application
program. Hardware or firmware support may be used to enforce this
restriction. Although only one bit is necessary to distinguish between
capabilities and other objects, more bits are often used. This extension
allows all objects to be tagged with their types by the hardware. Thus,
the hardware can distinguish integers, floating-point numbers, pointers,
Booleans, characters, instructions, capabilities, and uninitialized values by
their tags.
• Alternatively, the address space associated with a program can be split into
two parts. One part is accessible to the program and contains the program’s
normal data and instructions. The other part, containing the capability list,
is accessible only by the operating system. A segmented memory space
(Section 8.4) is useful to support this approach.
Several capability-based protection systems have been developed; we describe
them briefly in Section 14.8. The Mach operating system also uses a version of
capability-based protection; it is described in Appendix B.
638 Chapter 14 Protection
14.5.4 A Lock –Key Mechanism
The lock–key scheme is a compromise between access lists and capability
lists. Each object has a list of unique bit patterns, called locks. Similarly, each
domain has a list of unique bit patterns, called keys. A process executing in a
domain can access an object only if that domain has a key that matches one of
the locks of the object.
As with capability lists, the list of keys for a domain must be managed
by the operating system on behalf of the domain. Users are not allowed to
examine or modify the list of keys (or locks) directly.
14.5.5 Comparison
As you might expect, choosing a technique for implementing an access matrix
involves various trade-offs. Using a global table is simple; however, the table
can be quite large and often cannot take advantage of special groupings of
objects or domains. Access lists correspond directly to the needs of users.
When a user creates an object, he can specify which domains can access the
object, as well as what operations are allowed. However, because access-right
information for a particular domain is not localized, determining the set of
access rights for each domain is difficult. In addition, every access to the object
must be checked, requiring a search of the access list. In a large system with
long access lists, this search can be time consuming.
Capability lists do not correspond directly to the needs of users, but they are
useful for localizing information for a given process. The process attempting
access must present a capability for that access. Then, the protection system
needs only to verify that the capability is valid. Revocation of capabilities,
however, may be inefficient (Section 14.7).
The lock–key mechanism, as mentioned, is a compromise between access
lists and capability lists. The mechanism can be both effective and flexible,
depending on the length of the keys. The keys can be passed freely from
domain to domain. In addition, access privileges can be effectively revoked by
the simple technique of changing some of the locks associated with the object
(Section 14.7).
Most systems use a combination of access lists and capabilities. When a
process first tries to access an object, the access list is searched. If access is
denied, an exception condition occurs. Otherwise, a capability is created and
attached to the process. Additional references use the capability to demonstrate
swiftly that access is allowed. After the last access, the capability is destroyed.
This strategy is used in the MULTICS system and in the CAL system.
As an example of how such a strategy works, consider a file system in
which each file has an associated access list. When a process opens a file, the
directory structure is searched to find the file, access permission is checked, and
buffers are allocated. All this information is recorded in a new entry in a file
table associated with the process. The operation returns an index into this table
for the newly opened file. All operations on the file are made by specification
of the index into the file table. The entry in the file table then points to the file
and its buffers. When the file is closed, the file-table entry is deleted. Since the
file table is maintained by the operating system, the user cannot accidentally
corrupt it. Thus, the user can access only those files that have been opened.
14.6 Access Control 639
Since access is checked when the file is opened, protection is ensured. This
strategy is used in the UNIX system.
The right to access must still be checked on each access, and the file-table
entry has a capability only for the allowed operations. If a file is opened for
reading, then a capability for read access is placed in the file-table entry. If
an attempt is made to write onto the file, the system identifies this protection
violation by comparing the requested operation with the capability in the
file-table entry.""",
"13.2 I/O Hardware": """Computers operate a great many kinds of devices. Most fit into the general
categories of storage devices (disks, tapes), transmission devices (network connections, Bluetooth), and human-interface devices (screen, keyboard, mouse,
audio in and out). Other devices are more specialized, such as those involved in
the steering of a jet. In these aircraft, a human gives input to the flight computer
via a joystick and foot pedals, and the computer sends output commands that
cause motors to move rudders and flaps and fuels to the engines. Despite
the incredible variety of I/O devices, though, we need only a few concepts to
understand how the devices are attached and how the software can control the
hardware.
A device communicates with a computer system by sending signals over
a cable or even through the air. The device communicates with the machine
via a connection point, or port—for example, a serial port. If devices share a
common set of wires, the connection is called a bus. A bus is a set of wires and
a rigidly defined protocol that specifies a set of messages that can be sent on
the wires. In terms of the electronics, the messages are conveyed by patterns
of electrical voltages applied to the wires with defined timings. When device
A has a cable that plugs into device B, and device B has a cable that plugs into
device C, and device C plugs into a port on the computer, this arrangement is
called a daisy chain. A daisy chain usually operates as a bus.
Buses are used widely in computer architecture and vary in their signaling
methods, speed, throughput, and connection methods. A typical PC bus
structure appears in Figure 13.1. In the figure, a PCI bus (the common PC
system bus) connects the processor–memory subsystem to fast devices, and
an expansion bus connects relatively slow devices, such as the keyboard and
serial and USB ports. In the upper-right portion of the figure, four disks are
connected together on a Small Computer System Interface (SCSI) bus plugged
into a SCSI controller. Other common buses used to interconnect main parts of
a computer include PCI Express (PCIe), with throughput of up to 16 GB per
second, and HyperTransport, with throughput of up to 25 GB per second.
A controller is a collection of electronics that can operate a port, a bus,
or a device. A serial-port controller is a simple device controller. It is a single
chip (or portion of a chip) in the computer that controls the signals on the
13.2 I/O Hardware 589
expansion bus
PCI bus
disk
disk
disk
disk
disk
disk
disk
disk
SCSI controller SCSI bus
cache
memory
processor
bridge/memory
controller
monitor
IDE disk controller expansion bus
interface
graphics
controller
keyboard
parallel
port
serial
port
Figure 13.1 A typical PC bus structure.
wires of a serial port. By contrast, a SCSI bus controller is not simple. Because
the SCSI protocol is complex, the SCSI bus controller is often implemented as
a separate circuit board (or a host adapter) that plugs into the computer. It
typically contains a processor, microcode, and some private memory to enable
it to process the SCSI protocol messages. Some devices have their own built-in
controllers. If you look at a disk drive, you will see a circuit board attached to
one side. This board is the disk controller. It implements the disk side of the
protocol for some kind of connection—SCSI or Serial Advanced Technology
Attachment (SATA), for instance. It has microcode and a processor to do many
tasks, such as bad-sector mapping, prefetching, buffering, and caching.
How can the processor give commands and data to a controller to
accomplish an I/O transfer? The short answer is that the controller has one
or more registers for data and control signals. The processor communicates
with the controller by reading and writing bit patterns in these registers. One
way in which this communication can occur is through the use of special
I/O instructions that specify the transfer of a byte or word to an I/O port
address. The I/O instruction triggers bus lines to select the proper device and
to move bits into or out of a device register. Alternatively, the device controller
can support memory-mapped I/O. In this case, the device-control registers
are mapped into the address space of the processor. The CPU executes I/O
requests using the standard data-transfer instructions to read and write the
device-control registers at their mapped locations in physical memory.
Some systems use both techniques. For instance, PCs use I/O instructions
to control some devices and memory-mapped I/O to control others. Figure
13.2 shows the usual I/O port addresses for PCs. The graphics controller has
I/O ports for basic control operations, but the controller has a large memory-
590 Chapter 13 I/O Systems
I/O address range (hexadecimal)
000–00F
020–021
040–043
200–20F
2F8–2FF
320–32F
378–37F
3D0–3DF
3F0–3F7
3F8–3FF
device
DMA controller
interrupt controller
timer
game controller
serial port (secondary)
hard-disk controller
parallel port
graphics controller
diskette-drive controller
serial port (primary)
Figure 13.2 Device I/O port locations on PCs (partial).
mapped region to hold screen contents. The process sends output to the screen
by writing data into the memory-mapped region. The controller generates
the screen image based on the contents of this memory. This technique is
simple to use. Moreover, writing millions of bytes to the graphics memory
is faster than issuing millions of I/O instructions. But the ease of writing
to a memory-mapped I/O controller is offset by a disadvantage. Because a
common type of software fault is a write through an incorrect pointer to an
unintended region of memory, a memory-mapped device register is vulnerable
to accidental modification. Of course, protected memory helps to reduce this
risk.
An I/O port typically consists of four registers, called the status, control,
data-in, and data-out registers.
• The data-in register is read by the host to get input.
• The data-out register is written by the host to send output.
• The status register contains bits that can be read by the host. These bits
indicate states, such as whether the current command has completed,
whether a byte is available to be read from the data-in register, and whether
a device error has occurred.
• The control register can be written by the host to start a command or to
change the mode of a device. For instance, a certain bit in the control
register of a serial port chooses between full-duplex and half-duplex
communication, another bit enables parity checking, a third bit sets the
word length to 7 or 8 bits, and other bits select one of the speeds supported
by the serial port.
The data registers are typically 1 to 4 bytes in size. Some controllers have
FIFO chips that can hold several bytes of input or output data to expand the
capacity of the controller beyond the size of the data register. A FIFO chip can
hold a small burst of data until the device or host is able to receive those data.
13.2 I/O Hardware 591
13.2.1 Polling
The complete protocol for interaction between the host and a controller
can be intricate, but the basic handshaking notion is simple. We explain
handshaking with an example. Assume that 2 bits are used to coordinate
the producer–consumer relationship between the controller and the host. The
controller indicates its state through the busy bit in the status register. (Recall
that to set a bit means to write a 1 into the bit and to clear a bit means to write
a 0 into it.) The controller sets the busy bit when it is busy working and clears
the busy bit when it is ready to accept the next command. The host signals its
wishes via the command-ready bit in the command register. The host sets the
command-ready bit when a command is available for the controller to execute.
For this example, the host writes output through a port, coordinating with the
controller by handshaking as follows.
1. The host repeatedly reads the busy bit until that bit becomes clear.
2. The host sets the write bit in the command register and writes a byte into
the data-out register.
3. The host sets the command-ready bit.
4. When the controller notices that the command-ready bit is set, it sets the
busy bit.
5. The controller reads the command register and sees the write command.
It reads the data-out register to get the byte and does the I/O to the
device.
6. The controller clears the command-ready bit, clears the error bit in the
status register to indicate that the device I/O succeeded, and clears the
busy bit to indicate that it is finished.
This loop is repeated for each byte.
In step 1, the host is busy-waiting or polling: it is in a loop, reading the
status register over and over until the busy bit becomes clear. If the controller
and device are fast, this method is a reasonable one. But if the wait may be
long, the host should probably switch to another task. How, then, does the
host know when the controller has become idle? For some devices, the host
must service the device quickly, or data will be lost. For instance, when data
are streaming in on a serial port or from a keyboard, the small buffer on the
controller will overflow and data will be lost if the host waits too long before
returning to read the bytes.
In many computer architectures, three CPU-instruction cycles are sufficient
to poll a device: read a device register, logical--and to extract a status bit,
and branch if not zero. Clearly, the basic polling operation is efficient. But
polling becomes inefficient when it is attempted repeatedly yet rarely finds a
device ready for service, while other useful CPU processing remains undone. In
such instances, it may be more efficient to arrange for the hardware controller to
notify the CPU when the device becomes ready for service, rather than to require
the CPU to poll repeatedly for an I/O completion. The hardware mechanism
that enables a device to notify the CPU is called an interrupt.
592 Chapter 13 I/O Systems
device driver initiates I/O
CPU receiving interrupt,
transfers control to
interrupt handler
CPU resumes
processing of
interrupted task
CPU
1
I/O controller
CPU executing checks for
interrupts between instructions
5
interrupt handler
processes data,
returns from interrupt
initiates I/O
3
2
4
7
input ready, output
complete, or error
generates interrupt signal
6
Figure 13.3 Interrupt-driven I/O cycle.
13.2.2 Interrupts
The basic interrupt mechanism works as follows. The CPU hardware has a wire
called the interrupt-request line that the CPU senses after executing every
instruction. When the CPU detects that a controller has asserted a signal on
the interrupt-request line, the CPU performs a state save and jumps to the
interrupt-handler routine at a fixed address in memory. The interrupt handler
determines the cause of the interrupt, performs the necessary processing,
performs a state restore, and executes a return from interrupt instruction
to return the CPU to the execution state prior to the interrupt. We say that
the device controller raises an interrupt by asserting a signal on the interrupt
request line, the CPU catches the interrupt and dispatches it to the interrupt
handler, and the handler clears the interrupt by servicing the device. Figure 13.3
summarizes the interrupt-driven I/O cycle. We stress interrupt management
in this chapter because even single-user modern systems manage hundreds of
interrupts per second and servers hundreds of thousands per second.
The basic interrupt mechanism just described enables the CPU to respond
to an asynchronous event, as when a device controller becomes ready for
service. In a modern operating system, however, we need more sophisticated
interrupt-handling features.
13.2 I/O Hardware 593
1. We need the ability to defer interrupt handling during critical processing.
2. We need an efficient way to dispatch to the proper interrupt handler for
a device without first polling all the devices to see which one raised the
interrupt.
3. We need multilevel interrupts, so that the operating system can distinguish between high- and low-priority interrupts and can respond with
the appropriate degree of urgency.
In modern computer hardware, these three features are provided by the CPU
and by the interrupt-controller hardware.
Most CPUs have two interrupt request lines. One is the nonmaskable
interrupt, which is reserved for events such as unrecoverable memory errors.
The second interrupt line is maskable: it can be turned off by the CPU before
the execution of critical instruction sequences that must not be interrupted.
The maskable interrupt is used by device controllers to request service.
The interrupt mechanism accepts an address—a number that selects a
specific interrupt-handling routine from a small set. In most architectures, this
address is an offset in a table called the interrupt vector. This vector contains
the memory addresses of specialized interrupt handlers. The purpose of a
vectored interrupt mechanism is to reduce the need for a single interrupt
handler to search all possible sources of interrupts to determine which one
needs service. In practice, however, computers have more devices (and, hence,
interrupt handlers) than they have address elements in the interrupt vector.
A common way to solve this problem is to use interrupt chaining, in which
each element in the interrupt vector points to the head of a list of interrupt
handlers. When an interrupt is raised, the handlers on the corresponding list
are called one by one, until one is found that can service the request. This
structure is a compromise between the overhead of a huge interrupt table and
the inefficiency of dispatching to a single interrupt handler.
Figure 13.4 illustrates the design of the interrupt vector for the Intel Pentium
processor. The events from 0 to 31, which are nonmaskable, are used to signal
various error conditions. The events from 32 to 255, which are maskable, are
used for purposes such as device-generated interrupts.
The interrupt mechanism also implements a system of interrupt priority
levels. These levels enable the CPU to defer the handling of low-priority
interrupts without masking all interrupts and makes it possible for a highpriority interrupt to preempt the execution of a low-priority interrupt.
A modern operating system interacts with the interrupt mechanism in
several ways. At boot time, the operating system probes the hardware buses
to determine what devices are present and installs the corresponding interrupt
handlers into the interrupt vector. During I/O, the various device controllers
raise interrupts when they are ready for service. These interrupts signify that
output has completed, or that input data are available, or that a failure has
been detected. The interrupt mechanism is also used to handle a wide variety of
exceptions, such as dividing by 0, accessing a protected or nonexistent memory
address, or attempting to execute a privileged instruction from user mode. The
events that trigger interrupts have a common property: they are occurrences
that induce the operating system to execute an urgent, self-contained routine.
594 Chapter 13 I/O Systems
vector number description
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19–31
32–255
divide error
debug exception
null interrupt
breakpoint
INTO-detected overflow
bound range exception
invalid opcode
device not available
double fault
coprocessor segment overrun (reserved)
invalid task state segment
segment not present
stack fault
general protection
page fault
(Intel reserved, do not use)
floating-point error
alignment check
machine check
(Intel reserved, do not use)
maskable interrupts
Figure 13.4 Intel Pentium processor event-vector table.
An operating system has other good uses for an efficient hardware and
software mechanism that saves a small amount of processor state and then
calls a privileged routine in the kernel. For example, many operating systems
use the interrupt mechanism for virtual memory paging. A page fault is an
exception that raises an interrupt. The interrupt suspends the current process
and jumps to the page-fault handler in the kernel. This handler saves the state
of the process, moves the process to the wait queue, performs page-cache
management, schedules an I/O operation to fetch the page, schedules another
process to resume execution, and then returns from the interrupt.
Another example is found in the implementation of system calls. Usually,
a program uses library calls to issue system calls. The library routines check
the arguments given by the application, build a data structure to convey
the arguments to the kernel, and then execute a special instruction called a
software interrupt, or trap. This instruction has an operand that identifies
the desired kernel service. When a process executes the trap instruction, the
interrupt hardware saves the state of the user code, switches to kernel mode,
and dispatches to the kernel routine that implements the requested service. The
trap is given a relatively low interrupt priority compared with those assigned
to device interrupts—executing a system call on behalf of an application is less
urgent than servicing a device controller before its FIFO queue overflows and
loses data.
Interrupts can also be used to manage the flow of control within the kernel.
For example, consider one example of the processing required to complete
13.2 I/O Hardware 595
a disk read. One step is to copy data from kernel space to the user buffer.
This copying is time consuming but not urgent—it should not block other
high-priority interrupt handling. Another step is to start the next pending I/O
for that disk drive. This step has higher priority. If the disks are to be used
efficiently, we need to start the next I/O as soon as the previous one completes.
Consequently, a pair of interrupt handlers implements the kernel code that
completes a disk read. The high-priority handler records the I/O status, clears
the device interrupt, starts the next pending I/O, and raises a low-priority
interrupt to complete the work. Later, when the CPU is not occupied with highpriority work, the low-priority interrupt will be dispatched. The corresponding
handler completes the user-level I/O by copying data from kernel buffers to
the application space and then calling the scheduler to place the application
on the ready queue.
A threaded kernel architecture is well suited to implement multiple
interrupt priorities and to enforce the precedence of interrupt handling over
background processing in kernel and application routines. We illustrate this
point with the Solaris kernel. In Solaris, interrupt handlers are executed
as kernel threads. A range of high priorities is reserved for these threads.
These priorities give interrupt handlers precedence over application code and
kernel housekeeping and implement the priority relationships among interrupt
handlers. The priorities cause the Solaris thread scheduler to preempt lowpriority interrupt handlers in favor of higher-priority ones, and the threaded
implementation enables multiprocessor hardware to run several interrupt
handlers concurrently. We describe the interrupt architecture of Windows XP
and UNIX in Chapter 19 and Appendix A, respectively.
In summary, interrupts are used throughout modern operating systems to
handle asynchronous events and to trap to supervisor-mode routines in the
kernel. To enable the most urgent work to be done first, modern computers
use a system of interrupt priorities. Device controllers, hardware faults, and
system calls all raise interrupts to trigger kernel routines. Because interrupts
are used so heavily for time-sensitive processing, efficient interrupt handling
is required for good system performance.
13.2.3 Direct Memory Access
For a device that does large transfers, such as a disk drive, it seems wasteful
to use an expensive general-purpose processor to watch status bits and to
feed data into a controller register one byte at a time—a process termed
programmed I/O (PIO). Many computers avoid burdening the main CPU with
PIO by offloading some of this work to a special-purpose processor called a
direct-memory-access (DMA) controller. To initiate a DMA transfer, the host
writes a DMA command block into memory. This block contains a pointer to
the source of a transfer, a pointer to the destination of the transfer, and a count
of the number of bytes to be transferred. The CPU writes the address of this
command block to the DMA controller, then goes on with other work. The DMA
controller proceeds to operate the memory bus directly, placing addresses on
the bus to perform transfers without the help of the main CPU. A simple DMA
controller is a standard component in all modern computers, from smartphones
to mainframes.
596 Chapter 13 I/O Systems
Handshaking between the DMA controller and the device controller is
performed via a pair of wires called DMA-request and DMA-acknowledge.
The device controller places a signal on the DMA-request wire when a word
of data is available for transfer. This signal causes the DMA controller to seize
the memory bus, place the desired address on the memory-address wires,
and place a signal on the DMA-acknowledge wire. When the device controller
receives the DMA-acknowledge signal, it transfers the word of data to memory
and removes the DMA-request signal.
When the entire transfer is finished, the DMA controller interrupts the CPU.
This process is depicted in Figure 13.5. When the DMA controller seizes the
memory bus, the CPU is momentarily prevented from accessing main memory,
although it can still access data items in its primary and secondary caches.
Although this cycle stealing can slow down the CPU computation, offloading
the data-transfer work to a DMA controller generally improves the total system
performance. Some computer architectures use physical memory addresses for
DMA, but others perform direct virtual memory access (DVMA), using virtual
addresses that undergo translation to physical addresses. DVMA can perform
a transfer between two memory-mapped devices without the intervention of
the CPU or the use of main memory.
On protected-mode kernels, the operating system generally prevents
processes from issuing device commands directly. This discipline protects data
from access-control violations and also protects the system from erroneous use
of device controllers that could cause a system crash. Instead, the operating
system exports functions that a sufficiently privileged process can use to
access low-level operations on the underlying hardware. On kernels without
memory protection, processes can access device controllers directly. This direct
access can be used to achieve high performance, since it can avoid kernel
communication, context switches, and layers of kernel software. Unfortunately,
IDE disk
controller
DMA/bus/ x interrupt
controller
buffer x CPU memory bus memory
PCI bus
cache
CPU
5. DMA controller
 transfers bytes to
 buffer X, increasing
 memory address
 and decreasing C
 until C  0

1. device driver is told
 to transfer disk data
 to buffer at address X
2. device driver tells
 disk controller to
 transfer C bytes
 from disk to buffer
 at address X
6. when C  0, DMA
 interrupts CPU to signal
 transfer completion
3. disk controller initiates
 DMA transfer
4. disk controller sends
 each byte to DMA
 controller disk
disk
disk
disk
Figure 13.5 Steps in a DMA transfer.
13.3 Application I/O Interface 597
it interferes with system security and stability. The trend in general-purpose
operating systems is to protect memory and devices so that the system can try
to guard against erroneous or malicious applications.
13.2.4 I/O Hardware Summary
Although the hardware aspects of I/O are complex when considered at the
level of detail of electronics-hardware design, the concepts that we have
just described are sufficient to enable us to understand many I/O features
of operating systems. Let’s review the main concepts:
• A bus
• A controller
• An I/O port and its registers
• The handshaking relationship between the host and a device controller
• The execution of this handshaking in a polling loop or via interrupts
• The offloading of this work to a DMA controller for large transfers
We gave a basic example of the handshaking that takes place between a
device controller and the host earlier in this section. In reality, the wide variety
of available devices poses a problem for operating-system implementers. Each
kind of device has its own set of capabilities, control-bit definitions, and
protocols for interacting with the host—and they are all different. How can
the operating system be designed so that we can attach new devices to the
computer without rewriting the operating system? And when the devices
vary so widely, how can the operating system give a convenient, uniform I/O
interface to applications? We address those questions next.""",
"5.1 Background":"""
We’ve already seen that processes can execute concurrently or in parallel.
Section 3.2.2 introduced the role of process scheduling and described how
the CPU scheduler switches rapidly between processes to provide concurrent
execution. This means that one process may only partially complete execution
before another process is scheduled. In fact, a process may be interrupted at
any point in its instruction stream, and the processing core may be assigned
to execute instructions of another process. Additionally, Section 4.2 introduced
parallel execution, in which two instruction streams (representing different
processes) execute simultaneously on separate processing cores. In this chapter,
203
204 Chapter 5 Process Synchronization
we explain how concurrent or parallel execution can contribute to issues
involving the integrity of data shared by several processes.
Let’s consider an example of how this can happen. In Chapter 3, we developed a model of a system consisting of cooperating sequential processes or
threads, all running asynchronously and possibly sharing data. We illustrated
this model with the producer–consumer problem, which is representative of
operating systems. Specifically, in Section 3.4.1, we described how a bounded
buffer could be used to enable processes to share memory.
We now return to our consideration of the bounded buffer. As we pointed
out, our original solution allowed at most BUFFER SIZE − 1 items in the buffer
at the same time. Suppose we want to modify the algorithm to remedy this
deficiency. One possibility is to add an integer variable counter, initialized to
0. counter is incremented every time we add a new item to the buffer and is
decremented every time we remove one item from the buffer. The code for the
producer process can be modified as follows:
while (true) {
/* produce an item in next produced */
while (counter == BUFFER SIZE)
; /* do nothing */
buffer[in] = next produced;
in = (in + 1) % BUFFER SIZE;
counter++;
}
The code for the consumer process can be modified as follows:
while (true) {
while (counter == 0)
; /* do nothing */
next consumed = buffer[out];
out = (out + 1) % BUFFER SIZE;
counter--;
/* consume the item in next consumed */
}
Although the producer and consumer routines shown above are correct
separately, they may not function correctly when executed concurrently. As
an illustration, suppose that the value of the variable counter is currently
5 and that the producer and consumer processes concurrently execute the
statements “counter++” and “counter--”. Following the execution of these
two statements, the value of the variable counter may be 4, 5, or 6! The only
correct result, though, is counter == 5, which is generated correctly if the
producer and consumer execute separately.
5.1 Background 205
We can show that the value of counter may be incorrect as follows. Note
that the statement “counter++” may be implemented in machine language (on
a typical machine) as follows:
register1 = counter
register1 = register1 + 1
counter = register1
where register1 is one of the local CPU registers. Similarly, the statement
“counter--” is implemented as follows:
register2 = counter
register2 = register2 − 1
counter = register2
where again register2 is one of the local CPU registers. Even though register1 and
register2 may be the same physical register (an accumulator, say), remember
that the contents of this register will be saved and restored by the interrupt
handler (Section 1.2.3).
The concurrent execution of “counter++” and “counter--” is equivalent
to a sequential execution in which the lower-level statements presented
previously are interleaved in some arbitrary order (but the order within each
high-level statement is preserved). One such interleaving is the following:
T0: producer execute register1 = counter {register1 = 5}
T1: producer execute register1 = register1 + 1 {register1 = 6}
T2: consumer execute register2 = counter {register2 = 5}
T3: consumer execute register2 = register2 − 1 {register2 = 4}
T4: producer execute counter = register1 {counter = 6}
T5: consumer execute counter = register2 {counter = 4}
Notice that we have arrived at the incorrect state “counter == 4”, indicating
that four buffers are full, when, in fact, five buffers are full. If we reversed the
order of the statements at T4 and T5, we would arrive at the incorrect state
“counter == 6”.
We would arrive at this incorrect state because we allowed both processes
to manipulate the variable counter concurrently. A situation like this, where
several processes access and manipulate the same data concurrently and the
outcome of the execution depends on the particular order in which the access
takes place, is called a race condition. To guard against the race condition
above, we need to ensure that only one process at a time can be manipulating
the variable counter. To make such a guarantee, we require that the processes
be synchronized in some way.
Situations such as the one just described occur frequently in operating
systems as different parts of the system manipulate resources. Furthermore, as
we have emphasized in earlier chapters, the growing importance of multicore
systems has brought an increased emphasis on developing multithreaded
applications. In such applications, several threads—which are quite possibly
sharing data—are running in parallel on different processing cores. Clearly,
206 Chapter 5 Process Synchronization
do {
entry section
critical section
exit section
remainder section
} while (true);
Figure 5.1 General structure of a typical process Pi .
we want any changes that result from such activities not to interfere with one
another. Because of the importance of this issue, we devote a major portion of
this chapter to process synchronization and coordination among cooperating
processes.""",
"10.7 RAID Structure":"""Disk drives have continued to get smaller and cheaper, so it is now economically feasible to attach many disks to a computer system. Having a large
number of disks in a system presents opportunities for improving the rate
at which data can be read or written, if the disks are operated in parallel.
Furthermore, this setup offers the potential for improving the reliability of data
storage, because redundant information can be stored on multiple disks. Thus,
failure of one disk does not lead to loss of data. A variety of disk-organization
techniques, collectively called redundant arrays of independent disks (RAID),
are commonly used to address the performance and reliability issues.
In the past, RAIDs composed of small, cheap disks were viewed as a
cost-effective alternative to large, expensive disks. Today, RAIDs are used for
10.7 RAID Structure 485
STRUCTURING RAID
RAID storage can be structured in a variety of ways. For example, a system
can have disks directly attached to its buses. In this case, the operating
system or system software can implement RAID functionality. Alternatively,
an intelligent host controller can control multiple attached disks and can
implement RAID on those disks in hardware. Finally, a storage array, or RAID
array, can be used. A RAID array is a standalone unit with its own controller,
cache (usually), and disks. It is attached to the host via one or more standard
controllers (for example, FC). This common setup allows an operating system
or software without RAID functionality to have RAID-protected disks. It is
even used on systems that do have RAID software layers because of its
simplicity and flexibility.
their higher reliability and higher data-transfer rate, rather than for economic
reasons. Hence, the I in RAID, which once stood for “inexpensive,” now stands
for “independent.”
10.7.1 Improvement of Reliability via Redundancy
Let’s first consider the reliability of RAIDs. The chance that some disk out of
a set of N disks will fail is much higher than the chance that a specific single
disk will fail. Suppose that the mean time to failure of a single disk is 100,000
hours. Then the mean time to failure of some disk in an array of 100 disks
will be 100,000/100 = 1,000 hours, or 41.66 days, which is not long at all! If we
store only one copy of the data, then each disk failure will result in loss of a
significant amount of data—and such a high rate of data loss is unacceptable.
The solution to the problem of reliability is to introduce redundancy; we
store extra information that is not normally needed but that can be used in the
event of failure of a disk to rebuild the lost information. Thus, even if a disk
fails, data are not lost.
The simplest (but most expensive) approach to introducing redundancy is
to duplicate every disk. This technique is called mirroring. With mirroring, a
logical disk consists of two physical disks, and every write is carried out on
both disks. The result is called a mirrored volume. If one of the disks in the
volume fails, the data can be read from the other. Data will be lost only if the
second disk fails before the first failed disk is replaced.
The mean time to failure of a mirrored volume—where failure is the loss of
data—depends on two factors. One is the mean time to failure of the individual
disks. The other is the mean time to repair, which is the time it takes (on
average) to replace a failed disk and to restore the data on it. Suppose that the
failures of the two disks are independent; that is, the failure of one disk is not
connected to the failure of the other. Then, if the mean time to failure of a single
disk is 100,000 hours and the mean time to repair is 10 hours, the mean time
to data loss of a mirrored disk system is 100, 0002/(2 ∗ 10) = 500 ∗ 106 hours,
or 57,000 years!
486 Chapter 10 Mass-Storage Structure
You should be aware that we cannot really assume that disk failures will
be independent. Power failures and natural disasters, such as earthquakes,
fires, and floods, may result in damage to both disks at the same time.
Also, manufacturing defects in a batch of disks can cause correlated failures.
As disks age, the probability of failure grows, increasing the chance that a
second disk will fail while the first is being repaired. In spite of all these
considerations, however, mirrored-disk systems offer much higher reliability
than do single-disk systems.
Power failures are a particular source of concern, since they occur far more
frequently than do natural disasters. Even with mirroring of disks, if writes are
in progress to the same block in both disks, and power fails before both blocks
are fully written, the two blocks can be in an inconsistent state. One solution
to this problem is to write one copy first, then the next. Another is to add a
solid-state nonvolatile RAM (NVRAM) cache to the RAID array. This write-back
cache is protected from data loss during power failures, so the write can be
considered complete at that point, assuming the NVRAM has some kind of error
protection and correction, such as ECC or mirroring.
10.7.2 Improvement in Performance via Parallelism
Now let’s consider how parallel access to multiple disks improves performance. With disk mirroring, the rate at which read requests can be handled is
doubled, since read requests can be sent to either disk (as long as both disks
in a pair are functional, as is almost always the case). The transfer rate of each
read is the same as in a single-disk system, but the number of reads per unit
time has doubled.
With multiple disks, we can improve the transfer rate as well (or instead)
by striping data across the disks. In its simplest form, data striping consists
of splitting the bits of each byte across multiple disks; such striping is called
bit-level striping. For example, if we have an array of eight disks, we write
bit i of each byte to disk i. The array of eight disks can be treated as a single
disk with sectors that are eight times the normal size and, more important, that
have eight times the access rate. Every disk participates in every access (read
or write); so the number of accesses that can be processed per second is about
the same as on a single disk, but each access can read eight times as many data
in the same time as on a single disk.
Bit-level striping can be generalized to include a number of disks that either
is a multiple of 8 or divides 8. For example, if we use an array of four disks,
bits i and 4 + i of each byte go to disk i. Further, striping need not occur at
the bit level. In block-level striping, for instance, blocks of a file are striped
across multiple disks; with n disks, block i of a file goes to disk (i mod n) + 1.
Other levels of striping, such as bytes of a sector or sectors of a block, also are
possible. Block-level striping is the most common.
Parallelism in a disk system, as achieved through striping, has two main
goals:
1. Increase the throughput of multiple small accesses (that is, page accesses)
by load balancing.
2. Reduce the response time of large accesses.
10.7 RAID Structure 487
10.7.3 RAID Levels
Mirroring provides high reliability, but it is expensive. Striping provides high
data-transfer rates, but it does not improve reliability. Numerous schemes
to provide redundancy at lower cost by using disk striping combined with
“parity” bits (which we describe shortly) have been proposed. These schemes
have different cost–performance trade-offs and are classified according to
levels called RAID levels. We describe the various levels here; Figure 10.11
shows them pictorially (in the figure, P indicates error-correcting bits and C
indicates a second copy of the data). In all cases depicted in the figure, four
disks’ worth of data are stored, and the extra disks are used to store redundant
information for failure recovery.
(a) RAID 0: non-redundant striping.
(b) RAID 1: mirrored disks.
C C C C
(c) RAID 2: memory-style error-correcting codes.
(d) RAID 3: bit-interleaved parity.
(e) RAID 4: block-interleaved parity.
(f) RAID 5: block-interleaved distributed parity.
P P
P
P
PPP
(g) RAID 6: P  Q redundancy.
P P P
P
P P P P
P
P
P
P
P
Figure 10.11 RAID levels.
488 Chapter 10 Mass-Storage Structure
• RAID level 0. RAID level 0 refers to disk arrays with striping at the level of
blocks but without any redundancy (such as mirroring or parity bits), as
shown in Figure 10.11(a).
• RAID level 1. RAID level 1 refers to disk mirroring. Figure 10.11(b) shows
a mirrored organization.
• RAID level 2. RAID level 2 is also known as memory-style error-correctingcode (ECC) organization. Memory systems have long detected certain
errors by using parity bits. Each byte in a memory system may have a
parity bit associated with it that records whether the number of bits in the
byte set to 1 is even (parity = 0) or odd (parity = 1). If one of the bits in the
byte is damaged (either a 1 becomes a 0, or a 0 becomes a 1), the parity of
the byte changes and thus does not match the stored parity. Similarly, if the
stored parity bit is damaged, it does not match the computed parity. Thus,
all single-bit errors are detected by the memory system. Error-correcting
schemes store two or more extra bits and can reconstruct the data if a single
bit is damaged.
The idea of ECC can be used directly in disk arrays via striping of
bytes across disks. For example, the first bit of each byte can be stored in
disk 1, the second bit in disk 2, and so on until the eighth bit is stored in
disk 8; the error-correction bits are stored in further disks. This scheme
is shown in Figure 10.11(c), where the disks labeled P store the errorcorrection bits. If one of the disks fails, the remaining bits of the byte and
the associated error-correction bits can be read from other disks and used
to reconstruct the damaged data. Note that RAID level 2 requires only three
disks’ overhead for four disks of data, unlike RAID level 1, which requires
four disks’ overhead.
• RAID level 3. RAID level 3, or bit-interleaved parity organization, improves
on level 2 by taking into account the fact that, unlike memory systems, disk
controllers can detect whether a sector has been read correctly, so a single
parity bit can be used for error correction as well as for detection. The idea
is as follows: If one of the sectors is damaged, we know exactly which
sector it is, and we can figure out whether any bit in the sector is a 1 or
a 0 by computing the parity of the corresponding bits from sectors in the
other disks. If the parity of the remaining bits is equal to the stored parity,
the missing bit is 0; otherwise, it is 1. RAID level 3 is as good as level 2 but is
less expensive in the number of extra disks required (it has only a one-disk
overhead), so level 2 is not used in practice. Level 3 is shown pictorially in
Figure 10.11(d).
RAID level 3 has two advantages over level 1. First, the storage overhead is reduced because only one parity disk is needed for several regular
disks, whereas one mirror disk is needed for every disk in level 1. Second,
since reads and writes of a byte are spread out over multiple disks with
N-way striping of data, the transfer rate for reading or writing a single
block is N times as fast as with RAID level 1. On the negative side, RAID
level 3 supports fewer I/Os per second, since every disk has to participate
in every I/O request.
A further performance problem with RAID 3—and with all paritybased RAID levels—is the expense of computing and writing the parity.
10.7 RAID Structure 489
This overhead results in significantly slower writes than with non-parity
RAID arrays. To moderate this performance penalty, many RAID storage
arrays include a hardware controller with dedicated parity hardware. This
controller offloads the parity computation from the CPU to the array. The
array has an NVRAM cache as well, to store the blocks while the parity is
computed and to buffer the writes from the controller to the spindles. This
combination can make parity RAID almost as fast as non-parity. In fact, a
caching array doing parity RAID can outperform a non-caching non-parity
RAID.
• RAID level 4. RAID level 4, or block-interleaved parity organization, uses
block-level striping, as in RAID 0, and in addition keeps a parity block on
a separate disk for corresponding blocks from N other disks. This scheme
is diagrammed in Figure 10.11(e). If one of the disks fails, the parity block
can be used with the corresponding blocks from the other disks to restore
the blocks of the failed disk.
A block read accesses only one disk, allowing other requests to be
processed by the other disks. Thus, the data-transfer rate for each access
is slower, but multiple read accesses can proceed in parallel, leading to a
higher overall I/O rate. The transfer rates for large reads are high, since all
the disks can be read in parallel. Large writes also have high transfer rates,
since the data and parity can be written in parallel.
Small independent writes cannot be performed in parallel. An operatingsystem write of data smaller than a block requires that the block be read,
modified with the new data, and written back. The parity block has to be
updated as well. This is known as the read-modify-write cycle. Thus, a
single write requires four disk accesses: two to read the two old blocks and
two to write the two new blocks.
WAFL (which we cover in Chapter 12) uses RAID level 4 because this RAID
level allows disks to be added to a RAID set seamlessly. If the added disks
are initialized with blocks containing only zeros, then the parity value does
not change, and the RAID set is still correct.
• RAID level 5. RAID level 5, or block-interleaved distributed parity, differs
from level 4 in that it spreads data and parity among all N+1 disks, rather
than storing data in N disks and parity in one disk. For each block, one of
the disks stores the parity and the others store data. For example, with an
array of five disks, the parity for the nth block is stored in disk (n mod 5)+1.
The nth blocks of the other four disks store actual data for that block. This
setup is shown in Figure 10.11(f), where the Ps are distributed across all
the disks. A parity block cannot store parity for blocks in the same disk,
because a disk failure would result in loss of data as well as of parity, and
hence the loss would not be recoverable. By spreading the parity across
all the disks in the set, RAID 5 avoids potential overuse of a single parity
disk, which can occur with RAID 4. RAID 5 is the most common parity RAID
system.
• RAID level 6. RAID level 6, also called the P + Q redundancy scheme, is
much like RAID level 5 but stores extra redundant information to guard
against multiple disk failures. Instead of parity, error-correcting codes such
as the Reed–Solomon codes are used. In the scheme shown in Figure
490 Chapter 10 Mass-Storage Structure
10.11(g), 2 bits of redundant data are stored for every 4 bits of data—
compared with 1 parity bit in level 5—and the system can tolerate two
disk failures.
• RAID levels 0 + 1 and 1 + 0. RAID level 0 + 1 refers to a combination of RAID
levels 0 and 1. RAID 0 provides the performance, while RAID 1 provides
the reliability. Generally, this level provides better performance than RAID
5. It is common in environments where both performance and reliability
are important. Unfortunately, like RAID 1, it doubles the number of disks
needed for storage, so it is also relatively expensive. In RAID 0 + 1, a set
of disks are striped, and then the stripe is mirrored to another, equivalent
stripe.
Another RAID option that is becoming available commercially is RAID
level 1 + 0, in which disks are mirrored in pairs and then the resulting
mirrored pairs are striped. This scheme has some theoretical advantages
over RAID 0 + 1. For example, if a single disk fails in RAID 0 + 1, an entire
stripe is inaccessible, leaving only the other stripe. With a failure in RAID 1
+ 0, a single disk is unavailable, but the disk that mirrors it is still available,
as are all the rest of the disks (Figure 10.12).
Numerous variations have been proposed to the basic RAID schemes described
here. As a result, some confusion may exist about the exact definitions of the
different RAID levels.
x
x
mirror
a) RAID 0  1 with a single disk failure.
stripe
stripe
mirror
b) RAID 1  0 with a single disk failure.
stripe
mirror mirror mirror
Figure 10.12 RAID 0 + 1 and 1 + 0.
10.7 RAID Structure 491
The implementation of RAID is another area of variation. Consider the
following layers at which RAID can be implemented.
• Volume-management software can implement RAID within the kernel or
at the system software layer. In this case, the storage hardware can provide
minimal features and still be part of a full RAID solution. Parity RAID is
fairly slow when implemented in software, so typically RAID 0, 1, or 0 + 1
is used.
• RAID can be implemented in the host bus-adapter (HBA) hardware. Only
the disks directly connected to the HBA can be part of a given RAID set.
This solution is low in cost but not very flexible.
• RAID can be implemented in the hardware of the storage array. The storage
array can create RAID sets of various levels and can even slice these sets
into smaller volumes, which are then presented to the operating system.
The operating system need only implement the file system on each of the
volumes. Arrays can have multiple connections available or can be part of
a SAN, allowing multiple hosts to take advantage of the array’s features.
• RAID can be implemented in the SAN interconnect layer by disk virtualization devices. In this case, a device sits between the hosts and the storage.
It accepts commands from the servers and manages access to the storage.
It could provide mirroring, for example, by writing each block to two
separate storage devices.
Other features, such as snapshots and replication, can be implemented
at each of these levels as well. A snapshot is a view of the file system
before the last update took place. (Snapshots are covered more fully in
Chapter 12.) Replication involves the automatic duplication of writes between
separate sites for redundancy and disaster recovery. Replication can be
synchronous or asynchronous. In synchronous replication, each block must be
written locally and remotely before the write is considered complete, whereas
in asynchronous replication, the writes are grouped together and written
periodically. Asynchronous replication can result in data loss if the primary
site fails, but it is faster and has no distance limitations.
The implementation of these features differs depending on the layer at
which RAID is implemented. For example, if RAID is implemented in software,
then each host may need to carry out and manage its own replication. If
replication is implemented in the storage array or in the SAN interconnect,
however, then whatever the host operating system or its features, the host’s
data can be replicated.
One other aspect of most RAID implementations is a hot spare disk or disks.
A hot spare is not used for data but is configured to be used as a replacement in
case of disk failure. For instance, a hot spare can be used to rebuild a mirrored
pair should one of the disks in the pair fail. In this way, the RAID level can be
reestablished automatically, without waiting for the failed disk to be replaced.
Allocating more than one hot spare allows more than one failure to be repaired
without human intervention.
492 Chapter 10 Mass-Storage Structure
10.7.4 Selecting a RAID Level
Given the many choices they have, how do system designers choose a RAID
level? One consideration is rebuild performance. If a disk fails, the time needed
to rebuild its data can be significant. This may be an important factor if a
continuous supply of data is required, as it is in high-performance or interactive
database systems. Furthermore, rebuild performance influences the mean time
to failure.
Rebuild performance varies with the RAID level used. Rebuilding is easiest
for RAID level 1, since data can be copied from another disk. For the other
levels, we need to access all the other disks in the array to rebuild data in a
failed disk. Rebuild times can be hours for RAID 5 rebuilds of large disk sets.
RAID level 0 is used in high-performance applications where data loss is
not critical. RAID level 1 is popular for applications that require high reliability
with fast recovery. RAID 0 + 1 and 1 + 0 are used where both performance and
reliability are important—for example, for small databases. Due to RAID 1’s
high space overhead, RAID 5 is often preferred for storing large volumes of
data. Level 6 is not supported currently by many RAID implementations, but it
should offer better reliability than level 5.
RAID system designers and administrators of storage have to make several
other decisions as well. For example, how many disks should be in a given
RAID set? How many bits should be protected by each parity bit? If more disks
are in an array, data-transfer rates are higher, but the system is more expensive.
If more bits are protected by a parity bit, the space overhead due to parity bits
is lower, but the chance that a second disk will fail before the first failed disk is
repaired is greater, and that will result in data loss.
10.7.5 Extensions
The concepts of RAID have been generalized to other storage devices, including
arrays of tapes, and even to the broadcast of data over wireless systems. When
applied to arrays of tapes, RAID structures are able to recover data even if one
of the tapes in an array is damaged. When applied to broadcast of data, a block
of data is split into short units and is broadcast along with a parity unit. If one
of the units is not received for any reason, it can be reconstructed from the
other units. Commonly, tape-drive robots containing multiple tape drives will
stripe data across all the drives to increase throughput and decrease backup
time.
10.7.6 Problems with RAID
Unfortunately, RAID does not always assure that data are available for the
operating system and its users. A pointer to a file could be wrong, for example,
or pointers within the file structure could be wrong. Incomplete writes, if not
properly recovered, could result in corrupt data. Some other process could
accidentally write over a file system’s structures, too. RAID protects against
physical media errors, but not other hardware and software errors. As large as
is the landscape of software and hardware bugs, that is how numerous are the
potential perils for data on a system.
The Solaris ZFS file system takes an innovative approach to solving these
problems through the use of checksums—a technique used to verify the
10.7 RAID Structure 493
THE InServ STORAGE ARRAY
Innovation, in an effort to provide better, faster, and less expensive solutions,
frequently blurs the lines that separated previous technologies. Consider the
InServ storage array from 3Par. Unlike most other storage arrays, InServ
does not require that a set of disks be configured at a specific RAID level.
Rather, each disk is broken into 256-MB “chunklets.” RAID is then applied at
the chunklet level. A disk can thus participate in multiple and various RAID
levels as its chunklets are used for multiple volumes.
InServ also provides snapshots similar to those created by the WAFL file
system. The format of InServ snapshots can be read–write as well as readonly, allowing multiple hosts to mount copies of a given file system without
needing their own copies of the entire file system. Any changes a host makes
in its own copy are copy-on-write and so are not reflected in the other copies.
A further innovation is utility storage. Some file systems do not expand
or shrink. On these systems, the original size is the only size, and any change
requires copying data. An administrator can configure InServ to provide a
host with a large amount of logical storage that initially occupies only a small
amount of physical storage. As the host starts using the storage, unused disks
are allocated to the host, up to the original logical level. The host thus can
believe that it has a large fixed storage space, create its file systems there, and
so on. Disks can be added or removed from the file system by InServ without
the file system’s noticing the change. This feature can reduce the number of
drives needed by hosts, or at least delay the purchase of disks until they are
really needed.
integrity of data. ZFS maintains internal checksums of all blocks, including
data and metadata. These checksums are not kept with the block that is being
checksummed. Rather, they are stored with the pointer to that block. (See Figure
10.13.) Consider an inode — a data structure for storing file system metadata
— with pointers to its data. Within the inode is the checksum of each block
of data. If there is a problem with the data, the checksum will be incorrect,
and the file system will know about it. If the data are mirrored, and there is a
block with a correct checksum and one with an incorrect checksum, ZFS will
automatically update the bad block with the good one. Similarly, the directory
entry that points to the inode has a checksum for the inode. Any problem
in the inode is detected when the directory is accessed. This checksumming
takes places throughout all ZFS structures, providing a much higher level of
consistency, error detection, and error correction than is found in RAID disk sets
or standard file systems. The extra overhead that is created by the checksum
calculation and extra block read-modify-write cycles is not noticeable because
the overall performance of ZFS is very fast.
Another issue with most RAID implementations is lack of flexibility.
Consider a storage array with twenty disks divided into four sets of five disks.
Each set of five disks is a RAID level 5 set. As a result, there are four separate
volumes, each holding a file system. But what if one file system is too large to fit
on a five-disk RAID level 5 set? And what if another file system needs very little
space? If such factors are known ahead of time, then the disks and volumes
494 Chapter 10 Mass-Storage Structure
metadata block 1
address 1
checksum MB2 checksum
address 2
metadata block 2
address
checksum D1 checksum D2
data 1 data 2
address
Figure 10.13 ZFS checksums all metadata and data.
can be properly allocated. Very frequently, however, disk use and requirements
change over time.
Even if the storage array allowed the entire set of twenty disks to be
created as one large RAID set, other issues could arise. Several volumes of
various sizes could be built on the set. But some volume managers do not
allow us to change a volume’s size. In that case, we would be left with the same
issue described above—mismatched file-system sizes. Some volume managers
allow size changes, but some file systems do not allow for file-system growth
or shrinkage. The volumes could change sizes, but the file systems would need
to be recreated to take advantage of those changes.
ZFS combines file-system management and volume management into a
unit providing greater functionality than the traditional separation of those
functions allows. Disks, or partitions of disks, are gathered together via RAID
sets into pools of storage. A pool can hold one or more ZFS file systems. The
entire pool’s free space is available to all file systems within that pool. ZFS uses
the memory model of malloc() and free() to allocate and release storage for
each file system as blocks are used and freed within the file system. As a result,
there are no artificial limits on storage use and no need to relocate file systems
between volumes or resize volumes. ZFS provides quotas to limit the size of a
file system and reservations to assure that a file system can grow by a specified
amount, but those variables can be changed by the file-system owner at any
time. Figure 10.14(a) depicts traditional volumes and file systems, and Figure
10.14(b) shows the ZFS model.""",
"10.1 Overview of Mass-Storage Structure":"""In this section, we present a general overview of the physical structure of
secondary and tertiary storage devices.
10.1.1 Magnetic Disks
Magnetic disks provide the bulk of secondary storage for modern computer
systems. Conceptually, disks are relatively simple (Figure 10.1). Each disk
platter has a flat circular shape, like a CD. Common platter diameters range
from 1.8 to 3.5 inches. The two surfaces of a platter are covered with a magnetic
material. We store information by recording it magnetically on the platters.
467
468 Chapter 10 Mass-Storage Structure
track t
sector s
spindle
cylinder c
platter
arm
read-write
head
arm assembly
rotation
Figure 10.1 Moving-head disk mechanism.
A read–write head “flies” just above each surface of every platter. The
heads are attached to a disk arm that moves all the heads as a unit. The surface
of a platter is logically divided into circular tracks, which are subdivided into
sectors. The set of tracks that are at one arm position makes up a cylinder.
There may be thousands of concentric cylinders in a disk drive, and each track
may contain hundreds of sectors. The storage capacity of common disk drives
is measured in gigabytes.
When the disk is in use, a drive motor spins it at high speed. Most drives
rotate 60 to 250 times per second, specified in terms of rotations per minute
(RPM). Common drives spin at 5,400, 7,200, 10,000, and 15,000 RPM. Disk speed
has two parts. The transfer rate is the rate at which data flow between the drive
and the computer. The positioning time, or random-access time, consists of
two parts: the time necessary to move the disk arm to the desired cylinder,
called the seek time, and the time necessary for the desired sector to rotate to
the disk head, called the rotational latency. Typical disks can transfer several
megabytes of data per second, and they have seek times and rotational latencies
of several milliseconds.
Because the disk head flies on an extremely thin cushion of air (measured
in microns), there is a danger that the head will make contact with the disk
surface. Although the disk platters are coated with a thin protective layer, the
head will sometimes damage the magnetic surface. This accident is called a
head crash. A head crash normally cannot be repaired; the entire disk must be
replaced.
A disk can be removable, allowing different disks to be mounted as needed.
Removable magnetic disks generally consist of one platter, held in a plastic
case to prevent damage while not in the disk drive. Other forms of removable
disks include CDs, DVDs, and Blu-ray discs as well as removable flash-memory
devices known as flash drives (which are a type of solid-state drive).
10.1 Overview of Mass-Storage Structure 469
A disk drive is attached to a computer by a set of wires called an I/O
bus. Several kinds of buses are available, including advanced technology
attachment (ATA), serial ATA (SATA), eSATA, universal serial bus (USB), and
fibre channel (FC). The data transfers on a bus are carried out by special
electronic processors called controllers. The host controller is the controller at
the computer end of the bus. A disk controller is built into each disk drive. To
perform a disk I/O operation, the computer places a command into the host
controller, typically using memory-mapped I/O ports, as described in Section
9.7.3. The host controller then sends the command via messages to the disk
controller, and the disk controller operates the disk-drive hardware to carry
out the command. Disk controllers usually have a built-in cache. Data transfer
at the disk drive happens between the cache and the disk surface, and data
transfer to the host, at fast electronic speeds, occurs between the cache and the
host controller.
10.1.2 Solid-State Disks
Sometimes old technologies are used in new ways as economics change or
the technologies evolve. An example is the growing importance of solid-state
disks, or SSDs. Simply described, an SSD is nonvolatile memory that is used like
a hard drive. There are many variations of this technology, from DRAM with a
battery to allow it to maintain its state in a power failure through flash-memory
technologies like single-level cell (SLC) and multilevel cell (MLC) chips.
SSDs have the same characteristics as traditional hard disks but can be more
reliable because they have no moving parts and faster because they have no
seek time or latency. In addition, they consume less power. However, they are
more expensive per megabyte than traditional hard disks, have less capacity
than the larger hard disks, and may have shorter life spans than hard disks,
so their uses are somewhat limited. One use for SSDs is in storage arrays,
where they hold file-system metadata that require high performance. SSDs are
also used in some laptop computers to make them smaller, faster, and more
energy-efficient.
Because SSDs can be much faster than magnetic disk drives, standard bus
interfaces can cause a major limit on throughput. Some SSDs are designed to
connect directly to the system bus (PCI, for example). SSDs are changing other
traditional aspects of computer design as well. Some systems use them as
a direct replacement for disk drives, while others use them as a new cache
tier, moving data between magnetic disks, SSDs, and memory to optimize
performance.
In the remainder of this chapter, some sections pertain to SSDs, while
others do not. For example, because SSDs have no disk head, disk-scheduling
algorithms largely do not apply. Throughput and formatting, however, do
apply.
10.1.3 Magnetic Tapes
Magnetic tape was used as an early secondary-storage medium. Although it
is relatively permanent and can hold large quantities of data, its access time
is slow compared with that of main memory and magnetic disk. In addition,
random access to magnetic tape is about a thousand times slower than random
access to magnetic disk, so tapes are not very useful for secondary storage.
470 Chapter 10 Mass-Storage Structure
DISK TRANSFER RATES
As with many aspects of computing, published performance numbers for
disks are not the same as real-world performance numbers. Stated transfer
rates are always lower than effective transfer rates, for example. The transfer
rate may be the rate at which bits can be read from the magnetic media by
the disk head, but that is different from the rate at which blocks are delivered
to the operating system.
Tapes are used mainly for backup, for storage of infrequently used information,
and as a medium for transferring information from one system to another.
A tape is kept in a spool and is wound or rewound past a read–write head.
Moving to the correct spot on a tape can take minutes, but once positioned, tape
drives can write data at speeds comparable to disk drives. Tape capacities vary
greatly, depending on the particular kind of tape drive, with current capacities
exceeding several terabytes. Some tapes have built-in compression that can
more than double the effective storage. Tapes and their drivers are usually
categorized by width, including 4, 8, and 19 millimeters and 1/4 and 1/2 inch.
Some are named according to technology, such as LTO-5 and SDLT.""",
"10.4 Disk Scheduling":"""One of the responsibilities of the operating system is to use the hardware
efficiently. For the disk drives, meeting this responsibility entails having fast
10.4 Disk Scheduling 473
LAN/WAN
storage
array
storage
array
data-processing
center
web content
provider
server
client
client
client server
tape
library
SAN
Figure 10.3 Storage-area network.
access time and large disk bandwidth. For magnetic disks, the access time has
two major components, as mentioned in Section 10.1.1. The seek time is the
time for the disk arm to move the heads to the cylinder containing the desired
sector. The rotational latency is the additional time for the disk to rotate the
desired sector to the disk head. The disk bandwidth is the total number of bytes
transferred, divided by the total time between the first request for service and
the completion of the last transfer. We can improve both the access time and
the bandwidth by managing the order in which disk I/O requests are serviced.
Whenever a process needs I/O to or from the disk, it issues a system call to
the operating system. The request specifies several pieces of information:
• Whether this operation is input or output
• What the disk address for the transfer is
• What the memory address for the transfer is
• What the number of sectors to be transferred is
If the desired disk drive and controller are available, the request can be
serviced immediately. If the drive or controller is busy, any new requests
for service will be placed in the queue of pending requests for that drive.
For a multiprogramming system with many processes, the disk queue may
often have several pending requests. Thus, when one request is completed, the
operating system chooses which pending request to service next. How does
the operating system make this choice? Any one of several disk-scheduling
algorithms can be used, and we discuss them next.
10.4.1 FCFS Scheduling
The simplest form of disk scheduling is, of course, the first-come, first-served
(FCFS) algorithm. This algorithm is intrinsically fair, but it generally does not
provide the fastest service. Consider, for example, a disk queue with requests
for I/O to blocks on cylinders
98, 183, 37, 122, 14, 124, 65, 67,
474 Chapter 10 Mass-Storage Structure
0 14 37 536567 98 122124 183199
queue  98, 183, 37, 122, 14, 124, 65, 67
head starts at 53
Figure 10.4 FCFS disk scheduling.
in that order. If the disk head is initially at cylinder 53, it will first move from
53 to 98, then to 183, 37, 122, 14, 124, 65, and finally to 67, for a total head
movement of 640 cylinders. This schedule is diagrammed in Figure 10.4.
The wild swing from 122 to 14 and then back to 124 illustrates the problem
with this schedule. If the requests for cylinders 37 and 14 could be serviced
together, before or after the requests for 122 and 124, the total head movement
could be decreased substantially, and performance could be thereby improved.
10.4.2 SSTF Scheduling
It seems reasonable to service all the requests close to the current head position
before moving the head far away to service other requests. This assumption is
the basis for the shortest-seek-time-first (SSTF) algorithm. The SSTF algorithm
selects the request with the least seek time from the current head position.
In other words, SSTF chooses the pending request closest to the current head
position.
For our example request queue, the closest request to the initial head
position (53) is at cylinder 65. Once we are at cylinder 65, the next closest
request is at cylinder 67. From there, the request at cylinder 37 is closer than the
one at 98, so 37 is served next. Continuing, we service the request at cylinder 14,
then 98, 122, 124, and finally 183 (Figure 10.5). This scheduling method results
in a total head movement of only 236 cylinders—little more than one-third
of the distance needed for FCFS scheduling of this request queue. Clearly, this
algorithm gives a substantial improvement in performance.
SSTF scheduling is essentially a form of shortest-job-first (SJF) scheduling;
and like SJF scheduling, it may cause starvation of some requests. Remember
that requests may arrive at any time. Suppose that we have two requests in
the queue, for cylinders 14 and 186, and while the request from 14 is being
serviced, a new request near 14 arrives. This new request will be serviced
next, making the request at 186 wait. While this request is being serviced,
another request close to 14 could arrive. In theory, a continual stream of requests
near one another could cause the request for cylinder 186 to wait indefinitely.
10.4 Disk Scheduling 475
0 14 37 536567 98 122124 183199
queue  98, 183, 37, 122, 14, 124, 65, 67
head starts at 53
Figure 10.5 SSTF disk scheduling.
This scenario becomes increasingly likely as the pending-request queue grows
longer.
Although the SSTF algorithm is a substantial improvement over the FCFS
algorithm, it is not optimal. In the example, we can do better by moving the
head from 53 to 37, even though the latter is not closest, and then to 14, before
turning around to service 65, 67, 98, 122, 124, and 183. This strategy reduces
the total head movement to 208 cylinders.
10.4.3 SCAN Scheduling
In the SCAN algorithm, the disk arm starts at one end of the disk and moves
toward the other end, servicing requests as it reaches each cylinder, until it gets
to the other end of the disk. At the other end, the direction of head movement
is reversed, and servicing continues. The head continuously scans back and
forth across the disk. The SCAN algorithm is sometimes called the elevator
algorithm, since the disk arm behaves just like an elevator in a building, first
servicing all the requests going up and then reversing to service requests the
other way.
Let’s return to our example to illustrate. Before applying SCAN to schedule
the requests on cylinders 98, 183, 37, 122, 14, 124, 65, and 67, we need to know
the direction of head movement in addition to the head’s current position.
Assuming that the disk arm is moving toward 0 and that the initial head
position is again 53, the head will next service 37 and then 14. At cylinder 0,
the arm will reverse and will move toward the other end of the disk, servicing
the requests at 65, 67, 98, 122, 124, and 183 (Figure 10.6). If a request arrives in
the queue just in front of the head, it will be serviced almost immediately; a
request arriving just behind the head will have to wait until the arm moves to
the end of the disk, reverses direction, and comes back.
Assuming a uniform distribution of requests for cylinders, consider the
density of requests when the head reaches one end and reverses direction. At
this point, relatively few requests are immediately in front of the head, since
these cylinders have recently been serviced. The heaviest density of requests
476 Chapter 10 Mass-Storage Structure
0 14 37 536567 98 122124 183199
queue  98, 183, 37, 122, 14, 124, 65, 67
head starts at 53
Figure 10.6 SCAN disk scheduling.
is at the other end of the disk. These requests have also waited the longest, so
why not go there first? That is the idea of the next algorithm.
10.4.4 C-SCAN Scheduling
Circular SCAN (C-SCAN) scheduling is a variant of SCAN designed to provide
a more uniform wait time. Like SCAN, C-SCAN moves the head from one end
of the disk to the other, servicing requests along the way. When the head
reaches the other end, however, it immediately returns to the beginning of
the disk without servicing any requests on the return trip (Figure 10.7). The
C-SCAN scheduling algorithm essentially treats the cylinders as a circular list
that wraps around from the final cylinder to the first one.
0 14 37 53 65 67 98 122124 183199
queue = 98, 183, 37, 122, 14, 124, 65, 67
head starts at 53
Figure 10.7 C-SCAN disk scheduling.
10.4 Disk Scheduling 477
10.4.5 LOOK Scheduling
As we described them, both SCAN and C-SCAN move the disk arm across the
full width of the disk. In practice, neither algorithm is often implemented this
way. More commonly, the arm goes only as far as the final request in each
direction. Then, it reverses direction immediately, without going all the way to
the end of the disk. Versions of SCAN and C-SCAN that follow this pattern are
called LOOK and C-LOOK scheduling, because they look for a request before
continuing to move in a given direction (Figure 10.8).
10.4.6 Selection of a Disk-Scheduling Algorithm
Given so many disk-scheduling algorithms, how do we choose the best one?
SSTF is common and has a natural appeal because it increases performance over
FCFS. SCAN and C-SCAN perform better for systems that place a heavy load on
the disk, because they are less likely to cause a starvation problem. For any
particular list of requests, we can define an optimal order of retrieval, but the
computation needed to find an optimal schedule may not justify the savings
over SSTF or SCAN. With any scheduling algorithm, however, performance
depends heavily on the number and types of requests. For instance, suppose
that the queue usually has just one outstanding request. Then, all scheduling
algorithms behave the same, because they have only one choice of where to
move the disk head: they all behave like FCFS scheduling.
Requests for disk service can be greatly influenced by the file-allocation
method. A program reading a contiguously allocated file will generate several
requests that are close together on the disk, resulting in limited head movement.
A linked or indexed file, in contrast, may include blocks that are widely
scattered on the disk, resulting in greater head movement.
The location of directories and index blocks is also important. Since every
file must be opened to be used, and opening a file requires searching the
directory structure, the directories will be accessed frequently. Suppose that a
directory entry is on the first cylinder and a file’s data are on the final cylinder. In
this case, the disk head has to move the entire width of the disk. If the directory
0 14 37 536567 98 122124 183199
queue = 98, 183, 37, 122, 14, 124, 65, 67
head starts at 53
Figure 10.8 C-LOOK disk scheduling.
478 Chapter 10 Mass-Storage Structure
DISK SCHEDULING and SSDs
The disk-scheduling algorithms discussed in this section focus primarily on
minimizing the amount of disk head movement in magnetic disk drives.
SSDs—which do not contain moving disk heads—commonly use a simple
FCFS policy. For example, the Linux Noop scheduler uses an FCFS policy
but modifies it to merge adjacent requests. The observed behavior of SSDs
indicates that the time required to service reads is uniform but that, because
of the properties of flash memory, write service time is not uniform. Some
SSD schedulers have exploited this property and merge only adjacent write
requests, servicing all read requests in FCFS order.
entry were on the middle cylinder, the head would have to move only one-half
the width. Caching the directories and index blocks in main memory can also
help to reduce disk-arm movement, particularly for read requests.
Because of these complexities, the disk-scheduling algorithm should be
written as a separate module of the operating system, so that it can be replaced
with a different algorithm if necessary. Either SSTF or LOOK is a reasonable
choice for the default algorithm.
The scheduling algorithms described here consider only the seek distances.
For modern disks, the rotational latency can be nearly as large as the
average seek time. It is difficult for the operating system to schedule for
improved rotational latency, though, because modern disks do not disclose the
physical location of logical blocks. Disk manufacturers have been alleviating
this problem by implementing disk-scheduling algorithms in the controller
hardware built into the disk drive. If the operating system sends a batch of
requests to the controller, the controller can queue them and then schedule
them to improve both the seek time and the rotational latency.
If I/O performance were the only consideration, the operating system
would gladly turn over the responsibility of disk scheduling to the disk hardware. In practice, however, the operating system may have other constraints on
the service order for requests. For instance, demand paging may take priority
over application I/O, and writes are more urgent than reads if the cache is
running out of free pages. Also, it may be desirable to guarantee the order
of a set of disk writes to make the file system robust in the face of system
crashes. Consider what could happen if the operating system allocated a
disk page to a file and the application wrote data into that page before the
operating system had a chance to flush the file system metadata back to disk.
To accommodate such requirements, an operating system may choose to do its
own disk scheduling and to spoon-feed the requests to the disk controller, one
by one, for some types of I/O.""",
"6.3 Scheduling Algorithms":"""CPU scheduling deals with the problem of deciding which of the processes in the
ready queue is to be allocated the CPU. There are many different CPU-scheduling
algorithms. In this section, we describe several of them.
6.3.1 First-Come, First-Served Scheduling
By far the simplest CPU-scheduling algorithm is the first-come, first-served
(FCFS) scheduling algorithm. With this scheme, the process that requests the
CPU first is allocated the CPU first. The implementation of the FCFS policy is
easily managed with a FIFO queue. When a process enters the ready queue, its
PCB is linked onto the tail of the queue. When the CPU is free, it is allocated to
the process at the head of the queue. The running process is then removed from
the queue. The code for FCFS scheduling is simple to write and understand.
On the negative side, the average waiting time under the FCFS policy is
often quite long. Consider the following set of processes that arrive at time 0,
with the length of the CPU burst given in milliseconds:
Process Burst Time
P1 24
P2 3
P3 3
6.3 Scheduling Algorithms 267
If the processes arrive in the order P1, P2, P3, and are served in FCFS order,
we get the result shown in the following Gantt chart, which is a bar chart that
illustrates a particular schedule, including the start and finish times of each of
the participating processes:
P1 P2 P3
0 24 27 30
The waiting time is 0 milliseconds for process P1, 24 milliseconds for process
P2, and 27 milliseconds for process P3. Thus, the average waiting time is (0
+ 24 + 27)/3 = 17 milliseconds. If the processes arrive in the order P2, P3, P1,
however, the results will be as shown in the following Gantt chart:
P2 P3 P1
036 30
The average waiting time is now (6 + 0 + 3)/3 = 3 milliseconds. This reduction
is substantial. Thus, the average waiting time under an FCFS policy is generally
not minimal and may vary substantially if the processes’ CPU burst times vary
greatly.
In addition, consider the performance of FCFS scheduling in a dynamic
situation. Assume we have one CPU-bound process and many I/O-bound
processes. As the processes flow around the system, the following scenario
may result. The CPU-bound process will get and hold the CPU. During this
time, all the other processes will finish their I/O and will move into the ready
queue, waiting for the CPU. While the processes wait in the ready queue, the
I/O devices are idle. Eventually, the CPU-bound process finishes its CPU burst
and moves to an I/O device. All the I/O-bound processes, which have short
CPU bursts, execute quickly and move back to the I/O queues. At this point,
the CPU sits idle. The CPU-bound process will then move back to the ready
queue and be allocated the CPU. Again, all the I/O processes end up waiting in
the ready queue until the CPU-bound process is done. There is a convoy effect
as all the other processes wait for the one big process to get off the CPU. This
effect results in lower CPU and device utilization than might be possible if the
shorter processes were allowed to go first.
Note also that the FCFS scheduling algorithm is nonpreemptive. Once the
CPU has been allocated to a process, that process keeps the CPU until it releases
the CPU, either by terminating or by requesting I/O. The FCFS algorithm is thus
particularly troublesome for time-sharing systems, where it is important that
each user get a share of the CPU at regular intervals. It would be disastrous to
allow one process to keep the CPU for an extended period.
6.3.2 Shortest-Job-First Scheduling
A different approach to CPU scheduling is the shortest-job-first(SJF) scheduling
algorithm. This algorithm associates with each process the length of the
process’s next CPU burst. When the CPU is available, it is assigned to the
268 Chapter 6 CPU Scheduling
process that has the smallest next CPU burst. If the next CPU bursts of two
processes are the same, FCFS scheduling is used to break the tie. Note that a
more appropriate term for this scheduling method would be the shortest-nextCPU-burst algorithm, because scheduling depends on the length of the next
CPU burst of a process, rather than its total length. We use the term SJF because
most people and textbooks use this term to refer to this type of scheduling.
As an example of SJF scheduling, consider the following set of processes,
with the length of the CPU burst given in milliseconds:
Process Burst Time
P1 6
P2 8
P3 7
P4 3
Using SJF scheduling, we would schedule these processes according to the
following Gantt chart:
P4 P1 P3 P2
0 3 9 16 24
The waiting time is 3 milliseconds for process P1, 16 milliseconds for process
P2, 9 milliseconds for process P3, and 0 milliseconds for process P4. Thus, the
average waiting time is (3 + 16 + 9 + 0)/4 = 7 milliseconds. By comparison, if
we were using the FCFS scheduling scheme, the average waiting time would
be 10.25 milliseconds.
The SJF scheduling algorithm is provably optimal, in that it gives the
minimum average waiting time for a given set of processes. Moving a short
process before a long one decreases the waiting time of the short process more
than it increases the waiting time of the long process. Consequently, the average
waiting time decreases.
The real difficulty with the SJF algorithm is knowing the length of the next
CPU request. For long-term (job) scheduling in a batch system, we can use
the process time limit that a user specifies when he submits the job. In this
situation, users are motivated to estimate the process time limit accurately,
since a lower value may mean faster response but too low a value will cause
a time-limit-exceeded error and require resubmission. SJF scheduling is used
frequently in long-term scheduling.
Although the SJF algorithm is optimal, it cannot be implemented at the
level of short-term CPU scheduling. With short-term scheduling, there is no
way to know the length of the next CPU burst. One approach to this problem
is to try to approximate SJF scheduling. We may not know the length of the
next CPU burst, but we may be able to predict its value. We expect that the
next CPU burst will be similar in length to the previous ones. By computing
an approximation of the length of the next CPU burst, we can pick the process
with the shortest predicted CPU burst.
The next CPU burst is generally predicted as an exponential average of
the measured lengths of previous CPU bursts. We can define the exponential
6.3 Scheduling Algorithms 269
6 4 6 4 13 13 13 …
10 6 6 5 9 11 12 8 …
CPU burst (ti
)
"guess" (τi
)
ti
τi
2
time
4
6
8
10
12
Figure 6.3 Prediction of the length of the next CPU burst.
average with the following formula. Let tn be the length of the nth CPU burst,
and let n+1 be our predicted value for the next CPU burst. Then, for , 0 ≤  ≤
1, define
n+1 =  tn + (1 − )n.
The value of tn contains our most recent information, while n stores the past
history. The parameter  controls the relative weight of recent and past history
in our prediction. If  = 0, then n+1 = n, and recent history has no effect (current
conditions are assumed to be transient). If = 1, then n+1 = tn, and only the most
recent CPU burst matters (history is assumed to be old and irrelevant). More
commonly,  = 1/2, so recent history and past history are equally weighted.
The initial 0 can be defined as a constant or as an overall system average.
Figure 6.3 shows an exponential average with  = 1/2 and 0 = 10.
To understand the behavior of the exponential average, we can expand the
formula for n+1 by substituting for n to find
n+1 = tn + (1 − )tn−1 +···+ (1 − )
j
tn− j +···+ (1 − )
n+1
0.
Typically,  is less than 1. As a result, (1 − ) is also less than 1, and each
successive term has less weight than its predecessor.
The SJF algorithm can be either preemptive or nonpreemptive. The choice
arises when a new process arrives at the ready queue while a previous process is
still executing. The next CPU burst of the newly arrived process may be shorter
than what is left of the currently executing process. A preemptive SJF algorithm
will preempt the currently executing process, whereas a nonpreemptive SJF
algorithm will allow the currently running process to finish its CPU burst.
Preemptive SJF scheduling is sometimes called shortest-remaining-time-first
scheduling.
270 Chapter 6 CPU Scheduling
As an example, consider the following four processes, with the length of
the CPU burst given in milliseconds:
Process Arrival Time Burst Time
P1 0 8
P2 1 4
P3 2 9
P4 3 5
If the processes arrive at the ready queue at the times shown and need the
indicated burst times, then the resulting preemptive SJF schedule is as depicted
in the following Gantt chart:
P1 P3 P1 P2 P4
01 5 10 17 26
Process P1 is started at time 0, since it is the only process in the queue. Process
P2 arrives at time 1. The remaining time for process P1 (7 milliseconds) is
larger than the time required by process P2 (4 milliseconds), so process P1 is
preempted, and process P2 is scheduled. The average waiting time for this
example is [(10 − 1) + (1 − 1) + (17 − 2) + (5 − 3)]/4 = 26/4 = 6.5 milliseconds.
Nonpreemptive SJF scheduling would result in an average waiting time of 7.75
milliseconds.
6.3.3 Priority Scheduling
The SJF algorithm is a special case of the general priority-scheduling algorithm.
A priority is associated with each process, and the CPUis allocated to the process
with the highest priority. Equal-priority processes are scheduled in FCFS order.
An SJF algorithm is simply a priority algorithm where the priority (p) is the
inverse of the (predicted) next CPU burst. The larger the CPU burst, the lower
the priority, and vice versa.
Note that we discuss scheduling in terms of high priority and low priority.
Priorities are generally indicated by some fixed range of numbers, such as 0
to 7 or 0 to 4,095. However, there is no general agreement on whether 0 is the
highest or lowest priority. Some systems use low numbers to represent low
priority; others use low numbers for high priority. This difference can lead to
confusion. In this text, we assume that low numbers represent high priority.
As an example, consider the following set of processes, assumed to have
arrived at time 0 in the order P1, P2, ···, P5, with the length of the CPU burst
given in milliseconds:
Process Burst Time Priority
P1 10 3
P2 1 1
P3 2 4
P4 1 5
P5 5 2
6.3 Scheduling Algorithms 271
Using priority scheduling, we would schedule these processes according to the
following Gantt chart:
P1 P4 P3 P2 P5
0 1 6 16 18 19
The average waiting time is 8.2 milliseconds.
Priorities can be defined either internally or externally. Internally defined
priorities use some measurable quantity or quantities to compute the priority
of a process. For example, time limits, memory requirements, the number of
open files, and the ratio of average I/O burst to average CPU burst have been
used in computing priorities. External priorities are set by criteria outside the
operating system, such as the importance of the process, the type and amount
of funds being paid for computer use, the department sponsoring the work,
and other, often political, factors.
Priority scheduling can be either preemptive or nonpreemptive. When a
process arrives at the ready queue, its priority is compared with the priority
of the currently running process. A preemptive priority scheduling algorithm
will preempt the CPU if the priority of the newly arrived process is higher
than the priority of the currently running process. A nonpreemptive priority
scheduling algorithm will simply put the new process at the head of the ready
queue.
A major problem with priority scheduling algorithms is indefinite blocking, or starvation. A process that is ready to run but waiting for the CPU can
be considered blocked. A priority scheduling algorithm can leave some lowpriority processes waiting indefinitely. In a heavily loaded computer system, a
steady stream of higher-priority processes can prevent a low-priority process
from ever getting the CPU. Generally, one of two things will happen. Either the
process will eventually be run (at 2 A.M. Sunday, when the system is finally
lightly loaded), or the computer system will eventually crash and lose all
unfinished low-priority processes. (Rumor has it that when they shut down
the IBM 7094 at MIT in 1973, they found a low-priority process that had been
submitted in 1967 and had not yet been run.)
A solution to the problem of indefinite blockage of low-priority processes is
aging. Aging involves gradually increasing the priority of processes that wait
in the system for a long time. For example, if priorities range from 127 (low)
to 0 (high), we could increase the priority of a waiting process by 1 every 15
minutes. Eventually, even a process with an initial priority of 127 would have
the highest priority in the system and would be executed. In fact, it would take
no more than 32 hours for a priority-127 process to age to a priority-0 process.
6.3.4 Round-Robin Scheduling
The round-robin (RR) scheduling algorithm is designed especially for timesharing systems. It is similar to FCFS scheduling, but preemption is added to
enable the system to switch between processes. A small unit of time, called a
time quantum or time slice, is defined. A time quantum is generally from 10
to 100 milliseconds in length. The ready queue is treated as a circular queue.
272 Chapter 6 CPU Scheduling
The CPU scheduler goes around the ready queue, allocating the CPU to each
process for a time interval of up to 1 time quantum.
To implement RR scheduling, we again treat the ready queue as a FIFO
queue of processes. New processes are added to the tail of the ready queue.
The CPU scheduler picks the first process from the ready queue, sets a timer to
interrupt after 1 time quantum, and dispatches the process.
One of two things will then happen. The process may have a CPU burst of
less than 1 time quantum. In this case, the process itself will release the CPU
voluntarily. The scheduler will then proceed to the next process in the ready
queue. If the CPU burst of the currently running process is longer than 1 time
quantum, the timer will go off and will cause an interrupt to the operating
system. A context switch will be executed, and the process will be put at the
tail of the ready queue. The CPU scheduler will then select the next process in
the ready queue.
The average waiting time under the RR policy is often long. Consider the
following set of processes that arrive at time 0, with the length of the CPU burst
given in milliseconds:
Process Burst Time
P1 24
P2 3
P3 3
If we use a time quantum of 4 milliseconds, then process P1 gets the first 4
milliseconds. Since it requires another 20 milliseconds, it is preempted after
the first time quantum, and the CPU is given to the next process in the queue,
process P2. Process P2 does not need 4 milliseconds, so it quits before its time
quantum expires. The CPU is then given to the next process, process P3. Once
each process has received 1 time quantum, the CPU is returned to process P1
for an additional time quantum. The resulting RR schedule is as follows:
P1 P1 P1 P1 P1 P1 P2
0 4 7 10 14 26 18 22 30
P3
Let’s calculate the average waiting time for this schedule. P1 waits for 6
milliseconds (10 - 4), P2 waits for 4 milliseconds, and P3 waits for 7 milliseconds.
Thus, the average waiting time is 17/3 = 5.66 milliseconds.
In the RR scheduling algorithm, no process is allocated the CPU for more
than 1 time quantum in a row (unless it is the only runnable process). If a
process’s CPU burst exceeds 1 time quantum, that process is preempted and is
put back in the ready queue. The RR scheduling algorithm is thus preemptive.
If there are n processes in the ready queue and the time quantum is q,
then each process gets 1/n of the CPU time in chunks of at most q time units.
Each process must wait no longer than (n − 1) × q time units until its
next time quantum. For example, with five processes and a time quantum of 20
milliseconds, each process will get up to 20 milliseconds every 100 milliseconds.
The performance of the RR algorithm depends heavily on the size of the time
quantum. At one extreme, if the time quantum is extremely large, the RR policy
6.3 Scheduling Algorithms 273
process time  10 quantum context
switches
12 0
6 1
1 9
0 10
0 10
0 1 2 3 4 5 6 7 8 9 10
6
Figure 6.4 How a smaller time quantum increases context switches.
is the same as the FCFS policy. In contrast, if the time quantum is extremely
small (say, 1 millisecond), the RR approach can result in a large number of
context switches. Assume, for example, that we have only one process of 10
time units. If the quantum is 12 time units, the process finishes in less than 1
time quantum, with no overhead. If the quantum is 6 time units, however, the
process requires 2 quanta, resulting in a context switch. If the time quantum is
1 time unit, then nine context switches will occur, slowing the execution of the
process accordingly (Figure 6.4).
Thus, we want the time quantum to be large with respect to the contextswitch time. If the context-switch time is approximately 10 percent of the
time quantum, then about 10 percent of the CPU time will be spent in context
switching. In practice, most modern systems have time quanta ranging from
10 to 100 milliseconds. The time required for a context switch is typically less
than 10 microseconds; thus, the context-switch time is a small fraction of the
time quantum.
Turnaround time also depends on the size of the time quantum. As we
can see from Figure 6.5, the average turnaround time of a set of processes
does not necessarily improve as the time-quantum size increases. In general,
the average turnaround time can be improved if most processes finish their
next CPU burst in a single time quantum. For example, given three processes
of 10 time units each and a quantum of 1 time unit, the average turnaround
time is 29. If the time quantum is 10, however, the average turnaround time
drops to 20. If context-switch time is added in, the average turnaround time
increases even more for a smaller time quantum, since more context switches
are required.
Although the time quantum should be large compared with the contextswitch time, it should not be too large. As we pointed out earlier, if the time
quantum is too large, RR scheduling degenerates to an FCFS policy. A rule of
thumb is that 80 percent of the CPU bursts should be shorter than the time
quantum.
6.3.5 Multilevel Queue Scheduling
Another class of scheduling algorithms has been created for situations in
which processes are easily classified into different groups. For example, a
274 Chapter 6 CPU Scheduling average turnaround time
1
12.5
12.0
11.5
11.0
10.5
10.0
9.5
9.0
234
time quantum
567
P1
P2
P3
P4
6
3
1
7
process time
Figure 6.5 How turnaround time varies with the time quantum.
common division is made between foreground (interactive) processes and
background (batch) processes. These two types of processes have different
response-time requirements and so may have different scheduling needs. In
addition, foreground processes may have priority (externally defined) over
background processes.
A multilevel queue scheduling algorithm partitions the ready queue into
several separate queues (Figure 6.6). The processes are permanently assigned to
one queue, generally based on some property of the process, such as memory
size, process priority, or process type. Each queue has its own scheduling
algorithm. For example, separate queues might be used for foreground and
background processes. The foreground queue might be scheduled by an RR
algorithm, while the background queue is scheduled by an FCFS algorithm.
In addition, there must be scheduling among the queues, which is commonly implemented as fixed-priority preemptive scheduling. For example, the
foreground queue may have absolute priority over the background queue.
Let’s look at an example of a multilevel queue scheduling algorithm with
five queues, listed below in order of priority:
1. System processes
2. Interactive processes
3. Interactive editing processes
4. Batch processes
5. Student processes
6.3 Scheduling Algorithms 275
system processes
highest priority
lowest priority
interactive processes
interactive editing processes
batch processes
student processes
Figure 6.6 Multilevel queue scheduling.
Each queue has absolute priority over lower-priority queues. No process in the
batch queue, for example, could run unless the queues for system processes,
interactive processes, and interactive editing processes were all empty. If an
interactive editing process entered the ready queue while a batch process was
running, the batch process would be preempted.
Another possibility is to time-slice among the queues. Here, each queue gets
a certain portion of the CPU time, which it can then schedule among its various
processes. For instance, in the foreground–background queue example, the
foreground queue can be given 80 percent of the CPU time for RR scheduling
among its processes, while the background queue receives 20 percent of the
CPU to give to its processes on an FCFS basis.
6.3.6 Multilevel Feedback Queue Scheduling
Normally, when the multilevel queue scheduling algorithm is used, processes
are permanently assigned to a queue when they enter the system. If there
are separate queues for foreground and background processes, for example,
processes do not move from one queue to the other, since processes do not
change their foreground or background nature. This setup has the advantage
of low scheduling overhead, but it is inflexible.
The multilevel feedback queue scheduling algorithm, in contrast, allows
a process to move between queues. The idea is to separate processes according
to the characteristics of their CPU bursts. If a process uses too much CPU time,
it will be moved to a lower-priority queue. This scheme leaves I/O-bound and
interactive processes in the higher-priority queues. In addition, a process that
waits too long in a lower-priority queue may be moved to a higher-priority
queue. This form of aging prevents starvation.
For example, consider a multilevel feedback queue scheduler with three
queues, numbered from 0 to 2 (Figure 6.7). The scheduler first executes all
276 Chapter 6 CPU Scheduling
quantum  8
quantum  16
FCFS
Figure 6.7 Multilevel feedback queues.
processes in queue 0. Only when queue 0 is empty will it execute processes
in queue 1. Similarly, processes in queue 2 will be executed only if queues 0
and 1 are empty. A process that arrives for queue 1 will preempt a process in
queue 2. A process in queue 1 will in turn be preempted by a process arriving
for queue 0.
A process entering the ready queue is put in queue 0. A process in queue 0
is given a time quantum of 8 milliseconds. If it does not finish within this time,
it is moved to the tail of queue 1. If queue 0 is empty, the process at the head
of queue 1 is given a quantum of 16 milliseconds. If it does not complete, it is
preempted and is put into queue 2. Processes in queue 2 are run on an FCFS
basis but are run only when queues 0 and 1 are empty.
This scheduling algorithm gives highest priority to any process with a CPU
burst of 8 milliseconds or less. Such a process will quickly get the CPU, finish
its CPU burst, and go off to its next I/O burst. Processes that need more than
8 but less than 24 milliseconds are also served quickly, although with lower
priority than shorter processes. Long processes automatically sink to queue
2 and are served in FCFS order with any CPU cycles left over from queues 0
and 1.
In general, a multilevel feedback queue scheduler is defined by the
following parameters:
• The number of queues
• The scheduling algorithm for each queue
• The method used to determine when to upgrade a process to a higherpriority queue
• The method used to determine when to demote a process to a lowerpriority queue
• The method used to determine which queue a process will enter when that
process needs service
The definition of a multilevel feedback queue scheduler makes it the most
general CPU-scheduling algorithm. It can be configured to match a specific
system under design. Unfortunately, it is also the most complex algorithm,
6.4 Thread Scheduling 277
since defining the best scheduler requires some means by which to select
values for all the parameters.""",
"4.3 Multithreading Models":"""Our discussion so far has treated threads in a generic sense. However, support
for threads may be provided either at the user level, for user threads, or by the
kernel, for kernel threads. User threads are supported above the kernel and
are managed without kernel support, whereas kernel threads are supported
and managed directly by the operating system. Virtually all contemporary
operating systems—including Windows, Linux, Mac OS X, and Solaris—
support kernel threads.
Ultimately, a relationship must exist between user threads and kernel
threads. In this section, we look at three common ways of establishing such a
relationship: the many-to-one model, the one-to-one model, and the many-tomany model.
4.3.1 Many-to-One Model
The many-to-one model (Figure 4.5) maps many user-level threads to one
kernel thread. Thread management is done by the thread library in user space,
so it is efficient (we discuss thread libraries in Section 4.4). However, the entire
process will block if a thread makes a blocking system call. Also, because only
one thread can access the kernel at a time, multiple threads are unable to run in
parallel on multicore systems. Green threads—a thread library available for
Solaris systems and adopted in early versions of Java—used the many-to-one
model. However, very few systems continue to use the model because of its
inability to take advantage of multiple processing cores.
user thread
k kernel thread
Figure 4.5 Many-to-one model.
170 Chapter 4 Threads
user thread
k k k k kernel thread
Figure 4.6 One-to-one model.
4.3.2 One-to-One Model
The one-to-one model (Figure 4.6) maps each user thread to a kernel thread. It
provides more concurrency than the many-to-one model by allowing another
thread to run when a thread makes a blocking system call. It also allows
multiple threads to run in parallel on multiprocessors. The only drawback to
this model is that creating a user thread requires creating the corresponding
kernel thread. Because the overhead of creating kernel threads can burden the
performance of an application, most implementations of this model restrict the
number of threads supported by the system. Linux, along with the family of
Windows operating systems, implement the one-to-one model.
4.3.3 Many-to-Many Model
The many-to-many model (Figure 4.7) multiplexes many user-level threads to
a smaller or equal number of kernel threads. The number of kernel threads
may be specific to either a particular application or a particular machine (an
application may be allocated more kernel threads on a multiprocessor than on
a single processor).
Let’s consider the effect of this design on concurrency. Whereas the manyto-one model allows the developer to create as many user threads as she wishes,
it does not result in true concurrency, because the kernel can schedule only
one thread at a time. The one-to-one model allows greater concurrency, but the
developer has to be careful not to create too many threads within an application
(and in some instances may be limited in the number of threads she can
user thread
k k k kernel thread
Figure 4.7 Many-to-many model.
4.4 Thread Libraries 171
user thread
k k k k kernel thread
Figure 4.8 Two-level model.
create). The many-to-many model suffers from neither of these shortcomings:
developers can create as many user threads as necessary, and the corresponding
kernel threads can run in parallel on a multiprocessor. Also, when a thread
performs a blocking system call, the kernel can schedule another thread for
execution.
One variation on the many-to-many model still multiplexes many userlevel threads to a smaller or equal number of kernel threads but also allows a
user-level thread to be bound to a kernel thread. This variation is sometimes
referred to as the two-level model (Figure 4.8). The Solaris operating system
supported the two-level model in versions older than Solaris 9. However,
beginning with Solaris 9, this system uses the one-to-one model.""",
"2.3 System Calls":"""System calls provide an interface to the services made available by an operating
system. These calls are generally available as routines written in C and
C++, although certain low-level tasks (for example, tasks where hardware
must be accessed directly) may have to be written using assembly-language
instructions.
Before we discuss how an operating system makes system calls available,
let’s first use an example to illustrate how system calls are used: writing a
simple program to read data from one file and copy them to another file. The
first input that the program will need is the names of the two files: the input file
and the output file. These names can be specified in many ways, depending on
the operating-system design. One approach is for the program to ask the user
for the names. In an interactive system, this approach will require a sequence of
system calls, first to write a prompting message on the screen and then to read
from the keyboard the characters that define the two files. On mouse-based and
icon-based systems, a menu of file names is usually displayed in a window.
The user can then use the mouse to select the source name, and a window
can be opened for the destination name to be specified. This sequence requires
many I/O system calls.
Once the two file names have been obtained, the program must open the
input file and create the output file. Each of these operations requires another
system call. Possible error conditions for each operation can require additional
system calls. When the program tries to open the input file, for example, it may
find that there is no file of that name or that the file is protected against access.
In these cases, the program should print a message on the console (another
sequence of system calls) and then terminate abnormally (another system call).
If the input file exists, then we must create a new output file. We may find that
there is already an output file with the same name. This situation may cause
the program to abort (a system call), or we may delete the existing file (another
system call) and create a new one (yet another system call). Another option,
in an interactive system, is to ask the user (via a sequence of system calls to
output the prompting message and to read the response from the terminal)
whether to replace the existing file or to abort the program.
When both files are set up, we enter a loop that reads from the input file
(a system call) and writes to the output file (another system call). Each read
and write must return status information regarding various possible error
conditions. On input, the program may find that the end of the file has been
reached or that there was a hardware failure in the read (such as a parity error).
The write operation may encounter various errors, depending on the output
device (for example, no more disk space).
2.3 System Calls 63
Finally, after the entire file is copied, the program may close both files
(another system call), write a message to the console or window (more system
calls), and finally terminate normally (the final system call). This system-call
sequence is shown in Figure 2.5.
As you can see, even simple programs may make heavy use of the
operating system. Frequently, systems execute thousands of system calls
per second. Most programmers never see this level of detail, however.
Typically, application developers design programs according to an application
programming interface (API). The API specifies a set of functions that are
available to an application programmer, including the parameters that are
passed to each function and the return values the programmer can expect.
Three of the most common APIs available to application programmers are
the Windows API for Windows systems, the POSIX API for POSIX-based systems
(which include virtually all versions of UNIX, Linux, and Mac OS X), and the Java
API for programs that run on the Java virtual machine. A programmer accesses
an API via a library of code provided by the operating system. In the case of
UNIX and Linux for programs written in the C language, the library is called
libc. Note that—unless specified—the system-call names used throughout
this text are generic examples. Each operating system has its own name for
each system call.
Behind the scenes, the functions that make up an API typically invoke the
actual system calls on behalf of the application programmer. For example, the
Windows function CreateProcess() (which unsurprisingly is used to create
a new process) actually invokes the NTCreateProcess() system call in the
Windows kernel.
Why would an application programmer prefer programming according to
an API rather than invoking actual system calls? There are several reasons for
doing so. One benefit concerns program portability. An application programsource file destination file
Example System Call Sequence
Acquire input file name
 Write prompt to screen
 Accept input
Acquire output file name
 Write prompt to screen
 Accept input
Open the input file
 if file doesn't exist, abort
Create output file
 if file exists, abort
Loop
 Read from input file
 Write to output file
Until read fails
Close output file
Write completion message to screen
Terminate normally
Figure 2.5 Example of how system calls are used.
64 Chapter 2 Operating-System Structures
EXAMPLE OF STANDARD API
As an example of a standard API, consider the read() function that is
available in UNIX and Linux systems. The API for this function is obtained
from the man page by invoking the command
man read
on the command line. A description of this API appears below:
#include <unistd.h>
ssize_t read(int fd, void *buf, size_t count)
return
value
function
name
parameters
A program that uses the read() function must include the unistd.h header
file, as this file defines the ssize t and size t data types (among other
things). The parameters passed to read() are as follows:
• int fd—the file descriptor to be read
• void *buf—a buffer where the data will be read into
• size t count—the maximum number of bytes to be read into the
buffer
On a successful read, the number of bytes read is returned. A return value of
0 indicates end of file. If an error occurs, read() returns −1.
mer designing a program using an API can expect her program to compile and
run on any system that supports the same API (although, in reality, architectural
differences often make this more difficult than it may appear). Furthermore,
actual system calls can often be more detailed and difficult to work with than
the API available to an application programmer. Nevertheless, there often exists
a strong correlation between a function in the API and its associated system call
within the kernel. In fact, many of the POSIX and Windows APIs are similar to
the native system calls provided by the UNIX, Linux, and Windows operating
systems.
For most programming languages, the run-time support system (a set of
functions built into libraries included with a compiler) provides a systemcall interface that serves as the link to system calls made available by the
operating system. The system-call interface intercepts function calls in the API
and invokes the necessary system calls within the operating system. Typically,
a number is associated with each system call, and the system-call interface
maintains a table indexed according to these numbers. The system call interface
2.3 System Calls 65
Implementation
of open ( )
system call
open ( )
user
mode
return
user application
system call interface
kernel
mode
i
open ( )
Figure 2.6 The handling of a user application invoking the open() system call.
then invokes the intended system call in the operating-system kernel and
returns the status of the system call and any return values.
The caller need know nothing about how the system call is implemented
or what it does during execution. Rather, the caller need only obey the API and
understand what the operating system will do as a result of the execution of
that system call. Thus, most of the details of the operating-system interface
are hidden from the programmer by the API and are managed by the run-time
support library. The relationship between an API, the system-call interface,
and the operating system is shown in Figure 2.6, which illustrates how the
operating system handles a user application invoking the open() system call.
System calls occur in different ways, depending on the computer in use.
Often, more information is required than simply the identity of the desired
system call. The exact type and amount of information vary according to the
particular operating system and call. For example, to get input, we may need
to specify the file or device to use as the source, as well as the address and
length of the memory buffer into which the input should be read. Of course,
the device or file and length may be implicit in the call.
Three general methods are used to pass parameters to the operating system.
The simplest approach is to pass the parameters in registers. In some cases,
however, there may be more parameters than registers. In these cases, the
parameters are generally stored in a block, or table, in memory, and the
address of the block is passed as a parameter in a register (Figure 2.7). This
is the approach taken by Linux and Solaris. Parameters also can be placed,
or pushed, onto the stack by the program and popped off the stack by the
operating system. Some operating systems prefer the block or stack method
because those approaches do not limit the number or length of parameters
being passed.
66 Chapter 2 Operating-System Structures
code for
system
call 13
operating system
user program
use parameters
from table X
register
X
X: parameters
for call
load address X
system call 13
Figure 2.7 Passing of parameters as a table."""
}